{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### training data 로부터 20% 비율로 validation data 분리"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "%matplotlib inline\n",
    "\n",
    "def sigmoid(x):\n",
    "    return 1 / (1+np.exp(-x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DataGeneration:\n",
    "    \n",
    "    # target_position = 0 (첫번째열이 정답데이터), target_position=-1 (마지막열이 정답데이터)\n",
    "    def __init__(self, name, file_path, seperation_rate, target_position=-1):\n",
    "        \n",
    "        self.name = name\n",
    "        \n",
    "        self.file_path = file_path\n",
    "        \n",
    "        self.seperation_rate = seperation_rate\n",
    "        \n",
    "        if (target_position == -1  or  target_position == 0):      \n",
    "            self.target_position = target_position\n",
    "        \n",
    "        else:\n",
    "            err_str = 'target_position must be -1 or 0'            \n",
    "            raise Exception(err_str)    \n",
    "            \n",
    "    \n",
    "    # print data target distribution \n",
    "    # str_of_kind : 'original data' or  'training data'  or  'test data'\n",
    "    def __display_target_distribution(self, data, str_of_kind='original data'):\n",
    "        \n",
    "        print('=======================================================================================================')\n",
    "        \n",
    "        target_data = data[ :, self.target_position ]\n",
    "        \n",
    "        # numpy.unique() 사용하여 loaded data target 분포 확인\n",
    "        unique, counts = np.unique(target_data, return_counts=True)\n",
    "\n",
    "        unique_target = []\n",
    "    \n",
    "        for index in range(len(unique)):\n",
    "        \n",
    "            print('[DataGeneration] unique number of ' + str_of_kind + ' = ', unique[index], ', count = ', counts[index])\n",
    "        \n",
    "            unique_target.append(unique[index])\n",
    "\n",
    "        for index in range(len(unique_target)):\n",
    "        \n",
    "            print('[DataGeneration] unique number of ' + str_of_kind + ' = ', unique_target[index], ', ratio = ', np.round(100 * counts[index] / (target_data.shape[0]), 2), ' %')\n",
    "    \n",
    "        print('=======================================================================================================')\n",
    "        \n",
    "        \n",
    "    # numpy.random.shuffle()  이용하여 training_data / test_data 생성\n",
    "    def generate(self):\n",
    "    \n",
    "        # 데이터 불러오기, 파일이 없는 경우 exception 발생\n",
    "\n",
    "        try:\n",
    "            loaded_data = np.loadtxt(self.file_path, delimiter=',', dtype=np.float32)\n",
    "            \n",
    "        except Exception as err:\n",
    "            print('[DataGeneration::generate()]  ', str(err))\n",
    "            raise Exception(str(err))\n",
    "\n",
    "        print(\"[DataGeneration]  loaded_data.shape = \", loaded_data.shape)\n",
    "            \n",
    "        # print the target distribution of original data \n",
    "        \n",
    "        self.__display_target_distribution(loaded_data, 'original data')\n",
    "        \n",
    "        \n",
    "        # 분리비율에 맞게 테스트데이터로 분리\n",
    "        total_data_num = len(loaded_data)\n",
    "        test_data_num = int(len(loaded_data) * self.seperation_rate)\n",
    "\n",
    "        # numpy.random.shuffle 을 이용하여 랜덤하게 데이터 섞기\n",
    "        np.random.shuffle(loaded_data)\n",
    "        \n",
    "        # test_data 는 0 : test_data_num\n",
    "        \n",
    "        \n",
    "        test_data = loaded_data[ 0:test_data_num ]\n",
    "\n",
    "        # training_data 는 test_data_num 부터 끝까지 \n",
    "        training_data = loaded_data[ test_data_num: ]\n",
    "\n",
    "        # display target distribution of generated data \n",
    "        \n",
    "        self.__display_target_distribution(training_data, 'training data')\n",
    "        \n",
    "        self.__display_target_distribution(test_data, 'test data')\n",
    "        \n",
    "        return training_data, test_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datetime import datetime      # datetime.now() 를 이용하여 학습 경과 시간 측정\n",
    "\n",
    "class NeuralNetwork:\n",
    "    \n",
    "    def __init__(self, input_nodes, hidden_nodes, output_nodes, learning_rate):\n",
    "        \n",
    "        self.input_nodes = input_nodes\n",
    "        self.hidden_nodes = hidden_nodes\n",
    "        self.output_nodes = output_nodes\n",
    "        \n",
    "        # 은닉층 가중치  W2 = (784 X 100) Xavier/He 방법으로 self.W2 가중치 초기화\n",
    "        self.W2 = np.random.randn(self.input_nodes, self.hidden_nodes) / np.sqrt(self.input_nodes/2)\n",
    "        self.b2 = np.random.rand(self.hidden_nodes)      \n",
    "        \n",
    "        # 출력층 가중치는 W3 = (100X10)  Xavier/He 방법으로 self.W3 가중치 초기화\n",
    "        self.W3 = np.random.randn(self.hidden_nodes, self.output_nodes) / np.sqrt(self.hidden_nodes/2)\n",
    "        self.b3 = np.random.rand(self.output_nodes)      \n",
    "                        \n",
    "        # 출력층 선형회귀 값 Z3, 출력값 A3 정의 (모두 행렬로 표시)\n",
    "        self.Z3 = np.zeros([1,output_nodes])\n",
    "        self.A3 = np.zeros([1,output_nodes])\n",
    "        \n",
    "        # 은닉층 선형회귀 값 Z2, 출력값 A2 정의 (모두 행렬로 표시)\n",
    "        self.Z2 = np.zeros([1,hidden_nodes])\n",
    "        self.A2 = np.zeros([1,hidden_nodes])\n",
    "        \n",
    "        # 입력층 선형회귀 값 Z1, 출력값 A1 정의 (모두 행렬로 표시)\n",
    "        self.Z1 = np.zeros([1,input_nodes])    \n",
    "        self.A1 = np.zeros([1,input_nodes])       \n",
    "        \n",
    "        # 학습률 learning rate 초기화\n",
    "        self.learning_rate = learning_rate\n",
    "        \n",
    "    def feed_forward(self):  \n",
    "        \n",
    "        delta = 1e-7    # log 무한대 발산 방지\n",
    "        \n",
    "        # 입력층 선형회귀 값 Z1, 출력값 A1 계산\n",
    "        self.Z1 = self.input_data\n",
    "        self.A1 = self.input_data\n",
    "        \n",
    "        # 은닉층 선형회귀 값 Z2, 출력값 A2 계산    \n",
    "        self.Z2 = np.dot(self.A1, self.W2) + self.b2\n",
    "        self.A2 = sigmoid(self.Z2)\n",
    "        \n",
    "        # 출력층 선형회귀 값 Z3, 출력값 A3 계산\n",
    "        self.Z3 = np.dot(self.A2, self.W3) + self.b3\n",
    "        self.A3 = sigmoid(self.Z3)\n",
    "        \n",
    "        return  -np.sum( self.target_data*np.log(self.A3 + delta) + (1-self.target_data)*np.log((1 - self.A3)+delta ) )    \n",
    "    \n",
    "    def loss_val(self):\n",
    "        \n",
    "        delta = 1e-7    # log 무한대 발산 방지\n",
    "        \n",
    "        # 입력층 선형회귀 값 Z1, 출력값 A1 계산\n",
    "        self.Z1 = self.input_data\n",
    "        self.A1 = self.input_data\n",
    "        \n",
    "        # 은닉층 선형회귀 값 Z2, 출력값 A2 계산    \n",
    "        self.Z2 = np.dot(self.A1, self.W2) + self.b2\n",
    "        self.A2 = sigmoid(self.Z2)\n",
    "        \n",
    "        # 출력층 선형회귀 값 Z3, 출력값 A3 계산\n",
    "        self.Z3 = np.dot(self.A2, self.W3) + self.b3\n",
    "        self.A3 = sigmoid(self.Z3)\n",
    "        \n",
    "        return  -np.sum( self.target_data*np.log(self.A3 + delta) + (1-self.target_data)*np.log((1 - self.A3)+delta ) )\n",
    "   \n",
    "    \n",
    "    # 정확도 측정함수 \n",
    "    def accuracy(self, test_input_data, test_target_data):\n",
    "        \n",
    "        matched_list = []\n",
    "        \n",
    "        # index_label_prediction 저장 list\n",
    "        temp_list = []\n",
    "        index_label_prediction_list = []\n",
    "        \n",
    "        \n",
    "        for index in range(len(test_input_data)):\n",
    "                        \n",
    "            label = int(test_target_data[index])\n",
    "                        \n",
    "            # one-hot encoding을 위한 데이터 정규화 (data normalize)\n",
    "            data = (test_input_data[index] / 255.0 * 0.99) + 0.01\n",
    "                  \n",
    "            # predict 를 위해서 vector 을 matrix 로 변환하여 인수로 넘겨줌\n",
    "            predicted_num = self.predict(np.array(data, ndmin=2)) \n",
    "            #예측 함수 만들기\n",
    "            if label == predicted_num:\n",
    "                matched_list.append(index)\n",
    "                \n",
    "            else:\n",
    "                # index_label_prediction 리스트 생성\n",
    "                temp_list.append(index)\n",
    "                temp_list.append(label)\n",
    "                temp_list.append(predicted_num)\n",
    "                \n",
    "                index_label_prediction_list.append(temp_list)\n",
    "                \n",
    "                temp_list = []    # temp_list 초기화 해주지 않으면 심각한 error 발생\n",
    "                \n",
    "        #break point 로 디버깅을 해주어야 하는게 일반적인 정석이지만 머신러닝은 디버깅에 약하다\n",
    "        \n",
    "        accuracy_val = (len(matched_list)/(len(test_input_data)))\n",
    "        \n",
    "        return accuracy_val, index_label_prediction_list\n",
    "    \n",
    "    \n",
    "    def train(self, input_data, target_data):   # input_data : 784 개, target_data : 10개\n",
    "        \n",
    "        self.target_data = target_data    \n",
    "        self.input_data = input_data\n",
    "        \n",
    "        # 먼저 feed forward 를 통해서 최종 출력값과 이를 바탕으로 현재의 에러 값 계산\n",
    "        loss_val = self.feed_forward()\n",
    "        \n",
    "        # 출력층 loss 인 loss_3 구함\n",
    "        loss_3 = (self.A3-self.target_data) * self.A3 * (1-self.A3)\n",
    "        \n",
    "        # 출력층 가중치 W3, 출력층 바이어스 b3 업데이트\n",
    "        self.W3 = self.W3 - self.learning_rate * np.dot(self.A2.T, loss_3)   \n",
    "        \n",
    "        self.b3 = self.b3 - self.learning_rate * loss_3  \n",
    "        \n",
    "        # 은닉층 loss 인 loss_2 구함        \n",
    "        loss_2 = np.dot(loss_3, self.W3.T) * self.A2 * (1-self.A2)\n",
    "        \n",
    "        # 은닉층 가중치 W2, 은닉층 바이어스 b2 업데이트\n",
    "        self.W2 = self.W2 - self.learning_rate * np.dot(self.A1.T, loss_2)   \n",
    "        \n",
    "        self.b2 = self.b2 - self.learning_rate * loss_2\n",
    "        \n",
    "    def predict(self, input_data):        # input_data 는 행렬로 입력됨 즉, (1, 784) shape 을 가짐        \n",
    "        \n",
    "        Z2 = np.dot(input_data, self.W2) + self.b2\n",
    "        A2 = sigmoid(Z2)\n",
    "        \n",
    "        Z3 = np.dot(A2, self.W3) + self.b3\n",
    "        A3 = sigmoid(Z3)\n",
    "        \n",
    "        predicted_num = np.argmax(A3)\n",
    "    \n",
    "        return predicted_num"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### validation 비율 0.2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[DataGeneration]  loaded_data.shape =  (60000, 785)\n",
      "=======================================================================================================\n",
      "[DataGeneration] unique number of original data =  0.0 , count =  5923\n",
      "[DataGeneration] unique number of original data =  1.0 , count =  6742\n",
      "[DataGeneration] unique number of original data =  2.0 , count =  5958\n",
      "[DataGeneration] unique number of original data =  3.0 , count =  6131\n",
      "[DataGeneration] unique number of original data =  4.0 , count =  5842\n",
      "[DataGeneration] unique number of original data =  5.0 , count =  5421\n",
      "[DataGeneration] unique number of original data =  6.0 , count =  5918\n",
      "[DataGeneration] unique number of original data =  7.0 , count =  6265\n",
      "[DataGeneration] unique number of original data =  8.0 , count =  5851\n",
      "[DataGeneration] unique number of original data =  9.0 , count =  5949\n",
      "[DataGeneration] unique number of original data =  0.0 , ratio =  9.87  %\n",
      "[DataGeneration] unique number of original data =  1.0 , ratio =  11.24  %\n",
      "[DataGeneration] unique number of original data =  2.0 , ratio =  9.93  %\n",
      "[DataGeneration] unique number of original data =  3.0 , ratio =  10.22  %\n",
      "[DataGeneration] unique number of original data =  4.0 , ratio =  9.74  %\n",
      "[DataGeneration] unique number of original data =  5.0 , ratio =  9.04  %\n",
      "[DataGeneration] unique number of original data =  6.0 , ratio =  9.86  %\n",
      "[DataGeneration] unique number of original data =  7.0 , ratio =  10.44  %\n",
      "[DataGeneration] unique number of original data =  8.0 , ratio =  9.75  %\n",
      "[DataGeneration] unique number of original data =  9.0 , ratio =  9.91  %\n",
      "=======================================================================================================\n",
      "=======================================================================================================\n",
      "[DataGeneration] unique number of training data =  0.0 , count =  4732\n",
      "[DataGeneration] unique number of training data =  1.0 , count =  5382\n",
      "[DataGeneration] unique number of training data =  2.0 , count =  4789\n",
      "[DataGeneration] unique number of training data =  3.0 , count =  4919\n",
      "[DataGeneration] unique number of training data =  4.0 , count =  4727\n",
      "[DataGeneration] unique number of training data =  5.0 , count =  4323\n",
      "[DataGeneration] unique number of training data =  6.0 , count =  4733\n",
      "[DataGeneration] unique number of training data =  7.0 , count =  4992\n",
      "[DataGeneration] unique number of training data =  8.0 , count =  4662\n",
      "[DataGeneration] unique number of training data =  9.0 , count =  4741\n",
      "[DataGeneration] unique number of training data =  0.0 , ratio =  9.86  %\n",
      "[DataGeneration] unique number of training data =  1.0 , ratio =  11.21  %\n",
      "[DataGeneration] unique number of training data =  2.0 , ratio =  9.98  %\n",
      "[DataGeneration] unique number of training data =  3.0 , ratio =  10.25  %\n",
      "[DataGeneration] unique number of training data =  4.0 , ratio =  9.85  %\n",
      "[DataGeneration] unique number of training data =  5.0 , ratio =  9.01  %\n",
      "[DataGeneration] unique number of training data =  6.0 , ratio =  9.86  %\n",
      "[DataGeneration] unique number of training data =  7.0 , ratio =  10.4  %\n",
      "[DataGeneration] unique number of training data =  8.0 , ratio =  9.71  %\n",
      "[DataGeneration] unique number of training data =  9.0 , ratio =  9.88  %\n",
      "=======================================================================================================\n",
      "=======================================================================================================\n",
      "[DataGeneration] unique number of test data =  0.0 , count =  1191\n",
      "[DataGeneration] unique number of test data =  1.0 , count =  1360\n",
      "[DataGeneration] unique number of test data =  2.0 , count =  1169\n",
      "[DataGeneration] unique number of test data =  3.0 , count =  1212\n",
      "[DataGeneration] unique number of test data =  4.0 , count =  1115\n",
      "[DataGeneration] unique number of test data =  5.0 , count =  1098\n",
      "[DataGeneration] unique number of test data =  6.0 , count =  1185\n",
      "[DataGeneration] unique number of test data =  7.0 , count =  1273\n",
      "[DataGeneration] unique number of test data =  8.0 , count =  1189\n",
      "[DataGeneration] unique number of test data =  9.0 , count =  1208\n",
      "[DataGeneration] unique number of test data =  0.0 , ratio =  9.93  %\n",
      "[DataGeneration] unique number of test data =  1.0 , ratio =  11.33  %\n",
      "[DataGeneration] unique number of test data =  2.0 , ratio =  9.74  %\n",
      "[DataGeneration] unique number of test data =  3.0 , ratio =  10.1  %\n",
      "[DataGeneration] unique number of test data =  4.0 , ratio =  9.29  %\n",
      "[DataGeneration] unique number of test data =  5.0 , ratio =  9.15  %\n",
      "[DataGeneration] unique number of test data =  6.0 , ratio =  9.88  %\n",
      "[DataGeneration] unique number of test data =  7.0 , ratio =  10.61  %\n",
      "[DataGeneration] unique number of test data =  8.0 , ratio =  9.91  %\n",
      "[DataGeneration] unique number of test data =  9.0 , ratio =  10.07  %\n",
      "=======================================================================================================\n",
      "training_data.shape =  (48000, 785)\n",
      "validation_data.shape =  (12000, 785)\n"
     ]
    }
   ],
   "source": [
    "# DataGeneration class 이용하여 training data , validation data 생성\n",
    "seperation_rate = 0.2  # training data 10 % 비율로 validation data 생성\n",
    "target_position = 0    # 정답은 첫번째 열\n",
    "\n",
    "try:\n",
    "    data_obj = DataGeneration('MNIST', './mnist_train.csv', seperation_rate, target_position)\n",
    "\n",
    "    (training_data, validation_data) = data_obj.generate()\n",
    "    \n",
    "    print(\"training_data.shape = \", training_data.shape)\n",
    "    print(\"validation_data.shape = \", validation_data.shape)\n",
    "\n",
    "except Exception as err:\n",
    "    print('Exception Occur !!')\n",
    "    print(str(err))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 은닉층 노드 100 개 인 경우의 MNIST 오차역전파 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epochs =  1 , step =  0 , current loss_val =  5.408420079047045\n",
      "epochs =  1 , step =  1000 , current loss_val =  2.3089276374835572\n",
      "epochs =  1 , step =  2000 , current loss_val =  1.0075851733041632\n",
      "epochs =  1 , step =  3000 , current loss_val =  0.9782754638187658\n",
      "epochs =  1 , step =  4000 , current loss_val =  0.9112461115796878\n",
      "epochs =  1 , step =  5000 , current loss_val =  2.944517215657383\n",
      "epochs =  1 , step =  6000 , current loss_val =  1.2436519999165012\n",
      "epochs =  1 , step =  7000 , current loss_val =  1.2786177388701336\n",
      "epochs =  1 , step =  8000 , current loss_val =  0.7403460015838529\n",
      "epochs =  1 , step =  9000 , current loss_val =  1.0522330397103703\n",
      "epochs =  1 , step =  10000 , current loss_val =  2.4452453068296753\n",
      "epochs =  1 , step =  11000 , current loss_val =  0.8893545722392867\n",
      "epochs =  1 , step =  12000 , current loss_val =  0.7591572817813561\n",
      "epochs =  1 , step =  13000 , current loss_val =  0.6729140385906693\n",
      "epochs =  1 , step =  14000 , current loss_val =  0.6798272341204626\n",
      "epochs =  1 , step =  15000 , current loss_val =  0.869778267618027\n",
      "epochs =  1 , step =  16000 , current loss_val =  0.679501899343082\n",
      "epochs =  1 , step =  17000 , current loss_val =  0.956314125050536\n",
      "epochs =  1 , step =  18000 , current loss_val =  1.8074515295973128\n",
      "epochs =  1 , step =  19000 , current loss_val =  0.9845977056113709\n",
      "epochs =  1 , step =  20000 , current loss_val =  0.7017926544671107\n",
      "epochs =  1 , step =  21000 , current loss_val =  0.763045015991707\n",
      "epochs =  1 , step =  22000 , current loss_val =  0.6693382821292383\n",
      "epochs =  1 , step =  23000 , current loss_val =  0.7039883317746614\n",
      "epochs =  1 , step =  24000 , current loss_val =  0.954469930803131\n",
      "epochs =  1 , step =  25000 , current loss_val =  0.7741779415806482\n",
      "epochs =  1 , step =  26000 , current loss_val =  0.7573629280374276\n",
      "epochs =  1 , step =  27000 , current loss_val =  0.9513295563301697\n",
      "epochs =  1 , step =  28000 , current loss_val =  0.7259732448170477\n",
      "epochs =  1 , step =  29000 , current loss_val =  0.8661336413167973\n",
      "epochs =  1 , step =  30000 , current loss_val =  0.7900783043652129\n",
      "epochs =  1 , step =  31000 , current loss_val =  0.6978139225979773\n",
      "epochs =  1 , step =  32000 , current loss_val =  0.9330021220978919\n",
      "epochs =  1 , step =  33000 , current loss_val =  0.8140456494650008\n",
      "epochs =  1 , step =  34000 , current loss_val =  8.912408368534898\n",
      "epochs =  1 , step =  35000 , current loss_val =  0.6761506834676198\n",
      "epochs =  1 , step =  36000 , current loss_val =  1.5336074717492443\n",
      "epochs =  1 , step =  37000 , current loss_val =  0.740471316182006\n",
      "epochs =  1 , step =  38000 , current loss_val =  0.7968203431707742\n",
      "epochs =  1 , step =  39000 , current loss_val =  0.6835737466971167\n",
      "epochs =  1 , step =  40000 , current loss_val =  1.3730957043277905\n",
      "epochs =  1 , step =  41000 , current loss_val =  0.7423294908725446\n",
      "epochs =  1 , step =  42000 , current loss_val =  0.6914192831611565\n",
      "epochs =  1 , step =  43000 , current loss_val =  0.8435555856919777\n",
      "epochs =  1 , step =  44000 , current loss_val =  0.8482215537293365\n",
      "epochs =  1 , step =  45000 , current loss_val =  0.792413990325788\n",
      "epochs =  1 , step =  46000 , current loss_val =  0.7597250162961526\n",
      "epochs =  1 , step =  47000 , current loss_val =  0.8232650239228785\n",
      "\n",
      "current epochs =  1  , current training accuracy =  93.2  %\n",
      "current epochs =  1  , current validation accuracy =  92.60000000000001  %\n",
      "\n",
      "epochs =  2 , step =  0 , current loss_val =  3.9066243157515257\n",
      "epochs =  2 , step =  1000 , current loss_val =  0.9605620474018588\n",
      "epochs =  2 , step =  2000 , current loss_val =  0.8283524066317501\n",
      "epochs =  2 , step =  3000 , current loss_val =  0.7144509192999774\n",
      "epochs =  2 , step =  4000 , current loss_val =  0.7524816475724525\n",
      "epochs =  2 , step =  5000 , current loss_val =  0.9422971440500565\n",
      "epochs =  2 , step =  6000 , current loss_val =  0.7538871488327303\n",
      "epochs =  2 , step =  7000 , current loss_val =  0.817670217961179\n",
      "epochs =  2 , step =  8000 , current loss_val =  0.708998933292232\n",
      "epochs =  2 , step =  9000 , current loss_val =  0.9174973828601299\n",
      "epochs =  2 , step =  10000 , current loss_val =  2.398702858065935\n",
      "epochs =  2 , step =  11000 , current loss_val =  0.9147906237066139\n",
      "epochs =  2 , step =  12000 , current loss_val =  0.8766627074636968\n",
      "epochs =  2 , step =  13000 , current loss_val =  0.7671040795311173\n",
      "epochs =  2 , step =  14000 , current loss_val =  0.774962728125751\n",
      "epochs =  2 , step =  15000 , current loss_val =  0.7857453729877941\n",
      "epochs =  2 , step =  16000 , current loss_val =  0.7616656758155083\n",
      "epochs =  2 , step =  17000 , current loss_val =  0.9818137532262998\n",
      "epochs =  2 , step =  18000 , current loss_val =  1.3474905781435607\n",
      "epochs =  2 , step =  19000 , current loss_val =  0.8723854283101031\n",
      "epochs =  2 , step =  20000 , current loss_val =  0.7209010501103752\n",
      "epochs =  2 , step =  21000 , current loss_val =  0.8778847150610721\n",
      "epochs =  2 , step =  22000 , current loss_val =  0.6915411715082654\n",
      "epochs =  2 , step =  23000 , current loss_val =  0.7896678900758233\n",
      "epochs =  2 , step =  24000 , current loss_val =  0.9314556094710367\n",
      "epochs =  2 , step =  25000 , current loss_val =  0.887231815259173\n",
      "epochs =  2 , step =  26000 , current loss_val =  0.8211740419453133\n",
      "epochs =  2 , step =  27000 , current loss_val =  0.8918241384386906\n",
      "epochs =  2 , step =  28000 , current loss_val =  0.7818665985167337\n",
      "epochs =  2 , step =  29000 , current loss_val =  0.9640165545386532\n",
      "epochs =  2 , step =  30000 , current loss_val =  0.850574408707671\n",
      "epochs =  2 , step =  31000 , current loss_val =  0.7307706512778982\n",
      "epochs =  2 , step =  32000 , current loss_val =  0.8116303937715728\n",
      "epochs =  2 , step =  33000 , current loss_val =  0.9765898152628903\n",
      "epochs =  2 , step =  34000 , current loss_val =  5.411362175576934\n",
      "epochs =  2 , step =  35000 , current loss_val =  0.7148107262021012\n",
      "epochs =  2 , step =  36000 , current loss_val =  1.1177817869293518\n",
      "epochs =  2 , step =  37000 , current loss_val =  0.7523675766337637\n",
      "epochs =  2 , step =  38000 , current loss_val =  0.8056714592314252\n",
      "epochs =  2 , step =  39000 , current loss_val =  0.7019104654232302\n",
      "epochs =  2 , step =  40000 , current loss_val =  1.2096056911947346\n",
      "epochs =  2 , step =  41000 , current loss_val =  0.7909675155260134\n",
      "epochs =  2 , step =  42000 , current loss_val =  0.7028749947875905\n",
      "epochs =  2 , step =  43000 , current loss_val =  0.7901746586727073\n",
      "epochs =  2 , step =  44000 , current loss_val =  0.8864592251607122\n",
      "epochs =  2 , step =  45000 , current loss_val =  0.8359198811460951\n",
      "epochs =  2 , step =  46000 , current loss_val =  0.7658876660044361\n",
      "epochs =  2 , step =  47000 , current loss_val =  0.8544569980653896\n",
      "\n",
      "current epochs =  2  , current training accuracy =  95.3  %\n",
      "current epochs =  2  , current validation accuracy =  94.8  %\n",
      "\n",
      "epochs =  3 , step =  0 , current loss_val =  2.2254978783164017\n",
      "epochs =  3 , step =  1000 , current loss_val =  0.9659851887852012\n",
      "epochs =  3 , step =  2000 , current loss_val =  0.8430572549340055\n",
      "epochs =  3 , step =  3000 , current loss_val =  0.7409047834640489\n",
      "epochs =  3 , step =  4000 , current loss_val =  0.7701290679915125\n",
      "epochs =  3 , step =  5000 , current loss_val =  0.9346671822156397\n",
      "epochs =  3 , step =  6000 , current loss_val =  0.769151145501051\n",
      "epochs =  3 , step =  7000 , current loss_val =  0.792338710067572\n",
      "epochs =  3 , step =  8000 , current loss_val =  0.7410962964124875\n",
      "epochs =  3 , step =  9000 , current loss_val =  0.8708349167489386\n",
      "epochs =  3 , step =  10000 , current loss_val =  2.0834961899701794\n",
      "epochs =  3 , step =  11000 , current loss_val =  0.8758965915625636\n",
      "epochs =  3 , step =  12000 , current loss_val =  0.8571384522801101\n",
      "epochs =  3 , step =  13000 , current loss_val =  0.7835666421747308\n",
      "epochs =  3 , step =  14000 , current loss_val =  0.7892421922027972\n",
      "epochs =  3 , step =  15000 , current loss_val =  0.8209528440263377\n",
      "epochs =  3 , step =  16000 , current loss_val =  0.7975620282821647\n",
      "epochs =  3 , step =  17000 , current loss_val =  0.9928353845204367\n",
      "epochs =  3 , step =  18000 , current loss_val =  1.273068232328044\n",
      "epochs =  3 , step =  19000 , current loss_val =  0.7834728280227725\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epochs =  3 , step =  20000 , current loss_val =  0.7400617732531296\n",
      "epochs =  3 , step =  21000 , current loss_val =  0.9517780529814029\n",
      "epochs =  3 , step =  22000 , current loss_val =  0.7056663665613712\n",
      "epochs =  3 , step =  23000 , current loss_val =  0.8114458161583662\n",
      "epochs =  3 , step =  24000 , current loss_val =  0.9615785302508748\n",
      "epochs =  3 , step =  25000 , current loss_val =  0.9673447644768436\n",
      "epochs =  3 , step =  26000 , current loss_val =  0.8312394244364858\n",
      "epochs =  3 , step =  27000 , current loss_val =  0.8656698181830248\n",
      "epochs =  3 , step =  28000 , current loss_val =  0.804982129836404\n",
      "epochs =  3 , step =  29000 , current loss_val =  0.9986357125033127\n",
      "epochs =  3 , step =  30000 , current loss_val =  0.8583335279333593\n",
      "epochs =  3 , step =  31000 , current loss_val =  0.7511537517549515\n",
      "epochs =  3 , step =  32000 , current loss_val =  0.8192882305424547\n",
      "epochs =  3 , step =  33000 , current loss_val =  1.0206505191370858\n",
      "epochs =  3 , step =  34000 , current loss_val =  2.820648693718836\n",
      "epochs =  3 , step =  35000 , current loss_val =  0.7270629838092354\n",
      "epochs =  3 , step =  36000 , current loss_val =  1.056419769790847\n",
      "epochs =  3 , step =  37000 , current loss_val =  0.7543914527639493\n",
      "epochs =  3 , step =  38000 , current loss_val =  0.8210716013742816\n",
      "epochs =  3 , step =  39000 , current loss_val =  0.7136649441900428\n",
      "epochs =  3 , step =  40000 , current loss_val =  1.1762455420149325\n",
      "epochs =  3 , step =  41000 , current loss_val =  0.8017193178012564\n",
      "epochs =  3 , step =  42000 , current loss_val =  0.7235438018394862\n",
      "epochs =  3 , step =  43000 , current loss_val =  0.7949214293605634\n",
      "epochs =  3 , step =  44000 , current loss_val =  0.9264267749313891\n",
      "epochs =  3 , step =  45000 , current loss_val =  0.835280981993806\n",
      "epochs =  3 , step =  46000 , current loss_val =  0.78611264805357\n",
      "epochs =  3 , step =  47000 , current loss_val =  0.8750340625201178\n",
      "\n",
      "current epochs =  3  , current training accuracy =  96.3  %\n",
      "current epochs =  3  , current validation accuracy =  95.5  %\n",
      "\n",
      "epochs =  4 , step =  0 , current loss_val =  1.469238623306083\n",
      "epochs =  4 , step =  1000 , current loss_val =  0.9646427561764489\n",
      "epochs =  4 , step =  2000 , current loss_val =  0.8484167679602701\n",
      "epochs =  4 , step =  3000 , current loss_val =  0.7441250779793173\n",
      "epochs =  4 , step =  4000 , current loss_val =  0.7617161322492486\n",
      "epochs =  4 , step =  5000 , current loss_val =  0.9265905722923404\n",
      "epochs =  4 , step =  6000 , current loss_val =  0.8240703906659435\n",
      "epochs =  4 , step =  7000 , current loss_val =  0.8055817519323797\n",
      "epochs =  4 , step =  8000 , current loss_val =  0.7554213400397471\n",
      "epochs =  4 , step =  9000 , current loss_val =  0.945788469868268\n",
      "epochs =  4 , step =  10000 , current loss_val =  1.4517707506732516\n",
      "epochs =  4 , step =  11000 , current loss_val =  0.8547017325382713\n",
      "epochs =  4 , step =  12000 , current loss_val =  0.8365496065078176\n",
      "epochs =  4 , step =  13000 , current loss_val =  0.7915472680913885\n",
      "epochs =  4 , step =  14000 , current loss_val =  0.7854075687451451\n",
      "epochs =  4 , step =  15000 , current loss_val =  0.7833477856918402\n",
      "epochs =  4 , step =  16000 , current loss_val =  0.8236453651709619\n",
      "epochs =  4 , step =  17000 , current loss_val =  0.9944489251453102\n",
      "epochs =  4 , step =  18000 , current loss_val =  1.2333973247244174\n",
      "epochs =  4 , step =  19000 , current loss_val =  0.7385844243233861\n",
      "epochs =  4 , step =  20000 , current loss_val =  0.7575242605069352\n",
      "epochs =  4 , step =  21000 , current loss_val =  0.9722049655442584\n",
      "epochs =  4 , step =  22000 , current loss_val =  0.725378834234324\n",
      "epochs =  4 , step =  23000 , current loss_val =  0.8168481957949449\n",
      "epochs =  4 , step =  24000 , current loss_val =  0.926744357561418\n",
      "epochs =  4 , step =  25000 , current loss_val =  0.990625464124225\n",
      "epochs =  4 , step =  26000 , current loss_val =  0.8347854235546225\n",
      "epochs =  4 , step =  27000 , current loss_val =  0.8486024082397449\n",
      "epochs =  4 , step =  28000 , current loss_val =  0.8359649931891409\n",
      "epochs =  4 , step =  29000 , current loss_val =  1.011170400402644\n",
      "epochs =  4 , step =  30000 , current loss_val =  0.8458532060436921\n",
      "epochs =  4 , step =  31000 , current loss_val =  0.7601210241613341\n",
      "epochs =  4 , step =  32000 , current loss_val =  0.8291289297841391\n",
      "epochs =  4 , step =  33000 , current loss_val =  1.0176219800781339\n",
      "epochs =  4 , step =  34000 , current loss_val =  1.389304904970573\n",
      "epochs =  4 , step =  35000 , current loss_val =  0.7389768570160636\n",
      "epochs =  4 , step =  36000 , current loss_val =  1.0352462545830523\n",
      "epochs =  4 , step =  37000 , current loss_val =  0.7660027986632792\n",
      "epochs =  4 , step =  38000 , current loss_val =  0.8238587735248756\n",
      "epochs =  4 , step =  39000 , current loss_val =  0.7158993328384198\n",
      "epochs =  4 , step =  40000 , current loss_val =  1.160367370410351\n",
      "epochs =  4 , step =  41000 , current loss_val =  0.8021490083813988\n",
      "epochs =  4 , step =  42000 , current loss_val =  0.7427055164141877\n",
      "epochs =  4 , step =  43000 , current loss_val =  0.8038052209256326\n",
      "epochs =  4 , step =  44000 , current loss_val =  0.8954559556962457\n",
      "epochs =  4 , step =  45000 , current loss_val =  0.8336013907326147\n",
      "epochs =  4 , step =  46000 , current loss_val =  0.8222891413268308\n",
      "epochs =  4 , step =  47000 , current loss_val =  0.902306052141676\n",
      "\n",
      "current epochs =  4  , current training accuracy =  96.89999999999999  %\n",
      "current epochs =  4  , current validation accuracy =  95.89999999999999  %\n",
      "\n",
      "epochs =  5 , step =  0 , current loss_val =  1.2411934001695646\n",
      "epochs =  5 , step =  1000 , current loss_val =  0.9781274159072134\n",
      "epochs =  5 , step =  2000 , current loss_val =  0.8445736154430543\n",
      "epochs =  5 , step =  3000 , current loss_val =  0.7438475667137462\n",
      "epochs =  5 , step =  4000 , current loss_val =  0.7662845793592947\n",
      "epochs =  5 , step =  5000 , current loss_val =  0.9108477404784676\n",
      "epochs =  5 , step =  6000 , current loss_val =  0.819023169040002\n",
      "epochs =  5 , step =  7000 , current loss_val =  0.8216320230142667\n",
      "epochs =  5 , step =  8000 , current loss_val =  0.7687082780502734\n",
      "epochs =  5 , step =  9000 , current loss_val =  0.9625171367383323\n",
      "epochs =  5 , step =  10000 , current loss_val =  1.3846748133981974\n",
      "epochs =  5 , step =  11000 , current loss_val =  0.8378696407930311\n",
      "epochs =  5 , step =  12000 , current loss_val =  0.8500648457363497\n",
      "epochs =  5 , step =  13000 , current loss_val =  0.7939689839733683\n",
      "epochs =  5 , step =  14000 , current loss_val =  0.7879609216441655\n",
      "epochs =  5 , step =  15000 , current loss_val =  0.7815063840780337\n",
      "epochs =  5 , step =  16000 , current loss_val =  0.8547021046036543\n",
      "epochs =  5 , step =  17000 , current loss_val =  1.025272256922987\n",
      "epochs =  5 , step =  18000 , current loss_val =  1.1570645403341309\n",
      "epochs =  5 , step =  19000 , current loss_val =  0.721927215979664\n",
      "epochs =  5 , step =  20000 , current loss_val =  0.7687707968410713\n",
      "epochs =  5 , step =  21000 , current loss_val =  0.9872641775154686\n",
      "epochs =  5 , step =  22000 , current loss_val =  0.7480982132258763\n",
      "epochs =  5 , step =  23000 , current loss_val =  0.8253613386149878\n",
      "epochs =  5 , step =  24000 , current loss_val =  0.8842640102895742\n",
      "epochs =  5 , step =  25000 , current loss_val =  1.0000926296923214\n",
      "epochs =  5 , step =  26000 , current loss_val =  0.8369168214454213\n",
      "epochs =  5 , step =  27000 , current loss_val =  0.8760496510120832\n",
      "epochs =  5 , step =  28000 , current loss_val =  0.845685558260929\n",
      "epochs =  5 , step =  29000 , current loss_val =  1.0123767218843382\n",
      "epochs =  5 , step =  30000 , current loss_val =  0.8346499164418137\n",
      "epochs =  5 , step =  31000 , current loss_val =  0.7646216963304883\n",
      "epochs =  5 , step =  32000 , current loss_val =  0.8392856405990106\n",
      "epochs =  5 , step =  33000 , current loss_val =  1.0304783337695225\n",
      "epochs =  5 , step =  34000 , current loss_val =  1.2142780474472512\n",
      "epochs =  5 , step =  35000 , current loss_val =  0.7470826144884181\n",
      "epochs =  5 , step =  36000 , current loss_val =  1.024820339637333\n",
      "epochs =  5 , step =  37000 , current loss_val =  0.7785867099126612\n",
      "epochs =  5 , step =  38000 , current loss_val =  0.8220012898863583\n",
      "epochs =  5 , step =  39000 , current loss_val =  0.7252973122232709\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epochs =  5 , step =  40000 , current loss_val =  1.1463347110993438\n",
      "epochs =  5 , step =  41000 , current loss_val =  0.807662511647612\n",
      "epochs =  5 , step =  42000 , current loss_val =  0.7559492172589899\n",
      "epochs =  5 , step =  43000 , current loss_val =  0.8118415528107983\n",
      "epochs =  5 , step =  44000 , current loss_val =  0.884317971890448\n",
      "epochs =  5 , step =  45000 , current loss_val =  0.8342294908773356\n",
      "epochs =  5 , step =  46000 , current loss_val =  0.8523742065469933\n",
      "epochs =  5 , step =  47000 , current loss_val =  0.9276289208827035\n",
      "\n",
      "current epochs =  5  , current training accuracy =  97.39999999999999  %\n",
      "current epochs =  5  , current validation accuracy =  96.2  %\n",
      "\n",
      "epochs =  6 , step =  0 , current loss_val =  1.1494815212771414\n",
      "epochs =  6 , step =  1000 , current loss_val =  1.011619637241783\n",
      "epochs =  6 , step =  2000 , current loss_val =  0.8442475881296471\n",
      "epochs =  6 , step =  3000 , current loss_val =  0.7432100855182817\n",
      "epochs =  6 , step =  4000 , current loss_val =  0.7822460070857153\n",
      "epochs =  6 , step =  5000 , current loss_val =  0.8973349582882826\n",
      "epochs =  6 , step =  6000 , current loss_val =  0.8124025842798279\n",
      "epochs =  6 , step =  7000 , current loss_val =  0.8361234129822082\n",
      "epochs =  6 , step =  8000 , current loss_val =  0.7755964036721428\n",
      "epochs =  6 , step =  9000 , current loss_val =  0.9483347944498783\n",
      "epochs =  6 , step =  10000 , current loss_val =  1.2303361343566204\n",
      "epochs =  6 , step =  11000 , current loss_val =  0.8296492439905516\n",
      "epochs =  6 , step =  12000 , current loss_val =  0.8730038448453834\n",
      "epochs =  6 , step =  13000 , current loss_val =  0.7973464706376717\n",
      "epochs =  6 , step =  14000 , current loss_val =  0.8018156891100603\n",
      "epochs =  6 , step =  15000 , current loss_val =  0.7943426097935596\n",
      "epochs =  6 , step =  16000 , current loss_val =  0.883087200459692\n",
      "epochs =  6 , step =  17000 , current loss_val =  1.0578312762992121\n",
      "epochs =  6 , step =  18000 , current loss_val =  1.0824039678022597\n",
      "epochs =  6 , step =  19000 , current loss_val =  0.7233014642148727\n",
      "epochs =  6 , step =  20000 , current loss_val =  0.7724623262360244\n",
      "epochs =  6 , step =  21000 , current loss_val =  0.9995090824022664\n",
      "epochs =  6 , step =  22000 , current loss_val =  0.7660888915122408\n",
      "epochs =  6 , step =  23000 , current loss_val =  0.8386932342358836\n",
      "epochs =  6 , step =  24000 , current loss_val =  0.8649750119075361\n",
      "epochs =  6 , step =  25000 , current loss_val =  0.9930405512608576\n",
      "epochs =  6 , step =  26000 , current loss_val =  0.8408474680040651\n",
      "epochs =  6 , step =  27000 , current loss_val =  0.918900467317543\n",
      "epochs =  6 , step =  28000 , current loss_val =  0.8493242997376849\n",
      "epochs =  6 , step =  29000 , current loss_val =  0.9808813739082454\n",
      "epochs =  6 , step =  30000 , current loss_val =  0.8259818848649232\n",
      "epochs =  6 , step =  31000 , current loss_val =  0.7702511801968623\n",
      "epochs =  6 , step =  32000 , current loss_val =  0.8505536942414587\n",
      "epochs =  6 , step =  33000 , current loss_val =  1.0413995642591005\n",
      "epochs =  6 , step =  34000 , current loss_val =  1.0921176861606423\n",
      "epochs =  6 , step =  35000 , current loss_val =  0.7499170583708704\n",
      "epochs =  6 , step =  36000 , current loss_val =  0.9869809853207675\n",
      "epochs =  6 , step =  37000 , current loss_val =  0.7964931788599705\n",
      "epochs =  6 , step =  38000 , current loss_val =  0.8257242024945913\n",
      "epochs =  6 , step =  39000 , current loss_val =  0.7356308348311984\n",
      "epochs =  6 , step =  40000 , current loss_val =  1.1218105970087466\n",
      "epochs =  6 , step =  41000 , current loss_val =  0.8163967689959616\n",
      "epochs =  6 , step =  42000 , current loss_val =  0.7716516520747776\n",
      "epochs =  6 , step =  43000 , current loss_val =  0.8220645225616549\n",
      "epochs =  6 , step =  44000 , current loss_val =  0.8842257581920783\n",
      "epochs =  6 , step =  45000 , current loss_val =  0.8449427876793051\n",
      "epochs =  6 , step =  46000 , current loss_val =  0.8668042768389366\n",
      "epochs =  6 , step =  47000 , current loss_val =  0.950437909275074\n",
      "\n",
      "current epochs =  6  , current training accuracy =  97.7  %\n",
      "current epochs =  6  , current validation accuracy =  96.39999999999999  %\n",
      "\n",
      "epochs =  7 , step =  0 , current loss_val =  1.0862931767080695\n",
      "epochs =  7 , step =  1000 , current loss_val =  1.0449184023182692\n",
      "epochs =  7 , step =  2000 , current loss_val =  0.8554997677273642\n",
      "epochs =  7 , step =  3000 , current loss_val =  0.7434911097671216\n",
      "epochs =  7 , step =  4000 , current loss_val =  0.7935638719478738\n",
      "epochs =  7 , step =  5000 , current loss_val =  0.9048765323583176\n",
      "epochs =  7 , step =  6000 , current loss_val =  0.8129197895734322\n",
      "epochs =  7 , step =  7000 , current loss_val =  0.8510231496218207\n",
      "epochs =  7 , step =  8000 , current loss_val =  0.7800896515400045\n",
      "epochs =  7 , step =  9000 , current loss_val =  0.9328832355827648\n",
      "epochs =  7 , step =  10000 , current loss_val =  1.0803627895910846\n",
      "epochs =  7 , step =  11000 , current loss_val =  0.8229885261277343\n",
      "epochs =  7 , step =  12000 , current loss_val =  0.897617230190834\n",
      "epochs =  7 , step =  13000 , current loss_val =  0.8061295908506545\n",
      "epochs =  7 , step =  14000 , current loss_val =  0.8166832222164246\n",
      "epochs =  7 , step =  15000 , current loss_val =  0.80933209782861\n",
      "epochs =  7 , step =  16000 , current loss_val =  0.9140003736644043\n",
      "epochs =  7 , step =  17000 , current loss_val =  1.0897023092071922\n",
      "epochs =  7 , step =  18000 , current loss_val =  1.043585266131198\n",
      "epochs =  7 , step =  19000 , current loss_val =  0.7336099711120073\n",
      "epochs =  7 , step =  20000 , current loss_val =  0.7753097556376128\n",
      "epochs =  7 , step =  21000 , current loss_val =  1.0158759978594256\n",
      "epochs =  7 , step =  22000 , current loss_val =  0.7818991940201934\n",
      "epochs =  7 , step =  23000 , current loss_val =  0.8545158315680121\n",
      "epochs =  7 , step =  24000 , current loss_val =  0.8663456549087332\n",
      "epochs =  7 , step =  25000 , current loss_val =  0.9977458076000935\n",
      "epochs =  7 , step =  26000 , current loss_val =  0.8512689708862935\n",
      "epochs =  7 , step =  27000 , current loss_val =  0.93347622380018\n",
      "epochs =  7 , step =  28000 , current loss_val =  0.8476862547200341\n",
      "epochs =  7 , step =  29000 , current loss_val =  0.9506556644237004\n",
      "epochs =  7 , step =  30000 , current loss_val =  0.8179180502597743\n",
      "epochs =  7 , step =  31000 , current loss_val =  0.7769783276778173\n",
      "epochs =  7 , step =  32000 , current loss_val =  0.8608915185888754\n",
      "epochs =  7 , step =  33000 , current loss_val =  1.0491927922169182\n",
      "epochs =  7 , step =  34000 , current loss_val =  1.014048064852696\n",
      "epochs =  7 , step =  35000 , current loss_val =  0.757792639197289\n",
      "epochs =  7 , step =  36000 , current loss_val =  0.9703671661370465\n",
      "epochs =  7 , step =  37000 , current loss_val =  0.8150355379141506\n",
      "epochs =  7 , step =  38000 , current loss_val =  0.8336479315822555\n",
      "epochs =  7 , step =  39000 , current loss_val =  0.7473582393376432\n",
      "epochs =  7 , step =  40000 , current loss_val =  1.1043861239911987\n",
      "epochs =  7 , step =  41000 , current loss_val =  0.8250582827355615\n",
      "epochs =  7 , step =  42000 , current loss_val =  0.7933604713076646\n",
      "epochs =  7 , step =  43000 , current loss_val =  0.831878779799639\n",
      "epochs =  7 , step =  44000 , current loss_val =  0.8910337859505397\n",
      "epochs =  7 , step =  45000 , current loss_val =  0.8596945965735132\n",
      "epochs =  7 , step =  46000 , current loss_val =  0.8767090883946478\n",
      "epochs =  7 , step =  47000 , current loss_val =  0.9743653812868337\n",
      "\n",
      "current epochs =  7  , current training accuracy =  97.89999999999999  %\n",
      "current epochs =  7  , current validation accuracy =  96.6  %\n",
      "\n",
      "epochs =  8 , step =  0 , current loss_val =  1.0360160770674958\n",
      "epochs =  8 , step =  1000 , current loss_val =  1.0725569416782053\n",
      "epochs =  8 , step =  2000 , current loss_val =  0.8761139319128644\n",
      "epochs =  8 , step =  3000 , current loss_val =  0.7498547452770149\n",
      "epochs =  8 , step =  4000 , current loss_val =  0.8017587749040707\n",
      "epochs =  8 , step =  5000 , current loss_val =  0.9329515898159567\n",
      "epochs =  8 , step =  6000 , current loss_val =  0.8140955417806169\n",
      "epochs =  8 , step =  7000 , current loss_val =  0.863654093736759\n",
      "epochs =  8 , step =  8000 , current loss_val =  0.7877655984069521\n",
      "epochs =  8 , step =  9000 , current loss_val =  0.9019243023876988\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epochs =  8 , step =  10000 , current loss_val =  1.0215107716528349\n",
      "epochs =  8 , step =  11000 , current loss_val =  0.8245870367250057\n",
      "epochs =  8 , step =  12000 , current loss_val =  0.9183863635707559\n",
      "epochs =  8 , step =  13000 , current loss_val =  0.816680269335911\n",
      "epochs =  8 , step =  14000 , current loss_val =  0.8348596783903557\n",
      "epochs =  8 , step =  15000 , current loss_val =  0.8219683529957431\n",
      "epochs =  8 , step =  16000 , current loss_val =  0.9450154943240878\n",
      "epochs =  8 , step =  17000 , current loss_val =  1.1243045633284154\n",
      "epochs =  8 , step =  18000 , current loss_val =  1.0375629690460666\n",
      "epochs =  8 , step =  19000 , current loss_val =  0.7425486739147134\n",
      "epochs =  8 , step =  20000 , current loss_val =  0.7820780742571505\n",
      "epochs =  8 , step =  21000 , current loss_val =  1.0338776232867959\n",
      "epochs =  8 , step =  22000 , current loss_val =  0.7998104134741135\n",
      "epochs =  8 , step =  23000 , current loss_val =  0.8666819111294009\n",
      "epochs =  8 , step =  24000 , current loss_val =  0.8785350344794518\n",
      "epochs =  8 , step =  25000 , current loss_val =  1.0021561605255502\n",
      "epochs =  8 , step =  26000 , current loss_val =  0.8660467224666927\n",
      "epochs =  8 , step =  27000 , current loss_val =  0.9400356678667824\n",
      "epochs =  8 , step =  28000 , current loss_val =  0.8455115123656965\n",
      "epochs =  8 , step =  29000 , current loss_val =  0.9392947246153364\n",
      "epochs =  8 , step =  30000 , current loss_val =  0.8185293483615905\n",
      "epochs =  8 , step =  31000 , current loss_val =  0.7832983454966735\n",
      "epochs =  8 , step =  32000 , current loss_val =  0.8751048830154151\n",
      "epochs =  8 , step =  33000 , current loss_val =  1.0503250774456878\n",
      "epochs =  8 , step =  34000 , current loss_val =  0.972636760523605\n",
      "epochs =  8 , step =  35000 , current loss_val =  0.7711436200138746\n",
      "epochs =  8 , step =  36000 , current loss_val =  0.964204369078811\n",
      "epochs =  8 , step =  37000 , current loss_val =  0.8297010053475427\n",
      "epochs =  8 , step =  38000 , current loss_val =  0.8410683593944562\n",
      "epochs =  8 , step =  39000 , current loss_val =  0.7599654207590768\n",
      "epochs =  8 , step =  40000 , current loss_val =  1.109487053219673\n",
      "epochs =  8 , step =  41000 , current loss_val =  0.8389261093196857\n",
      "epochs =  8 , step =  42000 , current loss_val =  0.8173712842603278\n",
      "epochs =  8 , step =  43000 , current loss_val =  0.8400081791130821\n",
      "epochs =  8 , step =  44000 , current loss_val =  0.8996916179083282\n",
      "epochs =  8 , step =  45000 , current loss_val =  0.8740378186884975\n",
      "epochs =  8 , step =  46000 , current loss_val =  0.886531045864383\n",
      "epochs =  8 , step =  47000 , current loss_val =  0.9976173872902411\n",
      "\n",
      "current epochs =  8  , current training accuracy =  98.1  %\n",
      "current epochs =  8  , current validation accuracy =  96.7  %\n",
      "\n",
      "epochs =  9 , step =  0 , current loss_val =  1.0082336426616612\n",
      "epochs =  9 , step =  1000 , current loss_val =  1.0887864813126447\n",
      "epochs =  9 , step =  2000 , current loss_val =  0.8978483012447376\n",
      "epochs =  9 , step =  3000 , current loss_val =  0.7598261892245073\n",
      "epochs =  9 , step =  4000 , current loss_val =  0.8029903732741928\n",
      "epochs =  9 , step =  5000 , current loss_val =  0.9473776649568425\n",
      "epochs =  9 , step =  6000 , current loss_val =  0.8176751771624384\n",
      "epochs =  9 , step =  7000 , current loss_val =  0.875472981364369\n",
      "epochs =  9 , step =  8000 , current loss_val =  0.7959672311778644\n",
      "epochs =  9 , step =  9000 , current loss_val =  0.8811157036756943\n",
      "epochs =  9 , step =  10000 , current loss_val =  1.0063427365904438\n",
      "epochs =  9 , step =  11000 , current loss_val =  0.8291703285950575\n",
      "epochs =  9 , step =  12000 , current loss_val =  0.9351852808921925\n",
      "epochs =  9 , step =  13000 , current loss_val =  0.8262072656305964\n",
      "epochs =  9 , step =  14000 , current loss_val =  0.8560251387905214\n",
      "epochs =  9 , step =  15000 , current loss_val =  0.8334984437103192\n",
      "epochs =  9 , step =  16000 , current loss_val =  0.971238408094737\n",
      "epochs =  9 , step =  17000 , current loss_val =  1.1573084692419742\n",
      "epochs =  9 , step =  18000 , current loss_val =  1.0251471384596553\n",
      "epochs =  9 , step =  19000 , current loss_val =  0.7507517477178692\n",
      "epochs =  9 , step =  20000 , current loss_val =  0.7900751439279017\n",
      "epochs =  9 , step =  21000 , current loss_val =  1.0513825707535018\n",
      "epochs =  9 , step =  22000 , current loss_val =  0.8162803557139116\n",
      "epochs =  9 , step =  23000 , current loss_val =  0.8737367124046436\n",
      "epochs =  9 , step =  24000 , current loss_val =  0.8969603906246036\n",
      "epochs =  9 , step =  25000 , current loss_val =  0.9996908706768851\n",
      "epochs =  9 , step =  26000 , current loss_val =  0.8803337850323089\n",
      "epochs =  9 , step =  27000 , current loss_val =  0.9420600203932966\n",
      "epochs =  9 , step =  28000 , current loss_val =  0.8476970340680097\n",
      "epochs =  9 , step =  29000 , current loss_val =  0.9366237137082601\n",
      "epochs =  9 , step =  30000 , current loss_val =  0.8259912140407519\n",
      "epochs =  9 , step =  31000 , current loss_val =  0.7899922544500588\n",
      "epochs =  9 , step =  32000 , current loss_val =  0.8877920417549587\n",
      "epochs =  9 , step =  33000 , current loss_val =  1.045038565991654\n",
      "epochs =  9 , step =  34000 , current loss_val =  0.9483998755469092\n",
      "epochs =  9 , step =  35000 , current loss_val =  0.7856976886243722\n",
      "epochs =  9 , step =  36000 , current loss_val =  0.9563511574394036\n",
      "epochs =  9 , step =  37000 , current loss_val =  0.8418569235117833\n",
      "epochs =  9 , step =  38000 , current loss_val =  0.8496196291557229\n",
      "epochs =  9 , step =  39000 , current loss_val =  0.7727826182177878\n",
      "epochs =  9 , step =  40000 , current loss_val =  1.120521862242224\n",
      "epochs =  9 , step =  41000 , current loss_val =  0.8546335775449488\n",
      "epochs =  9 , step =  42000 , current loss_val =  0.8341153824133886\n",
      "epochs =  9 , step =  43000 , current loss_val =  0.8483739876936932\n",
      "epochs =  9 , step =  44000 , current loss_val =  0.8987097026313174\n",
      "epochs =  9 , step =  45000 , current loss_val =  0.8877835402886103\n",
      "epochs =  9 , step =  46000 , current loss_val =  0.8947226214200116\n",
      "epochs =  9 , step =  47000 , current loss_val =  1.0187753694825374\n",
      "\n",
      "current epochs =  9  , current training accuracy =  98.3  %\n",
      "current epochs =  9  , current validation accuracy =  96.8  %\n",
      "\n",
      "epochs =  10 , step =  0 , current loss_val =  0.9912850115440309\n",
      "epochs =  10 , step =  1000 , current loss_val =  1.0848088751511604\n",
      "epochs =  10 , step =  2000 , current loss_val =  0.9196712579142862\n",
      "epochs =  10 , step =  3000 , current loss_val =  0.7685177856266479\n",
      "epochs =  10 , step =  4000 , current loss_val =  0.7959812207854567\n",
      "epochs =  10 , step =  5000 , current loss_val =  0.9431955841839564\n",
      "epochs =  10 , step =  6000 , current loss_val =  0.8251943263692817\n",
      "epochs =  10 , step =  7000 , current loss_val =  0.8875244733393017\n",
      "epochs =  10 , step =  8000 , current loss_val =  0.8036928492633091\n",
      "epochs =  10 , step =  9000 , current loss_val =  0.8804207941498078\n",
      "epochs =  10 , step =  10000 , current loss_val =  1.0187569859196979\n",
      "epochs =  10 , step =  11000 , current loss_val =  0.8324976878166565\n",
      "epochs =  10 , step =  12000 , current loss_val =  0.9497108059688687\n",
      "epochs =  10 , step =  13000 , current loss_val =  0.8347753782639509\n",
      "epochs =  10 , step =  14000 , current loss_val =  0.8768861217830757\n",
      "epochs =  10 , step =  15000 , current loss_val =  0.8461456164887046\n",
      "epochs =  10 , step =  16000 , current loss_val =  0.9909679706083836\n",
      "epochs =  10 , step =  17000 , current loss_val =  1.1806551380429557\n",
      "epochs =  10 , step =  18000 , current loss_val =  0.9986358894898612\n",
      "epochs =  10 , step =  19000 , current loss_val =  0.757304658257642\n",
      "epochs =  10 , step =  20000 , current loss_val =  0.7967968360818366\n",
      "epochs =  10 , step =  21000 , current loss_val =  1.0650289523483256\n",
      "epochs =  10 , step =  22000 , current loss_val =  0.8283736143247179\n",
      "epochs =  10 , step =  23000 , current loss_val =  0.8768214958245826\n",
      "epochs =  10 , step =  24000 , current loss_val =  0.912286930626448\n",
      "epochs =  10 , step =  25000 , current loss_val =  0.9763212672607468\n",
      "epochs =  10 , step =  26000 , current loss_val =  0.8890535325842224\n",
      "epochs =  10 , step =  27000 , current loss_val =  0.9376460903225886\n",
      "epochs =  10 , step =  28000 , current loss_val =  0.8553378607482621\n",
      "epochs =  10 , step =  29000 , current loss_val =  0.931121820106958\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epochs =  10 , step =  30000 , current loss_val =  0.8328741438997029\n",
      "epochs =  10 , step =  31000 , current loss_val =  0.7979634271836809\n",
      "epochs =  10 , step =  32000 , current loss_val =  0.8977929531790168\n",
      "epochs =  10 , step =  33000 , current loss_val =  1.0524438898447381\n",
      "epochs =  10 , step =  34000 , current loss_val =  0.9338466315374387\n",
      "epochs =  10 , step =  35000 , current loss_val =  0.80059946904673\n",
      "epochs =  10 , step =  36000 , current loss_val =  0.9484339720513403\n",
      "epochs =  10 , step =  37000 , current loss_val =  0.8510939489404139\n",
      "epochs =  10 , step =  38000 , current loss_val =  0.8594510203294375\n",
      "epochs =  10 , step =  39000 , current loss_val =  0.7856036803289819\n",
      "epochs =  10 , step =  40000 , current loss_val =  1.1306343435381458\n",
      "epochs =  10 , step =  41000 , current loss_val =  0.8686222549253484\n",
      "epochs =  10 , step =  42000 , current loss_val =  0.8468082602761013\n",
      "epochs =  10 , step =  43000 , current loss_val =  0.8560907271625207\n",
      "epochs =  10 , step =  44000 , current loss_val =  0.8976674269927835\n",
      "epochs =  10 , step =  45000 , current loss_val =  0.9044936637961523\n",
      "epochs =  10 , step =  46000 , current loss_val =  0.9022152459439796\n",
      "epochs =  10 , step =  47000 , current loss_val =  1.0377436719398652\n",
      "\n",
      "current epochs =  10  , current training accuracy =  98.4  %\n",
      "current epochs =  10  , current validation accuracy =  96.89999999999999  %\n",
      "\n",
      "epochs =  11 , step =  0 , current loss_val =  0.9833744854463654\n",
      "epochs =  11 , step =  1000 , current loss_val =  1.0658840439381156\n",
      "epochs =  11 , step =  2000 , current loss_val =  0.9390118733689793\n",
      "epochs =  11 , step =  3000 , current loss_val =  0.7743247191825127\n",
      "epochs =  11 , step =  4000 , current loss_val =  0.794443117640353\n",
      "epochs =  11 , step =  5000 , current loss_val =  0.936670970860231\n",
      "epochs =  11 , step =  6000 , current loss_val =  0.8349836354926814\n",
      "epochs =  11 , step =  7000 , current loss_val =  0.9007070281208462\n",
      "epochs =  11 , step =  8000 , current loss_val =  0.8098851679640434\n",
      "epochs =  11 , step =  9000 , current loss_val =  0.8910906732319379\n",
      "epochs =  11 , step =  10000 , current loss_val =  1.0318407152925366\n",
      "epochs =  11 , step =  11000 , current loss_val =  0.8372352679638003\n",
      "epochs =  11 , step =  12000 , current loss_val =  0.9619568255616237\n",
      "epochs =  11 , step =  13000 , current loss_val =  0.843996506666925\n",
      "epochs =  11 , step =  14000 , current loss_val =  0.8968664203073327\n",
      "epochs =  11 , step =  15000 , current loss_val =  0.8598869703089759\n",
      "epochs =  11 , step =  16000 , current loss_val =  1.0084689483524178\n",
      "epochs =  11 , step =  17000 , current loss_val =  1.1962529665075667\n",
      "epochs =  11 , step =  18000 , current loss_val =  0.9874957627798788\n",
      "epochs =  11 , step =  19000 , current loss_val =  0.7653889933477366\n",
      "epochs =  11 , step =  20000 , current loss_val =  0.8044020117117926\n",
      "epochs =  11 , step =  21000 , current loss_val =  1.0744835647229631\n",
      "epochs =  11 , step =  22000 , current loss_val =  0.8357623823932459\n",
      "epochs =  11 , step =  23000 , current loss_val =  0.8794517310939272\n",
      "epochs =  11 , step =  24000 , current loss_val =  0.9193989660217198\n",
      "epochs =  11 , step =  25000 , current loss_val =  0.9447516263254163\n",
      "epochs =  11 , step =  26000 , current loss_val =  0.8957588677222047\n",
      "epochs =  11 , step =  27000 , current loss_val =  0.9281158889888799\n",
      "epochs =  11 , step =  28000 , current loss_val =  0.8662962190049697\n",
      "epochs =  11 , step =  29000 , current loss_val =  0.9283759585501713\n",
      "epochs =  11 , step =  30000 , current loss_val =  0.8386786365070222\n",
      "epochs =  11 , step =  31000 , current loss_val =  0.8059313211893215\n",
      "epochs =  11 , step =  32000 , current loss_val =  0.9071804014267487\n",
      "epochs =  11 , step =  33000 , current loss_val =  1.0655907542003984\n",
      "epochs =  11 , step =  34000 , current loss_val =  0.9304960547794487\n",
      "epochs =  11 , step =  35000 , current loss_val =  0.8144900761585256\n",
      "epochs =  11 , step =  36000 , current loss_val =  0.936835679339138\n",
      "epochs =  11 , step =  37000 , current loss_val =  0.8590285255722263\n",
      "epochs =  11 , step =  38000 , current loss_val =  0.8695951466465413\n",
      "epochs =  11 , step =  39000 , current loss_val =  0.7973714890013479\n",
      "epochs =  11 , step =  40000 , current loss_val =  1.1359511521842538\n",
      "epochs =  11 , step =  41000 , current loss_val =  0.8784098813643182\n",
      "epochs =  11 , step =  42000 , current loss_val =  0.8610847507093387\n",
      "epochs =  11 , step =  43000 , current loss_val =  0.8651679491959671\n",
      "epochs =  11 , step =  44000 , current loss_val =  0.9057782460505507\n",
      "epochs =  11 , step =  45000 , current loss_val =  0.9224720071152781\n",
      "epochs =  11 , step =  46000 , current loss_val =  0.9105243440175326\n",
      "epochs =  11 , step =  47000 , current loss_val =  1.0576876917631968\n",
      "\n",
      "current epochs =  11  , current training accuracy =  98.5  %\n",
      "current epochs =  11  , current validation accuracy =  97.0  %\n",
      "\n",
      "epochs =  12 , step =  0 , current loss_val =  0.9788189661746882\n",
      "epochs =  12 , step =  1000 , current loss_val =  1.0548332970857948\n",
      "epochs =  12 , step =  2000 , current loss_val =  0.9550394057436321\n",
      "epochs =  12 , step =  3000 , current loss_val =  0.7788426638220532\n",
      "epochs =  12 , step =  4000 , current loss_val =  0.7976053948517224\n",
      "epochs =  12 , step =  5000 , current loss_val =  0.9267304119069136\n",
      "epochs =  12 , step =  6000 , current loss_val =  0.8452710908293112\n",
      "epochs =  12 , step =  7000 , current loss_val =  0.9125939830887413\n",
      "epochs =  12 , step =  8000 , current loss_val =  0.8152803881301008\n",
      "epochs =  12 , step =  9000 , current loss_val =  0.9070787802690663\n",
      "epochs =  12 , step =  10000 , current loss_val =  1.0295629474247803\n",
      "epochs =  12 , step =  11000 , current loss_val =  0.8436881244090259\n",
      "epochs =  12 , step =  12000 , current loss_val =  0.972221365343273\n",
      "epochs =  12 , step =  13000 , current loss_val =  0.8538920062567916\n",
      "epochs =  12 , step =  14000 , current loss_val =  0.9143641046324633\n",
      "epochs =  12 , step =  15000 , current loss_val =  0.873091683264823\n",
      "epochs =  12 , step =  16000 , current loss_val =  1.0252104978919918\n",
      "epochs =  12 , step =  17000 , current loss_val =  1.2056553155202312\n",
      "epochs =  12 , step =  18000 , current loss_val =  0.9884733377975593\n",
      "epochs =  12 , step =  19000 , current loss_val =  0.773572490284185\n",
      "epochs =  12 , step =  20000 , current loss_val =  0.8136669935588471\n",
      "epochs =  12 , step =  21000 , current loss_val =  1.0800479775215304\n",
      "epochs =  12 , step =  22000 , current loss_val =  0.842541725739848\n",
      "epochs =  12 , step =  23000 , current loss_val =  0.884921161451332\n",
      "epochs =  12 , step =  24000 , current loss_val =  0.9236586030476691\n",
      "epochs =  12 , step =  25000 , current loss_val =  0.9333443687030935\n",
      "epochs =  12 , step =  26000 , current loss_val =  0.9029679206649226\n",
      "epochs =  12 , step =  27000 , current loss_val =  0.9222918751199418\n",
      "epochs =  12 , step =  28000 , current loss_val =  0.8787932223643352\n",
      "epochs =  12 , step =  29000 , current loss_val =  0.9302643051544881\n",
      "epochs =  12 , step =  30000 , current loss_val =  0.8452665691764712\n",
      "epochs =  12 , step =  31000 , current loss_val =  0.8129511572906795\n",
      "epochs =  12 , step =  32000 , current loss_val =  0.9157563379544923\n",
      "epochs =  12 , step =  33000 , current loss_val =  1.0780325671743671\n",
      "epochs =  12 , step =  34000 , current loss_val =  0.933451651678052\n",
      "epochs =  12 , step =  35000 , current loss_val =  0.8267007615682921\n",
      "epochs =  12 , step =  36000 , current loss_val =  0.9191827972315764\n",
      "epochs =  12 , step =  37000 , current loss_val =  0.8668602895089118\n",
      "epochs =  12 , step =  38000 , current loss_val =  0.8787582079865383\n",
      "epochs =  12 , step =  39000 , current loss_val =  0.8090599650690165\n",
      "epochs =  12 , step =  40000 , current loss_val =  1.1393125209191752\n",
      "epochs =  12 , step =  41000 , current loss_val =  0.884586330724503\n",
      "epochs =  12 , step =  42000 , current loss_val =  0.8773216147567695\n",
      "epochs =  12 , step =  43000 , current loss_val =  0.876217619633948\n",
      "epochs =  12 , step =  44000 , current loss_val =  0.9189334646538404\n",
      "epochs =  12 , step =  45000 , current loss_val =  0.936917739984601\n",
      "epochs =  12 , step =  46000 , current loss_val =  0.9192203107869403\n",
      "epochs =  12 , step =  47000 , current loss_val =  1.079176968669187\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "current epochs =  12  , current training accuracy =  98.6  %\n",
      "current epochs =  12  , current validation accuracy =  97.0  %\n",
      "\n",
      "epochs =  13 , step =  0 , current loss_val =  0.974264145647317\n",
      "epochs =  13 , step =  1000 , current loss_val =  1.0567656196208974\n",
      "epochs =  13 , step =  2000 , current loss_val =  0.9687388566781897\n",
      "epochs =  13 , step =  3000 , current loss_val =  0.7836470592014017\n",
      "epochs =  13 , step =  4000 , current loss_val =  0.80110206659197\n",
      "epochs =  13 , step =  5000 , current loss_val =  0.9131511654521479\n",
      "epochs =  13 , step =  6000 , current loss_val =  0.8521703621772565\n",
      "epochs =  13 , step =  7000 , current loss_val =  0.9224721472812242\n",
      "epochs =  13 , step =  8000 , current loss_val =  0.8212628746402362\n",
      "epochs =  13 , step =  9000 , current loss_val =  0.9233131220139716\n",
      "epochs =  13 , step =  10000 , current loss_val =  1.0180406563889346\n",
      "epochs =  13 , step =  11000 , current loss_val =  0.8511193173317761\n",
      "epochs =  13 , step =  12000 , current loss_val =  0.9788637606981041\n",
      "epochs =  13 , step =  13000 , current loss_val =  0.8635331838807463\n",
      "epochs =  13 , step =  14000 , current loss_val =  0.9271145605470396\n",
      "epochs =  13 , step =  15000 , current loss_val =  0.8850554761093635\n",
      "epochs =  13 , step =  16000 , current loss_val =  1.0411229128976691\n",
      "epochs =  13 , step =  17000 , current loss_val =  1.2091770959327184\n",
      "epochs =  13 , step =  18000 , current loss_val =  0.9916453449839854\n",
      "epochs =  13 , step =  19000 , current loss_val =  0.7804445840353716\n",
      "epochs =  13 , step =  20000 , current loss_val =  0.8219706294774555\n",
      "epochs =  13 , step =  21000 , current loss_val =  1.0832655610542659\n",
      "epochs =  13 , step =  22000 , current loss_val =  0.8493200198273372\n",
      "epochs =  13 , step =  23000 , current loss_val =  0.893246536670187\n",
      "epochs =  13 , step =  24000 , current loss_val =  0.9339149427561794\n",
      "epochs =  13 , step =  25000 , current loss_val =  0.9292383316235787\n",
      "epochs =  13 , step =  26000 , current loss_val =  0.9124584649247127\n",
      "epochs =  13 , step =  27000 , current loss_val =  0.9240762029709463\n",
      "epochs =  13 , step =  28000 , current loss_val =  0.8916197040842012\n",
      "epochs =  13 , step =  29000 , current loss_val =  0.9356078290953533\n",
      "epochs =  13 , step =  30000 , current loss_val =  0.8523171926504686\n",
      "epochs =  13 , step =  31000 , current loss_val =  0.8196365796444918\n",
      "epochs =  13 , step =  32000 , current loss_val =  0.9225260252959093\n",
      "epochs =  13 , step =  33000 , current loss_val =  1.0898302484331903\n",
      "epochs =  13 , step =  34000 , current loss_val =  0.9405942167888716\n",
      "epochs =  13 , step =  35000 , current loss_val =  0.8379332040268129\n",
      "epochs =  13 , step =  36000 , current loss_val =  0.8991603320080228\n",
      "epochs =  13 , step =  37000 , current loss_val =  0.8744115985686797\n",
      "epochs =  13 , step =  38000 , current loss_val =  0.8880654217996985\n",
      "epochs =  13 , step =  39000 , current loss_val =  0.8213446447706096\n",
      "epochs =  13 , step =  40000 , current loss_val =  1.139802670802245\n",
      "epochs =  13 , step =  41000 , current loss_val =  0.8910407092411035\n",
      "epochs =  13 , step =  42000 , current loss_val =  0.893601293785061\n",
      "epochs =  13 , step =  43000 , current loss_val =  0.8881806058390401\n",
      "epochs =  13 , step =  44000 , current loss_val =  0.9326978395063177\n",
      "epochs =  13 , step =  45000 , current loss_val =  0.9470596466381416\n",
      "epochs =  13 , step =  46000 , current loss_val =  0.9267795575539483\n",
      "epochs =  13 , step =  47000 , current loss_val =  1.09945732283105\n",
      "\n",
      "current epochs =  13  , current training accuracy =  98.7  %\n",
      "current epochs =  13  , current validation accuracy =  97.1  %\n",
      "\n",
      "epochs =  14 , step =  0 , current loss_val =  0.9667215932369393\n",
      "epochs =  14 , step =  1000 , current loss_val =  1.068712528654129\n",
      "epochs =  14 , step =  2000 , current loss_val =  0.9808902936016743\n",
      "epochs =  14 , step =  3000 , current loss_val =  0.7894080888359477\n",
      "epochs =  14 , step =  4000 , current loss_val =  0.8052417754939483\n",
      "epochs =  14 , step =  5000 , current loss_val =  0.9068071682806722\n",
      "epochs =  14 , step =  6000 , current loss_val =  0.8565911660036627\n",
      "epochs =  14 , step =  7000 , current loss_val =  0.9315162315241117\n",
      "epochs =  14 , step =  8000 , current loss_val =  0.8291621251535399\n",
      "epochs =  14 , step =  9000 , current loss_val =  0.9376180031966432\n",
      "epochs =  14 , step =  10000 , current loss_val =  1.0051216663995184\n",
      "epochs =  14 , step =  11000 , current loss_val =  0.8587750826519126\n",
      "epochs =  14 , step =  12000 , current loss_val =  0.9838151311327101\n",
      "epochs =  14 , step =  13000 , current loss_val =  0.8735281310441924\n",
      "epochs =  14 , step =  14000 , current loss_val =  0.9378591264955001\n",
      "epochs =  14 , step =  15000 , current loss_val =  0.8983783760941982\n",
      "epochs =  14 , step =  16000 , current loss_val =  1.0558619551328137\n",
      "epochs =  14 , step =  17000 , current loss_val =  1.2102314957775155\n",
      "epochs =  14 , step =  18000 , current loss_val =  0.9935040511536365\n",
      "epochs =  14 , step =  19000 , current loss_val =  0.7870693747480344\n",
      "epochs =  14 , step =  20000 , current loss_val =  0.8300976492135292\n",
      "epochs =  14 , step =  21000 , current loss_val =  1.0872704573260492\n",
      "epochs =  14 , step =  22000 , current loss_val =  0.8546042444781744\n",
      "epochs =  14 , step =  23000 , current loss_val =  0.9024961566471925\n",
      "epochs =  14 , step =  24000 , current loss_val =  0.9410598777601261\n",
      "epochs =  14 , step =  25000 , current loss_val =  0.927314848652039\n",
      "epochs =  14 , step =  26000 , current loss_val =  0.9228553283128244\n",
      "epochs =  14 , step =  27000 , current loss_val =  0.931475132419797\n",
      "epochs =  14 , step =  28000 , current loss_val =  0.9055949435201418\n",
      "epochs =  14 , step =  29000 , current loss_val =  0.9421902408129225\n",
      "epochs =  14 , step =  30000 , current loss_val =  0.858897681049194\n",
      "epochs =  14 , step =  31000 , current loss_val =  0.8268786338220769\n",
      "epochs =  14 , step =  32000 , current loss_val =  0.9275499636242654\n",
      "epochs =  14 , step =  33000 , current loss_val =  1.1003896418157764\n",
      "epochs =  14 , step =  34000 , current loss_val =  0.9508445291115788\n",
      "epochs =  14 , step =  35000 , current loss_val =  0.8487383815274265\n",
      "epochs =  14 , step =  36000 , current loss_val =  0.8845989924811941\n",
      "epochs =  14 , step =  37000 , current loss_val =  0.8813955360247873\n",
      "epochs =  14 , step =  38000 , current loss_val =  0.8974425622004837\n",
      "epochs =  14 , step =  39000 , current loss_val =  0.8324158575193168\n",
      "epochs =  14 , step =  40000 , current loss_val =  1.1356727475849104\n",
      "epochs =  14 , step =  41000 , current loss_val =  0.8965953607636812\n",
      "epochs =  14 , step =  42000 , current loss_val =  0.9086375192006795\n",
      "epochs =  14 , step =  43000 , current loss_val =  0.9007578614353947\n",
      "epochs =  14 , step =  44000 , current loss_val =  0.9425501505154396\n",
      "epochs =  14 , step =  45000 , current loss_val =  0.9543378208247862\n",
      "epochs =  14 , step =  46000 , current loss_val =  0.9328237182408685\n",
      "epochs =  14 , step =  47000 , current loss_val =  1.117780874844013\n",
      "\n",
      "current epochs =  14  , current training accuracy =  98.8  %\n",
      "current epochs =  14  , current validation accuracy =  97.2  %\n",
      "\n",
      "epochs =  15 , step =  0 , current loss_val =  0.9564636625961109\n",
      "epochs =  15 , step =  1000 , current loss_val =  1.0809970326402094\n",
      "epochs =  15 , step =  2000 , current loss_val =  0.9912458046560682\n",
      "epochs =  15 , step =  3000 , current loss_val =  0.7960764924879387\n",
      "epochs =  15 , step =  4000 , current loss_val =  0.8099099125516372\n",
      "epochs =  15 , step =  5000 , current loss_val =  0.9017862128704252\n",
      "epochs =  15 , step =  6000 , current loss_val =  0.8592292554068303\n",
      "epochs =  15 , step =  7000 , current loss_val =  0.9408422246406911\n",
      "epochs =  15 , step =  8000 , current loss_val =  0.8384494420766563\n",
      "epochs =  15 , step =  9000 , current loss_val =  0.9514570434426592\n",
      "epochs =  15 , step =  10000 , current loss_val =  0.993702088310947\n",
      "epochs =  15 , step =  11000 , current loss_val =  0.8659003148898985\n",
      "epochs =  15 , step =  12000 , current loss_val =  0.9882638204127195\n",
      "epochs =  15 , step =  13000 , current loss_val =  0.8843756131864225\n",
      "epochs =  15 , step =  14000 , current loss_val =  0.9490759244219238\n",
      "epochs =  15 , step =  15000 , current loss_val =  0.9141025400858122\n",
      "epochs =  15 , step =  16000 , current loss_val =  1.0686302169025625\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epochs =  15 , step =  17000 , current loss_val =  1.2088853237945765\n",
      "epochs =  15 , step =  18000 , current loss_val =  0.9967645130585336\n",
      "epochs =  15 , step =  19000 , current loss_val =  0.796262125263438\n",
      "epochs =  15 , step =  20000 , current loss_val =  0.8389265051226555\n",
      "epochs =  15 , step =  21000 , current loss_val =  1.0937198121902427\n",
      "epochs =  15 , step =  22000 , current loss_val =  0.8575937960193294\n",
      "epochs =  15 , step =  23000 , current loss_val =  0.9091144054414299\n",
      "epochs =  15 , step =  24000 , current loss_val =  0.9416165517302463\n",
      "epochs =  15 , step =  25000 , current loss_val =  0.9294501393482447\n",
      "epochs =  15 , step =  26000 , current loss_val =  0.9344120585120812\n",
      "epochs =  15 , step =  27000 , current loss_val =  0.9414404126133383\n",
      "epochs =  15 , step =  28000 , current loss_val =  0.9163731012097087\n",
      "epochs =  15 , step =  29000 , current loss_val =  0.9474582821160118\n",
      "epochs =  15 , step =  30000 , current loss_val =  0.8661178037292229\n",
      "epochs =  15 , step =  31000 , current loss_val =  0.8351151145123478\n",
      "epochs =  15 , step =  32000 , current loss_val =  0.9320069106449447\n",
      "epochs =  15 , step =  33000 , current loss_val =  1.11076948664169\n",
      "epochs =  15 , step =  34000 , current loss_val =  0.9629510871806899\n",
      "epochs =  15 , step =  35000 , current loss_val =  0.8594124183843099\n",
      "epochs =  15 , step =  36000 , current loss_val =  0.8744318138369893\n",
      "epochs =  15 , step =  37000 , current loss_val =  0.8880417274869041\n",
      "epochs =  15 , step =  38000 , current loss_val =  0.9056129090462647\n",
      "epochs =  15 , step =  39000 , current loss_val =  0.8421445459461969\n",
      "epochs =  15 , step =  40000 , current loss_val =  1.1266063650310414\n",
      "epochs =  15 , step =  41000 , current loss_val =  0.9007976097835722\n",
      "epochs =  15 , step =  42000 , current loss_val =  0.9211936283308524\n",
      "epochs =  15 , step =  43000 , current loss_val =  0.9133166951453214\n",
      "epochs =  15 , step =  44000 , current loss_val =  0.9463788477124756\n",
      "epochs =  15 , step =  45000 , current loss_val =  0.9596924563899075\n",
      "epochs =  15 , step =  46000 , current loss_val =  0.9371168515773394\n",
      "epochs =  15 , step =  47000 , current loss_val =  1.1341341047183344\n",
      "\n",
      "current epochs =  15  , current training accuracy =  98.9  %\n",
      "current epochs =  15  , current validation accuracy =  97.3  %\n",
      "\n",
      "epochs =  16 , step =  0 , current loss_val =  0.9473877824956622\n",
      "epochs =  16 , step =  1000 , current loss_val =  1.0933679521492332\n",
      "epochs =  16 , step =  2000 , current loss_val =  1.0002565299884343\n",
      "epochs =  16 , step =  3000 , current loss_val =  0.802937621160652\n",
      "epochs =  16 , step =  4000 , current loss_val =  0.8142780199191346\n",
      "epochs =  16 , step =  5000 , current loss_val =  0.8999831665925344\n",
      "epochs =  16 , step =  6000 , current loss_val =  0.8621253383550525\n",
      "epochs =  16 , step =  7000 , current loss_val =  0.9512808882333142\n",
      "epochs =  16 , step =  8000 , current loss_val =  0.8467685948545142\n",
      "epochs =  16 , step =  9000 , current loss_val =  0.966339205190221\n",
      "epochs =  16 , step =  10000 , current loss_val =  0.9858999416118439\n",
      "epochs =  16 , step =  11000 , current loss_val =  0.8723997921938181\n",
      "epochs =  16 , step =  12000 , current loss_val =  0.9947581179138477\n",
      "epochs =  16 , step =  13000 , current loss_val =  0.8938864321572892\n",
      "epochs =  16 , step =  14000 , current loss_val =  0.9602022789630842\n",
      "epochs =  16 , step =  15000 , current loss_val =  0.9301464205939833\n",
      "epochs =  16 , step =  16000 , current loss_val =  1.0794741354732669\n",
      "epochs =  16 , step =  17000 , current loss_val =  1.2095636502373386\n",
      "epochs =  16 , step =  18000 , current loss_val =  1.0032726148776163\n",
      "epochs =  16 , step =  19000 , current loss_val =  0.8064007639108226\n",
      "epochs =  16 , step =  20000 , current loss_val =  0.8473546012657736\n",
      "epochs =  16 , step =  21000 , current loss_val =  1.101164806085197\n",
      "epochs =  16 , step =  22000 , current loss_val =  0.8585103928484668\n",
      "epochs =  16 , step =  23000 , current loss_val =  0.912110163386274\n",
      "epochs =  16 , step =  24000 , current loss_val =  0.9440370157066249\n",
      "epochs =  16 , step =  25000 , current loss_val =  0.9344558695919538\n",
      "epochs =  16 , step =  26000 , current loss_val =  0.9482911270945028\n",
      "epochs =  16 , step =  27000 , current loss_val =  0.948351565986988\n",
      "epochs =  16 , step =  28000 , current loss_val =  0.9248150278619945\n",
      "epochs =  16 , step =  29000 , current loss_val =  0.9518399725241433\n",
      "epochs =  16 , step =  30000 , current loss_val =  0.8740070334179462\n",
      "epochs =  16 , step =  31000 , current loss_val =  0.8439857691080744\n",
      "epochs =  16 , step =  32000 , current loss_val =  0.9358693056877694\n",
      "epochs =  16 , step =  33000 , current loss_val =  1.1211688706831633\n",
      "epochs =  16 , step =  34000 , current loss_val =  0.9763470581695222\n",
      "epochs =  16 , step =  35000 , current loss_val =  0.8695896566287507\n",
      "epochs =  16 , step =  36000 , current loss_val =  0.866283618456986\n",
      "epochs =  16 , step =  37000 , current loss_val =  0.895042128465816\n",
      "epochs =  16 , step =  38000 , current loss_val =  0.9130457251476211\n",
      "epochs =  16 , step =  39000 , current loss_val =  0.8509172262364096\n",
      "epochs =  16 , step =  40000 , current loss_val =  1.115833995518399\n",
      "epochs =  16 , step =  41000 , current loss_val =  0.9044234239699325\n",
      "epochs =  16 , step =  42000 , current loss_val =  0.9307784299225812\n",
      "epochs =  16 , step =  43000 , current loss_val =  0.9247413901731347\n",
      "epochs =  16 , step =  44000 , current loss_val =  0.947338787186415\n",
      "epochs =  16 , step =  45000 , current loss_val =  0.9641980532590951\n",
      "epochs =  16 , step =  46000 , current loss_val =  0.940194918790948\n",
      "epochs =  16 , step =  47000 , current loss_val =  1.147422439648671\n",
      "\n",
      "current epochs =  16  , current training accuracy =  99.0  %\n",
      "current epochs =  16  , current validation accuracy =  97.3  %\n",
      "\n",
      "epochs =  17 , step =  0 , current loss_val =  0.9416315392607276\n",
      "epochs =  17 , step =  1000 , current loss_val =  1.1024878039775625\n",
      "epochs =  17 , step =  2000 , current loss_val =  1.0073553771877488\n",
      "epochs =  17 , step =  3000 , current loss_val =  0.8088621852887062\n",
      "epochs =  17 , step =  4000 , current loss_val =  0.8163475423879244\n",
      "epochs =  17 , step =  5000 , current loss_val =  0.9028448129948458\n",
      "epochs =  17 , step =  6000 , current loss_val =  0.8664214256391786\n",
      "epochs =  17 , step =  7000 , current loss_val =  0.9619498773472641\n",
      "epochs =  17 , step =  8000 , current loss_val =  0.8530305067233658\n",
      "epochs =  17 , step =  9000 , current loss_val =  0.9796713783845519\n",
      "epochs =  17 , step =  10000 , current loss_val =  0.9817094286073557\n",
      "epochs =  17 , step =  11000 , current loss_val =  0.8795241800274067\n",
      "epochs =  17 , step =  12000 , current loss_val =  1.0038250520921344\n",
      "epochs =  17 , step =  13000 , current loss_val =  0.9012494005659777\n",
      "epochs =  17 , step =  14000 , current loss_val =  0.9705235474805636\n",
      "epochs =  17 , step =  15000 , current loss_val =  0.9451357318148884\n",
      "epochs =  17 , step =  16000 , current loss_val =  1.0883372729710008\n",
      "epochs =  17 , step =  17000 , current loss_val =  1.2182398786535578\n",
      "epochs =  17 , step =  18000 , current loss_val =  1.0109102943792987\n",
      "epochs =  17 , step =  19000 , current loss_val =  0.8136507755784105\n",
      "epochs =  17 , step =  20000 , current loss_val =  0.8551665796348095\n",
      "epochs =  17 , step =  21000 , current loss_val =  1.108326815355997\n",
      "epochs =  17 , step =  22000 , current loss_val =  0.8582968873386313\n",
      "epochs =  17 , step =  23000 , current loss_val =  0.9139270603121649\n",
      "epochs =  17 , step =  24000 , current loss_val =  0.9470948242196249\n",
      "epochs =  17 , step =  25000 , current loss_val =  0.9427672286877008\n",
      "epochs =  17 , step =  26000 , current loss_val =  0.9639659103850721\n",
      "epochs =  17 , step =  27000 , current loss_val =  0.9542036358353831\n",
      "epochs =  17 , step =  28000 , current loss_val =  0.93111398252666\n",
      "epochs =  17 , step =  29000 , current loss_val =  0.9573168060198703\n",
      "epochs =  17 , step =  30000 , current loss_val =  0.8828125516551137\n",
      "epochs =  17 , step =  31000 , current loss_val =  0.8522381303285614\n",
      "epochs =  17 , step =  32000 , current loss_val =  0.9394194051079052\n",
      "epochs =  17 , step =  33000 , current loss_val =  1.1325587406600934\n",
      "epochs =  17 , step =  34000 , current loss_val =  0.9895896012352426\n",
      "epochs =  17 , step =  35000 , current loss_val =  0.8789607894876027\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epochs =  17 , step =  36000 , current loss_val =  0.8609272986762012\n",
      "epochs =  17 , step =  37000 , current loss_val =  0.9025911159184308\n",
      "epochs =  17 , step =  38000 , current loss_val =  0.9213908224229639\n",
      "epochs =  17 , step =  39000 , current loss_val =  0.8590378864482572\n",
      "epochs =  17 , step =  40000 , current loss_val =  1.1059365130934953\n",
      "epochs =  17 , step =  41000 , current loss_val =  0.9081682937092923\n",
      "epochs =  17 , step =  42000 , current loss_val =  0.9385113881928124\n",
      "epochs =  17 , step =  43000 , current loss_val =  0.9346906549329737\n",
      "epochs =  17 , step =  44000 , current loss_val =  0.9496704843820568\n",
      "epochs =  17 , step =  45000 , current loss_val =  0.969555477370457\n",
      "epochs =  17 , step =  46000 , current loss_val =  0.9437802259664427\n",
      "epochs =  17 , step =  47000 , current loss_val =  1.159116270186715\n",
      "\n",
      "current epochs =  17  , current training accuracy =  99.0  %\n",
      "current epochs =  17  , current validation accuracy =  97.3  %\n",
      "\n",
      "epochs =  18 , step =  0 , current loss_val =  0.9412919648160826\n",
      "epochs =  18 , step =  1000 , current loss_val =  1.1085733993634044\n",
      "epochs =  18 , step =  2000 , current loss_val =  1.0134658128076068\n",
      "epochs =  18 , step =  3000 , current loss_val =  0.8144925859264134\n",
      "epochs =  18 , step =  4000 , current loss_val =  0.8167179585359007\n",
      "epochs =  18 , step =  5000 , current loss_val =  0.906439099016333\n",
      "epochs =  18 , step =  6000 , current loss_val =  0.8725674585632557\n",
      "epochs =  18 , step =  7000 , current loss_val =  0.9699910212461029\n",
      "epochs =  18 , step =  8000 , current loss_val =  0.8575729450720584\n",
      "epochs =  18 , step =  9000 , current loss_val =  0.9912242963465372\n",
      "epochs =  18 , step =  10000 , current loss_val =  0.9812401436908914\n",
      "epochs =  18 , step =  11000 , current loss_val =  0.8872195274630712\n",
      "epochs =  18 , step =  12000 , current loss_val =  1.0124488475456659\n",
      "epochs =  18 , step =  13000 , current loss_val =  0.9069088113217327\n",
      "epochs =  18 , step =  14000 , current loss_val =  0.9806929855233942\n",
      "epochs =  18 , step =  15000 , current loss_val =  0.9589432779829757\n",
      "epochs =  18 , step =  16000 , current loss_val =  1.0961272520752208\n",
      "epochs =  18 , step =  17000 , current loss_val =  1.2296179511012661\n",
      "epochs =  18 , step =  18000 , current loss_val =  1.0195633069061842\n",
      "epochs =  18 , step =  19000 , current loss_val =  0.8200045143671113\n",
      "epochs =  18 , step =  20000 , current loss_val =  0.8639801954657172\n",
      "epochs =  18 , step =  21000 , current loss_val =  1.1165780218079009\n",
      "epochs =  18 , step =  22000 , current loss_val =  0.8581969941384792\n",
      "epochs =  18 , step =  23000 , current loss_val =  0.9175341198628754\n",
      "epochs =  18 , step =  24000 , current loss_val =  0.9513010395412311\n",
      "epochs =  18 , step =  25000 , current loss_val =  0.9531302908514337\n",
      "epochs =  18 , step =  26000 , current loss_val =  0.9795859624486933\n",
      "epochs =  18 , step =  27000 , current loss_val =  0.9596866549594463\n",
      "epochs =  18 , step =  28000 , current loss_val =  0.9348043244625027\n",
      "epochs =  18 , step =  29000 , current loss_val =  0.9636101512759423\n",
      "epochs =  18 , step =  30000 , current loss_val =  0.8928290148187149\n",
      "epochs =  18 , step =  31000 , current loss_val =  0.8596008822976798\n",
      "epochs =  18 , step =  32000 , current loss_val =  0.9443781647374426\n",
      "epochs =  18 , step =  33000 , current loss_val =  1.1434741696516515\n",
      "epochs =  18 , step =  34000 , current loss_val =  1.0017567124167774\n",
      "epochs =  18 , step =  35000 , current loss_val =  0.8880970795573295\n",
      "epochs =  18 , step =  36000 , current loss_val =  0.8589879971060833\n",
      "epochs =  18 , step =  37000 , current loss_val =  0.9103705883034248\n",
      "epochs =  18 , step =  38000 , current loss_val =  0.9309235717937929\n",
      "epochs =  18 , step =  39000 , current loss_val =  0.8665432189892095\n",
      "epochs =  18 , step =  40000 , current loss_val =  1.0960418997137225\n",
      "epochs =  18 , step =  41000 , current loss_val =  0.9123975029473663\n",
      "epochs =  18 , step =  42000 , current loss_val =  0.9443205737141428\n",
      "epochs =  18 , step =  43000 , current loss_val =  0.9435424936186825\n",
      "epochs =  18 , step =  44000 , current loss_val =  0.9549774148492338\n",
      "epochs =  18 , step =  45000 , current loss_val =  0.9782996112990177\n",
      "epochs =  18 , step =  46000 , current loss_val =  0.9508307325675599\n",
      "epochs =  18 , step =  47000 , current loss_val =  1.169408245030423\n",
      "\n",
      "current epochs =  18  , current training accuracy =  99.1  %\n",
      "current epochs =  18  , current validation accuracy =  97.3  %\n",
      "\n",
      "epochs =  19 , step =  0 , current loss_val =  0.9454203973515383\n",
      "epochs =  19 , step =  1000 , current loss_val =  1.114168332634705\n",
      "epochs =  19 , step =  2000 , current loss_val =  1.0205239452072383\n",
      "epochs =  19 , step =  3000 , current loss_val =  0.8197267026412548\n",
      "epochs =  19 , step =  4000 , current loss_val =  0.8170834289420406\n",
      "epochs =  19 , step =  5000 , current loss_val =  0.9097628755663126\n",
      "epochs =  19 , step =  6000 , current loss_val =  0.8811133386421838\n",
      "epochs =  19 , step =  7000 , current loss_val =  0.976493588034244\n",
      "epochs =  19 , step =  8000 , current loss_val =  0.8610997865459962\n",
      "epochs =  19 , step =  9000 , current loss_val =  1.00041253546128\n",
      "epochs =  19 , step =  10000 , current loss_val =  0.9845937392906999\n",
      "epochs =  19 , step =  11000 , current loss_val =  0.8948362935663516\n",
      "epochs =  19 , step =  12000 , current loss_val =  1.0200805047203145\n",
      "epochs =  19 , step =  13000 , current loss_val =  0.9125259999120805\n",
      "epochs =  19 , step =  14000 , current loss_val =  0.9908593047479182\n",
      "epochs =  19 , step =  15000 , current loss_val =  0.9722320619381183\n",
      "epochs =  19 , step =  16000 , current loss_val =  1.1056213457358104\n",
      "epochs =  19 , step =  17000 , current loss_val =  1.2361928406364773\n",
      "epochs =  19 , step =  18000 , current loss_val =  1.032139433097622\n",
      "epochs =  19 , step =  19000 , current loss_val =  0.8265035513560317\n",
      "epochs =  19 , step =  20000 , current loss_val =  0.8732569625420123\n",
      "epochs =  19 , step =  21000 , current loss_val =  1.1256178603744518\n",
      "epochs =  19 , step =  22000 , current loss_val =  0.8593440079139667\n",
      "epochs =  19 , step =  23000 , current loss_val =  0.9232897679845067\n",
      "epochs =  19 , step =  24000 , current loss_val =  0.9564379506733353\n",
      "epochs =  19 , step =  25000 , current loss_val =  0.9634950267202612\n",
      "epochs =  19 , step =  26000 , current loss_val =  0.9937597957926403\n",
      "epochs =  19 , step =  27000 , current loss_val =  0.9651781692067568\n",
      "epochs =  19 , step =  28000 , current loss_val =  0.9379566076842583\n",
      "epochs =  19 , step =  29000 , current loss_val =  0.9691358640093177\n",
      "epochs =  19 , step =  30000 , current loss_val =  0.9029579248668478\n",
      "epochs =  19 , step =  31000 , current loss_val =  0.8658780794705241\n",
      "epochs =  19 , step =  32000 , current loss_val =  0.947010558890783\n",
      "epochs =  19 , step =  33000 , current loss_val =  1.1519777122123898\n",
      "epochs =  19 , step =  34000 , current loss_val =  1.012092735886059\n",
      "epochs =  19 , step =  35000 , current loss_val =  0.8968422937933063\n",
      "epochs =  19 , step =  36000 , current loss_val =  0.8596169424161555\n",
      "epochs =  19 , step =  37000 , current loss_val =  0.9184454041532597\n",
      "epochs =  19 , step =  38000 , current loss_val =  0.9412761884868437\n",
      "epochs =  19 , step =  39000 , current loss_val =  0.8740428317264262\n",
      "epochs =  19 , step =  40000 , current loss_val =  1.0851786421915481\n",
      "epochs =  19 , step =  41000 , current loss_val =  0.91680733623156\n",
      "epochs =  19 , step =  42000 , current loss_val =  0.9492827677293498\n",
      "epochs =  19 , step =  43000 , current loss_val =  0.9522894365953074\n",
      "epochs =  19 , step =  44000 , current loss_val =  0.9615049677624916\n",
      "epochs =  19 , step =  45000 , current loss_val =  0.9885603807807587\n",
      "epochs =  19 , step =  46000 , current loss_val =  0.9622558946383598\n",
      "epochs =  19 , step =  47000 , current loss_val =  1.1782866316736638\n",
      "\n",
      "current epochs =  19  , current training accuracy =  99.1  %\n",
      "current epochs =  19  , current validation accuracy =  97.3  %\n",
      "\n",
      "epochs =  20 , step =  0 , current loss_val =  0.9496520256268426\n",
      "epochs =  20 , step =  1000 , current loss_val =  1.1220168196632296\n",
      "epochs =  20 , step =  2000 , current loss_val =  1.0283268889354948\n",
      "epochs =  20 , step =  3000 , current loss_val =  0.8257311364139617\n",
      "epochs =  20 , step =  4000 , current loss_val =  0.820007004820055\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epochs =  20 , step =  5000 , current loss_val =  0.9162181212784308\n",
      "epochs =  20 , step =  6000 , current loss_val =  0.8905786408237177\n",
      "epochs =  20 , step =  7000 , current loss_val =  0.9835682954851612\n",
      "epochs =  20 , step =  8000 , current loss_val =  0.8648911828594716\n",
      "epochs =  20 , step =  9000 , current loss_val =  1.0077695254244268\n",
      "epochs =  20 , step =  10000 , current loss_val =  0.9889680557513407\n",
      "epochs =  20 , step =  11000 , current loss_val =  0.9014363564506668\n",
      "epochs =  20 , step =  12000 , current loss_val =  1.0264675319507839\n",
      "epochs =  20 , step =  13000 , current loss_val =  0.9190744803092432\n",
      "epochs =  20 , step =  14000 , current loss_val =  1.0007834644901077\n",
      "epochs =  20 , step =  15000 , current loss_val =  0.9857375092009912\n",
      "epochs =  20 , step =  16000 , current loss_val =  1.1154223291671\n",
      "epochs =  20 , step =  17000 , current loss_val =  1.2377216073971855\n",
      "epochs =  20 , step =  18000 , current loss_val =  1.0432582679188451\n",
      "epochs =  20 , step =  19000 , current loss_val =  0.8335425036289766\n",
      "epochs =  20 , step =  20000 , current loss_val =  0.8814107336583771\n",
      "epochs =  20 , step =  21000 , current loss_val =  1.1337671037589967\n",
      "epochs =  20 , step =  22000 , current loss_val =  0.8627825674118592\n",
      "epochs =  20 , step =  23000 , current loss_val =  0.9300429716809121\n",
      "epochs =  20 , step =  24000 , current loss_val =  0.9621027873422423\n",
      "epochs =  20 , step =  25000 , current loss_val =  0.9720582515532379\n",
      "epochs =  20 , step =  26000 , current loss_val =  1.0054785404645297\n",
      "epochs =  20 , step =  27000 , current loss_val =  0.9706939275980677\n",
      "epochs =  20 , step =  28000 , current loss_val =  0.9422790146913476\n",
      "epochs =  20 , step =  29000 , current loss_val =  0.9739375360625717\n",
      "epochs =  20 , step =  30000 , current loss_val =  0.9118223578666256\n",
      "epochs =  20 , step =  31000 , current loss_val =  0.8713537321708001\n",
      "epochs =  20 , step =  32000 , current loss_val =  0.9491275751365369\n",
      "epochs =  20 , step =  33000 , current loss_val =  1.1582314164083085\n",
      "epochs =  20 , step =  34000 , current loss_val =  1.0214703168480697\n",
      "epochs =  20 , step =  35000 , current loss_val =  0.9047372585171534\n",
      "epochs =  20 , step =  36000 , current loss_val =  0.8617838264895116\n",
      "epochs =  20 , step =  37000 , current loss_val =  0.9260534025953086\n",
      "epochs =  20 , step =  38000 , current loss_val =  0.9519781508097994\n",
      "epochs =  20 , step =  39000 , current loss_val =  0.8812094708159247\n",
      "epochs =  20 , step =  40000 , current loss_val =  1.074519333096273\n",
      "epochs =  20 , step =  41000 , current loss_val =  0.9212079390039043\n",
      "epochs =  20 , step =  42000 , current loss_val =  0.9543771532642967\n",
      "epochs =  20 , step =  43000 , current loss_val =  0.9607915377221252\n",
      "epochs =  20 , step =  44000 , current loss_val =  0.9681413819865609\n",
      "epochs =  20 , step =  45000 , current loss_val =  0.9970069839295101\n",
      "epochs =  20 , step =  46000 , current loss_val =  0.9726330668408572\n",
      "epochs =  20 , step =  47000 , current loss_val =  1.1866284401017113\n",
      "\n",
      "current epochs =  20  , current training accuracy =  99.1  %\n",
      "current epochs =  20  , current validation accuracy =  97.3  %\n",
      "\n",
      "\n",
      "elapsed time =  0:15:27.480859\n"
     ]
    }
   ],
   "source": [
    "# hyper-parameter\n",
    "i_nodes = training_data.shape[1] - 1    # input nodes 개수\n",
    "h1_nodes = 100     # hidden 1 nodes\n",
    "o_nodes = 10       # output nodes\n",
    "lr = 0.1           # learning rate\n",
    "epochs = 20         # epochs\n",
    "\n",
    "# 손실함수 값을 저장할 list 생성\n",
    "loss_val_list = []\n",
    "\n",
    "# 정확도 저장 리스트\n",
    "training_accuracy_list = []\n",
    "validation_accuracy_list = []\n",
    "\n",
    "# 객체 생성\n",
    "nn = NeuralNetwork(i_nodes, h1_nodes, o_nodes, lr)\n",
    "\n",
    "start_time = datetime.now()\n",
    "\n",
    "for i in range(epochs):\n",
    "    \n",
    "    for step in range(len(training_data)):  # train\n",
    "    \n",
    "        # input_data, target_data normalize        \n",
    "        target_data = np.zeros(o_nodes) + 0.01    \n",
    "        target_data[int(training_data[step, 0])] = 0.99\n",
    "    \n",
    "        input_data = ((training_data[step, 1:] / 255.0) * 0.99) + 0.01\n",
    "    \n",
    "        nn.train( np.array(input_data, ndmin=2), np.array(target_data, ndmin=2) )\n",
    "    \n",
    "        if step % 1000 == 0:\n",
    "            print(\"epochs = \", i+1, \", step = \", step,  \", current loss_val = \", nn.loss_val())\n",
    "            \n",
    "        # 손실함수 값 저장 per step\n",
    "        loss_val_list.append(nn.loss_val())    \n",
    "        \n",
    "    # 정확도 계산 및 저장 per epochs\n",
    "    (training_accuracy, index_label_prediction_list) = nn.accuracy(training_data[:, 1:], training_data[:, 0])\n",
    "    (validation_accuracy, index_label_prediction_list) = nn.accuracy(validation_data[:, 1:], validation_data[:, 0])\n",
    "    \n",
    "    print('\\ncurrent epochs = ', i+1,' , current training accuracy = ', 100*np.round(training_accuracy,3), ' %')\n",
    "    print('current epochs = ', i+1,' , current validation accuracy = ', 100*np.round(validation_accuracy,3), ' %\\n')\n",
    "        \n",
    "    training_accuracy_list.append(training_accuracy)\n",
    "    validation_accuracy_list.append(validation_accuracy)\n",
    "        \n",
    "        \n",
    "end_time = datetime.now() \n",
    "print(\"\\nelapsed time = \", end_time - start_time) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.title('Training / Validation Accuracy Trend')\n",
    "plt.xlabel('epochs')\n",
    "plt.ylabel('Accuracy')\n",
    "plt.grid()\n",
    "plt.plot(training_accuracy_list)\n",
    "plt.plot(validation_accuracy_list)\n",
    "plt.legend(['training acc', 'validation acc'])\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "test_data.shape =  (10000, 785)\n",
      "test_data[0,0] =  7.0 , len(test_data[0]) =  785\n",
      "Accuracy =  97.55  %\n"
     ]
    }
   ],
   "source": [
    "# 0~9 숫자 이미지가 784개의 숫자 (28X28) 로 구성되어 있는 test data 읽어옴\n",
    "test_data = np.loadtxt('./mnist_test.csv', delimiter=',', dtype=np.float32)\n",
    "\n",
    "test_input_data = test_data[ : , 1: ]\n",
    "test_target_data = test_data[ : , 0 ]\n",
    "\n",
    "print(\"test_data.shape = \", test_data.shape)\n",
    "print(\"test_data[0,0] = \", test_data[0,0], \", len(test_data[0]) = \", len(test_data[0]))\n",
    "\n",
    "# measure accuracy\n",
    "(accuracy_ret, index_label_prediction_list) = nn.accuracy(test_input_data, test_target_data)   \n",
    "\n",
    "print('Accuracy = ', np.round(100*accuracy_ret, 3), ' %')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "accuracy =  0.9755\n",
      "false prediction data num =  245\n"
     ]
    }
   ],
   "source": [
    "# 총 오답 개수\n",
    "total_test_data_num = len(test_data)\n",
    "false_prediction_data_num = len(index_label_prediction_list)\n",
    "\n",
    "print('accuracy = ', (total_test_data_num - false_prediction_data_num) / total_test_data_num)\n",
    "print('false prediction data num = ', false_prediction_data_num)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[115, 4, 9], [247, 4, 2], [259, 6, 0], [320, 9, 8], [321, 2, 7], [340, 5, 3], [445, 6, 0], [447, 4, 9], [448, 9, 8], [495, 8, 2], [582, 8, 2], [591, 8, 3], [613, 2, 8], [619, 1, 8], [659, 2, 8], [691, 8, 4], [717, 0, 7], [720, 5, 8], [740, 4, 9], [810, 7, 2], [924, 2, 7], [951, 5, 7], [965, 6, 0], [1014, 6, 5], [1039, 7, 2], [1044, 6, 2], [1062, 3, 7], [1112, 4, 6], [1128, 3, 7], [1182, 6, 5], [1192, 9, 4], [1194, 7, 9], [1226, 7, 2], [1232, 9, 4], [1242, 4, 9], [1247, 9, 3], [1260, 7, 1], [1289, 5, 9], [1299, 5, 7], [1319, 8, 3], [1326, 7, 1], [1328, 7, 9], [1393, 5, 3], [1395, 2, 8], [1496, 7, 9], [1500, 7, 1], [1522, 7, 9], [1549, 4, 6], [1553, 9, 3], [1609, 2, 6], [1621, 0, 6], [1681, 3, 7], [1709, 9, 3], [1717, 8, 0], [1737, 5, 2], [1754, 7, 2], [1790, 2, 8], [1901, 9, 4], [1940, 5, 0], [1941, 7, 2], [1982, 6, 5], [1984, 2, 0], [2016, 7, 2], [2035, 5, 3], [2040, 5, 4], [2044, 2, 7], [2053, 4, 9], [2073, 5, 0], [2098, 2, 0], [2109, 3, 7], [2118, 6, 0], [2125, 5, 9], [2129, 9, 2], [2130, 4, 9], [2135, 6, 1], [2182, 1, 2], [2189, 9, 1], [2224, 5, 8], [2266, 1, 6], [2272, 8, 6], [2293, 9, 6], [2299, 2, 8], [2333, 0, 2], [2369, 5, 9], [2387, 9, 1], [2395, 8, 3], [2406, 9, 1], [2414, 9, 4], [2447, 4, 9], [2488, 2, 4], [2560, 3, 1], [2607, 7, 1], [2648, 9, 0], [2654, 6, 1], [2720, 9, 4], [2863, 9, 4], [2877, 4, 9], [2896, 8, 0], [2921, 3, 2], [2927, 3, 2], [2938, 4, 6], [2939, 9, 7], [2995, 6, 8], [3060, 9, 7], [3117, 5, 9], [3189, 7, 6], [3405, 4, 9], [3422, 6, 0], [3503, 9, 1], [3533, 4, 9], [3549, 3, 2], [3558, 5, 0], [3559, 8, 5], [3567, 8, 5], [3597, 9, 3], [3757, 8, 5], [3767, 7, 2], [3780, 4, 6], [3811, 2, 4], [3818, 0, 6], [3853, 6, 5], [3869, 9, 4], [3893, 5, 6], [3906, 1, 2], [3941, 4, 6], [3943, 3, 5], [3968, 5, 3], [3985, 9, 4], [4065, 0, 9], [4075, 8, 0], [4078, 9, 7], [4152, 5, 1], [4163, 9, 0], [4176, 2, 6], [4193, 6, 4], [4199, 7, 9], [4201, 1, 7], [4205, 2, 1], [4224, 9, 7], [4248, 2, 8], [4289, 2, 8], [4300, 5, 9], [4306, 3, 7], [4360, 5, 3], [4382, 4, 9], [4425, 9, 4], [4437, 3, 2], [4497, 8, 7], [4575, 4, 2], [4578, 7, 9], [4601, 8, 4], [4635, 3, 5], [4639, 8, 9], [4699, 6, 1], [4731, 8, 7], [4761, 9, 8], [4807, 8, 3], [4823, 9, 4], [4874, 9, 0], [4880, 0, 8], [4886, 7, 1], [4943, 2, 8], [4956, 8, 4], [4966, 7, 8], [4990, 3, 2], [5140, 3, 0], [5331, 1, 6], [5457, 1, 8], [5600, 7, 9], [5634, 2, 3], [5642, 1, 5], [5654, 7, 8], [5734, 3, 2], [5735, 5, 6], [5749, 8, 2], [5887, 7, 2], [5888, 4, 0], [5936, 4, 9], [5937, 5, 3], [5955, 3, 8], [5972, 5, 3], [5973, 3, 8], [5982, 5, 3], [6011, 3, 5], [6023, 3, 9], [6030, 3, 9], [6045, 3, 9], [6059, 3, 9], [6071, 9, 3], [6091, 9, 8], [6166, 9, 3], [6172, 9, 0], [6173, 9, 0], [6174, 3, 5], [6400, 0, 6], [6505, 9, 0], [6555, 8, 9], [6560, 9, 3], [6571, 9, 7], [6577, 7, 1], [6597, 0, 7], [6608, 9, 5], [6625, 8, 7], [6651, 0, 5], [7216, 0, 6], [7432, 7, 1], [7434, 4, 9], [7451, 5, 6], [7821, 3, 2], [7823, 8, 2], [7849, 3, 2], [7921, 8, 6], [8020, 1, 8], [8183, 8, 5], [8246, 3, 9], [8277, 3, 5], [8293, 3, 5], [8311, 6, 4], [8339, 8, 2], [8527, 4, 9], [9009, 7, 2], [9015, 7, 2], [9019, 7, 2], [9024, 7, 2], [9036, 7, 2], [9280, 8, 5], [9422, 5, 3], [9587, 9, 4], [9634, 0, 3], [9642, 9, 7], [9669, 4, 5], [9679, 6, 3], [9692, 9, 7], [9716, 2, 5], [9729, 5, 6], [9745, 4, 2], [9749, 5, 6], [9768, 2, 0], [9770, 5, 0], [9779, 2, 0], [9808, 9, 4], [9839, 2, 7], [9905, 3, 8], [9944, 3, 8], [9982, 5, 6]]\n"
     ]
    }
   ],
   "source": [
    "# index_label_prediction_list 확인\n",
    "print(index_label_prediction_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# check max loss data\n",
    "import matplotlib.pyplot as plt\n",
    "import random\n",
    "%matplotlib inline\n",
    "\n",
    "img = training_data[115:116, 1:].reshape(28,28)\n",
    "\n",
    "plt.imshow(img, cmap='gray')\n",
    "plt.title(random.choice(index_label_prediction_list))\n",
    "plt.show()\n",
    "\n",
    "# print(\"label = \", training_data[115:116, 0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
