{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## One-Hot Encoding 방식으로 구현된 Diabetes Example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from datetime import datetime\n",
    "\n",
    "np.random.seed(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 수치미분 함수\n",
    "\n",
    "def numerical_derivative(f, x):\n",
    "    delta_x = 1e-4 # 0.0001\n",
    "    grad = np.zeros_like(x)\n",
    "    \n",
    "    it = np.nditer(x, flags=['multi_index'], op_flags=['readwrite'])\n",
    "    \n",
    "    while not it.finished:\n",
    "        idx = it.multi_index        \n",
    "        tmp_val = x[idx]\n",
    "        x[idx] = float(tmp_val) + delta_x\n",
    "        fx1 = f(x) # f(x+delta_x)\n",
    "        \n",
    "        x[idx] = tmp_val - delta_x \n",
    "        fx2 = f(x) # f(x-delta_x)\n",
    "        grad[idx] = (fx1 - fx2) / (2*delta_x)\n",
    "        \n",
    "        x[idx] = tmp_val \n",
    "        it.iternext()   \n",
    "        \n",
    "    return grad\n",
    "\n",
    "# sigmoid 함수\n",
    "\n",
    "def sigmoid(x):\n",
    "    return 1 / (1+np.exp(-x))\n",
    "\n",
    "# softmax 함수\n",
    "\n",
    "def softmax(z):\n",
    "    \n",
    "    c = np.max(z)  # overflow 방지\n",
    "    \n",
    "    numerator = np.exp(z-c)\n",
    "    denominator = np.sum(np.exp(z-c))\n",
    "    \n",
    "    y = numerator / denominator\n",
    "    \n",
    "    return y"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### One-Hot Encoding 방식으로 구현된 Diabetes class 구현"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Diabetes Class\n",
    "\n",
    "class Diabetes:\n",
    "    \n",
    "    # 생성자\n",
    "    # xdata, tdata => numpy.array(...)\n",
    "    def __init__(self, name, input_nodes, hidden1_nodes, output_nodes, learning_rate):\n",
    "        \n",
    "        self.name = name\n",
    "        \n",
    "        # 2층 hidden layer unit \n",
    "        # 은닉층 가중치  W2  Xavier/He 방법으로 self.W2 가중치 초기화\n",
    "        self.W2 = np.random.randn(input_nodes, hidden1_nodes) / np.sqrt(input_nodes/2)\n",
    "        self.b2 = np.random.rand(hidden1_nodes)      \n",
    "        \n",
    "        # 출력층 가중치는 W3  Xavier/He 방법으로 self.W3 가중치 초기화\n",
    "        self.W3 = np.random.randn(hidden1_nodes, output_nodes) / np.sqrt(hidden1_nodes/2)\n",
    "        self.b3 = np.random.rand(output_nodes)      \n",
    "        \n",
    "        # 학습률 learning rate 초기화\n",
    "        self.learning_rate = learning_rate\n",
    "        \n",
    "        print(self.name, \" is created !!!\")\n",
    "        \n",
    "    # 손실함수\n",
    "    def feed_forward(self):\n",
    "        \n",
    "        delta = 1e-7    # log 무한대 발산 방지\n",
    "    \n",
    "        z2 = np.dot(self.input_data, self.W2) + self.b2\n",
    "        a2 = sigmoid(z2)\n",
    "        \n",
    "        z3 = np.dot(a2, self.W3) + self.b3\n",
    "        \n",
    "        # softmax 함수를 이용하여 출력 값 계산\n",
    "        y = a3 = softmax(z3)\n",
    "    \n",
    "        # cross-entropy \n",
    "        return  -np.sum( self.target_data*np.log(y + delta) + (1-self.target_data)*np.log((1 - y)+delta ) )\n",
    "    \n",
    "    # 손실 값 계산\n",
    "    def loss_val(self):\n",
    "        \n",
    "        delta = 1e-7    # log 무한대 발산 방지\n",
    "    \n",
    "        z2 = np.dot(self.input_data, self.W2) + self.b2\n",
    "        a2 = sigmoid(z2)\n",
    "        \n",
    "        z3 = np.dot(a2, self.W3) + self.b3\n",
    "        \n",
    "        # softmax 함수를 이용하여 출력 값 계산\n",
    "        y = a3 = softmax(z3)\n",
    "            \n",
    "        # cross-entropy \n",
    "        return  -np.sum( self.target_data*np.log(y + delta) + (1-self.target_data)*np.log((1 - y)+delta ) )\n",
    "    \n",
    "    # query, 즉 미래 값 예측 함수\n",
    "    def predict(self, input_data):    \n",
    "        \n",
    "        z2 = np.dot(input_data, self.W2) + self.b2\n",
    "        a2 = sigmoid(z2)\n",
    "        \n",
    "        z3 = np.dot(a2, self.W3) + self.b3\n",
    "        \n",
    "        # softmax 함수를 이용하여 출력 값 계산\n",
    "        y = a3 = softmax(z3)\n",
    "        \n",
    "        # one-hot encoding 방식으로 하기 위한 코드 수정\n",
    "        predicted_num = np.argmax(y)\n",
    "    \n",
    "        return predicted_num\n",
    "    \n",
    "    def accuracy(self, input_data, target_data):\n",
    "        \n",
    "        matched_list = []\n",
    "        not_matched_list = []\n",
    "        \n",
    "        for index in range(len(input_data)):\n",
    "            \n",
    "            # one-hot encoding 방식으로 하기 위한 코드 수정\n",
    "            label = int(target_data[index])\n",
    "            \n",
    "            predicted_num = self.predict(input_data[index])\n",
    "            \n",
    "            if predicted_num == label:\n",
    "                matched_list.append(index)\n",
    "            else:\n",
    "                not_matched_list.append(index)\n",
    "                \n",
    "        accuracy_result = len(matched_list) / len(input_data)\n",
    "        \n",
    "        #print(\"Accuracy => \", accuracy_result)\n",
    "        \n",
    "        return not_matched_list, accuracy_result\n",
    "        \n",
    "        \n",
    "    # 수치미분을 이용하여 손실함수가 최소가 될때 까지 학습하는 함수\n",
    "    def train(self, input_data, target_data):\n",
    "        \n",
    "        self.input_data = input_data\n",
    "        self.target_data = target_data\n",
    "        \n",
    "        f = lambda x : self.feed_forward()\n",
    "        \n",
    "        self.W2 -= self.learning_rate * numerical_derivative(f, self.W2)\n",
    "    \n",
    "        self.b2 -= self.learning_rate * numerical_derivative(f, self.b2)\n",
    "        \n",
    "        self.W3 -= self.learning_rate * numerical_derivative(f, self.W3)\n",
    "    \n",
    "        self.b3 -= self.learning_rate * numerical_derivative(f, self.b3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### DataGeneration class 구현"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DataGeneration:\n",
    "    \n",
    "    # target_position = 0 (첫번째열이 정답데이터), target_position=-1 (마지막열이 정답데이터)\n",
    "    def __init__(self, name, file_path, seperation_rate, target_position=-1):\n",
    "        \n",
    "        self.name = name\n",
    "        \n",
    "        self.file_path = file_path\n",
    "        \n",
    "        self.seperation_rate = seperation_rate\n",
    "        \n",
    "        if (target_position == -1  or  target_position == 0):      \n",
    "            self.target_position = target_position\n",
    "        \n",
    "        else:\n",
    "            err_str = 'target_position must be -1 or 0'            \n",
    "            raise Exception(err_str)    \n",
    "            \n",
    "    \n",
    "    # print data target distribution \n",
    "    # str_of_kind : 'original data' or  'training data'  or  'test data'\n",
    "    def __display_target_distribution(self, data, str_of_kind='original data'):\n",
    "        \n",
    "        print('=======================================================================================================')\n",
    "        \n",
    "        target_data = data[ :, self.target_position ]\n",
    "        \n",
    "        # numpy.unique() 사용하여 loaded data target 분포 확인\n",
    "        unique, counts = np.unique(target_data, return_counts=True)\n",
    "\n",
    "        unique_target = []\n",
    "    \n",
    "        for index in range(len(unique)):\n",
    "        \n",
    "            print('[DataGeneration] unique number of ' + str_of_kind + ' = ', unique[index], ', count = ', counts[index])\n",
    "        \n",
    "            unique_target.append(unique[index])\n",
    "\n",
    "        for index in range(len(unique_target)):\n",
    "        \n",
    "            print('[DataGeneration] unique number of ' + str_of_kind + ' = ', unique_target[index], ', ratio = ', np.round(100 * counts[index] / (target_data.shape[0]), 2), ' %')\n",
    "    \n",
    "        print('=======================================================================================================')\n",
    "        \n",
    "        \n",
    "    # numpy.random.shuffle()  이용하여 training_data / test_data 생성\n",
    "    def generate(self):\n",
    "    \n",
    "        # 데이터 불러오기, 파일이 없는 경우 exception 발생\n",
    "\n",
    "        try:\n",
    "            loaded_data = np.loadtxt(self.file_path, delimiter=',', dtype=np.float32)\n",
    "            \n",
    "        except Exception as err:\n",
    "            print('[DataGeneration::generate()]  ', str(err))\n",
    "            raise Exception(str(err))\n",
    "\n",
    "        print(\"[DataGeneration]  loaded_data.shape = \", loaded_data.shape)\n",
    "            \n",
    "        # print the target distribution of original data \n",
    "        \n",
    "        self.__display_target_distribution(loaded_data, 'original data')\n",
    "        \n",
    "        \n",
    "        # 분리비율에 맞게 테스트데이터로 분리\n",
    "        total_data_num = len(loaded_data)\n",
    "        test_data_num = int(len(loaded_data) * self.seperation_rate)\n",
    "\n",
    "        # numpy.random.shuffle 을 이용하여 랜덤하게 데이터 섞기\n",
    "        np.random.shuffle(loaded_data)\n",
    "        \n",
    "        # test_data 는 0 : test_data_num\n",
    "        \n",
    "        \n",
    "        test_data = loaded_data[ 0:test_data_num ]\n",
    "\n",
    "        # training_data 는 test_data_num 부터 끝까지 \n",
    "        training_data = loaded_data[ test_data_num: ]\n",
    "\n",
    "        # display target distribution of generated data \n",
    "        \n",
    "        self.__display_target_distribution(training_data, 'training data')\n",
    "        \n",
    "        self.__display_target_distribution(test_data, 'test data')\n",
    "        \n",
    "        return training_data, test_data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (분리비율 0.3)  DataGeneration 객체를 통한 training_data , test_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[DataGeneration]  loaded_data.shape =  (759, 9)\n",
      "=======================================================================================================\n",
      "[DataGeneration] unique number of original data =  0.0 , count =  263\n",
      "[DataGeneration] unique number of original data =  1.0 , count =  496\n",
      "[DataGeneration] unique number of original data =  0.0 , ratio =  34.65  %\n",
      "[DataGeneration] unique number of original data =  1.0 , ratio =  65.35  %\n",
      "=======================================================================================================\n",
      "=======================================================================================================\n",
      "[DataGeneration] unique number of training data =  0.0 , count =  180\n",
      "[DataGeneration] unique number of training data =  1.0 , count =  352\n",
      "[DataGeneration] unique number of training data =  0.0 , ratio =  33.83  %\n",
      "[DataGeneration] unique number of training data =  1.0 , ratio =  66.17  %\n",
      "=======================================================================================================\n",
      "=======================================================================================================\n",
      "[DataGeneration] unique number of test data =  0.0 , count =  83\n",
      "[DataGeneration] unique number of test data =  1.0 , count =  144\n",
      "[DataGeneration] unique number of test data =  0.0 , ratio =  36.56  %\n",
      "[DataGeneration] unique number of test data =  1.0 , ratio =  63.44  %\n",
      "=======================================================================================================\n",
      "================================================\n",
      "training data.shape =  (532, 9)\n",
      "test data.shape =  (227, 9)\n",
      "================================================\n"
     ]
    }
   ],
   "source": [
    "# DataGeneration 객체 생성.\n",
    "test_seperation_rate = 0.3 # 테스트 데이터 분리비율\n",
    "\n",
    "data_obj = DataGeneration('Diabetes', './(200309)Diabetes.csv', test_seperation_rate)\n",
    "\n",
    "(training_data, test_data) = data_obj.generate()\n",
    "\n",
    "print('================================================')\n",
    "print('training data.shape = ', training_data.shape)\n",
    "print('test data.shape = ', test_data.shape)\n",
    "print('================================================')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Diabetes 객체 생성 및 학습 (One-Hot Encoding 방식으로 표현)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Diabetes  is created !!!\n",
      "Neural Network Learning using Numerical Derivative...\n",
      "step =  0  , current loss value =  1.7203248161210762\n",
      "step =  1  , current loss value =  1.6730969712311357\n",
      "step =  2  , current loss value =  1.6133162716729226\n",
      "step =  3  , current loss value =  1.550603424340928\n",
      "step =  4  , current loss value =  1.493489217727368\n",
      "step =  5  , current loss value =  1.4450793404337776\n",
      "step =  6  , current loss value =  1.405016274600812\n",
      "step =  7  , current loss value =  1.3719898413959544\n",
      "step =  8  , current loss value =  1.3447542908971486\n",
      "step =  9  , current loss value =  1.3223034329896364\n",
      "step =  10  , current loss value =  1.3038264370332342\n",
      "step =  11  , current loss value =  1.2886527479298995\n",
      "step =  12  , current loss value =  1.2762183852305105\n",
      "step =  13  , current loss value =  1.2660468361893462\n",
      "step =  14  , current loss value =  1.2577366133083747\n",
      "step =  15  , current loss value =  1.2509514082047326\n",
      "step =  16  , current loss value =  1.2454113068464507\n",
      "step =  17  , current loss value =  1.2408846728893084\n",
      "step =  18  , current loss value =  1.2371807054907302\n",
      "step =  19  , current loss value =  1.234142759500682\n",
      "step =  20  , current loss value =  1.2316424857346944\n",
      "step =  21  , current loss value =  1.2295747949246214\n",
      "step =  22  , current loss value =  1.2278536030869973\n",
      "step =  23  , current loss value =  1.2264082864841839\n",
      "step =  24  , current loss value =  1.2251807599715798\n",
      "step =  25  , current loss value =  1.2241230893794304\n",
      "step =  26  , current loss value =  1.2231955526157823\n",
      "step =  27  , current loss value =  1.2223650722519568\n",
      "step =  28  , current loss value =  1.221603952104406\n",
      "step =  29  , current loss value =  1.2208888604360923\n",
      "\n",
      "Elapsed Time =>  0:08:27.680838\n"
     ]
    }
   ],
   "source": [
    "#hyper-parameter\n",
    "i_nodes = training_data.shape[1] - 1    # input nodes 개수\n",
    "h1_nodes = 10  # hidden 1 nodes 개수. \n",
    "\n",
    "o_nodes = 2    # one-hot encoding 표현하기 위한 output nodes 개수는 2개. 즉 정답은 0 또는 1 두개만 있음\n",
    "\n",
    "lr = 1e-2     # learning rate. hi_node = 2, 1e-1 에서 수렴\n",
    "epochs = 30   # 반복횟수. \n",
    "\n",
    "# 저장 리스트\n",
    "loss_val_list = []    # loss val list\n",
    "\n",
    "# Diabetes 객체 생성\n",
    "obj1 = Diabetes(\"Diabetes\", i_nodes, h1_nodes, o_nodes, lr)\n",
    "\n",
    "print(\"Neural Network Learning using Numerical Derivative...\")\n",
    "\n",
    "start_time = datetime.now()\n",
    "\n",
    "for step in range(epochs):\n",
    "    \n",
    "    for index in range(len(training_data)):\n",
    "        \n",
    "        input_data = training_data[index, 0:-1]\n",
    "        \n",
    "        # one-hot encoding 방식으로 표현                \n",
    "        target_data = np.zeros(o_nodes) + 0.01    \n",
    "        target_data[int(training_data[index, -1])] = 0.99\n",
    "        \n",
    "        obj1.train(input_data, target_data)\n",
    "        \n",
    "    cur_loss_val = obj1.loss_val()    \n",
    "    loss_val_list.append(cur_loss_val)\n",
    "    \n",
    "    print(\"step = \", step, \" , current loss value = \", cur_loss_val)\n",
    "\n",
    "end_time = datetime.now()\n",
    "        \n",
    "print(\"\")\n",
    "print(\"Elapsed Time => \", end_time - start_time)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.title('Loss Value Trend')\n",
    "plt.xlabel('epochs')\n",
    "plt.ylabel('Loss')\n",
    "plt.grid()\n",
    "plt.plot(loss_val_list)\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Data Accuracy =  0.797\n"
     ]
    }
   ],
   "source": [
    "test_input_data = test_data[ :, 0:-1 ]\n",
    "test_target_data = test_data[ :, -1 ]\n",
    "\n",
    "(false_list, accuracy_ret) = obj1.accuracy(test_input_data, test_target_data) \n",
    "\n",
    "print('Test Data Accuracy = ', np.round(accuracy_ret, 3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
