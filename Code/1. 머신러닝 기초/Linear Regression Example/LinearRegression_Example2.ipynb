{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "def numerical_derivative(f, x):\n",
    "    delta_x = 1e-4 # 0.0001\n",
    "    grad = np.zeros_like(x)\n",
    "    \n",
    "    it = np.nditer(x, flags=['multi_index'], op_flags=['readwrite'])\n",
    "    \n",
    "    while not it.finished:\n",
    "        idx = it.multi_index        \n",
    "        tmp_val = x[idx]\n",
    "        x[idx] = float(tmp_val) + delta_x\n",
    "        fx1 = f(x) # f(x+delta_x)\n",
    "        \n",
    "        x[idx] = tmp_val - delta_x \n",
    "        fx2 = f(x) # f(x-delta_x)\n",
    "        grad[idx] = (fx1 - fx2) / (2*delta_x)\n",
    "        \n",
    "        x[idx] = tmp_val \n",
    "        it.iternext()   \n",
    "        \n",
    "    return grad"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "x_data.ndim =  2 , x_data.shape =  (25, 3)\n",
      "t_data.ndim =  2 , t_data.shape =  (25, 1)\n"
     ]
    }
   ],
   "source": [
    "loaded_data = np.loadtxt('./(200221)data-01.csv', delimiter=',', dtype=np.float32)\n",
    "\n",
    "x_data = loaded_data[ :, 0:-1]\n",
    "t_data = loaded_data[ :, [-1]]\n",
    "\n",
    "# 데이터 차원 및 shape 확인\n",
    "print(\"x_data.ndim = \", x_data.ndim, \", x_data.shape = \", x_data.shape) \n",
    "# 행과 열이 많아져도 data shape 의 인덱스 즉 행과 열의 갯수를 불러오면 된다  x_data.shpae[0]or [1]\n",
    "print(\"t_data.ndim = \", t_data.ndim, \", t_data.shape = \", t_data.shape) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "W =  [[0.88956013]\n",
      " [0.27327916]\n",
      " [0.27170983]] , W.shape =  (3, 1) , b =  [0.88008017] , b.shape =  (1,)\n"
     ]
    }
   ],
   "source": [
    "W = np.random.rand(3,1)  # 3X1 행렬\n",
    "b = np.random.rand(1)  \n",
    "print(\"W = \", W, \", W.shape = \", W.shape, \", b = \", b, \", b.shape = \", b.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def loss_func(x, t):\n",
    "    y = np.dot(x,W) + b\n",
    "    \n",
    "    return ( np.sum( (t - y)**2 ) ) / ( len(x) )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 손실함수 값 계산 함수\n",
    "# 입력변수 x, t : numpy type\n",
    "def loss_val(x, t):\n",
    "    y = np.dot(x,W) + b\n",
    "    \n",
    "    return ( np.sum( (t - y)**2 ) ) / ( len(x) )\n",
    "\n",
    "\n",
    "# 학습을 마친 후, 임의의 데이터에 대해 미래 값 예측 함수\n",
    "# 입력변수 x : numpy type\n",
    "def predict(x):\n",
    "    y = np.dot(x,W) + b\n",
    "    \n",
    "    return y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initial loss value =  2273.6233109559685 Initial W =  [[0.88956013]\n",
      " [0.27327916]\n",
      " [0.27170983]] \n",
      " , b =  [0.88008017]\n",
      "step =  0 loss value =  853.3455230086952 W =  [[0.96523055]\n",
      " [0.34950596]\n",
      " [0.34998749]] , b =  [0.88065162]\n",
      "step =  400 loss value =  17.100383298894993 W =  [[1.01248692]\n",
      " [0.45217071]\n",
      " [0.55483388]] , b =  [0.88146863]\n",
      "step =  800 loss value =  14.75724760046558 W =  [[0.94896791]\n",
      " [0.44147777]\n",
      " [0.62701666]] , b =  [0.88130535]\n",
      "step =  1200 loss value =  12.95222743291421 W =  [[0.89166582]\n",
      " [0.43471767]\n",
      " [0.68931521]] , b =  [0.88106118]\n",
      "step =  1600 loss value =  11.553821621346305 W =  [[0.83996077]\n",
      " [0.43104131]\n",
      " [0.74316162]] , b =  [0.88074707]\n",
      "step =  2000 loss value =  10.464532427720904 W =  [[0.79329651]\n",
      " [0.42975774]\n",
      " [0.78977157]] , b =  [0.88037241]\n",
      "step =  2400 loss value =  9.611658419098656 W =  [[0.75117356]\n",
      " [0.43030651]\n",
      " [0.83017805]] , b =  [0.87994523]\n",
      "step =  2800 loss value =  8.940667302418387 W =  [[0.71314323]\n",
      " [0.43223471]\n",
      " [0.86525963]] , b =  [0.87947246]\n",
      "step =  3200 loss value =  8.410408369100313 W =  [[0.67880224]\n",
      " [0.43517793]\n",
      " [0.89576429]] , b =  [0.87896002]\n",
      "step =  3600 loss value =  7.98964001855467 W =  [[0.64778793]\n",
      " [0.43884439]\n",
      " [0.92232955]] , b =  [0.87841305]\n",
      "step =  4000 loss value =  7.65450010187414 W =  [[0.61977406]\n",
      " [0.44300188]\n",
      " [0.94549932]] , b =  [0.87783594]\n",
      "step =  4400 loss value =  7.386654378507887 W =  [[0.59446701]\n",
      " [0.4474669 ]\n",
      " [0.96573822]] , b =  [0.87723252]\n",
      "step =  4800 loss value =  7.171934439913413 W =  [[0.57160246]\n",
      " [0.45209571]\n",
      " [0.9834436 ]] , b =  [0.87660609]\n",
      "step =  5200 loss value =  6.999330323450437 W =  [[0.5509423 ]\n",
      " [0.45677688]\n",
      " [0.99895568]] , b =  [0.87595948]\n",
      "step =  5600 loss value =  6.860241255437282 W =  [[0.53227205]\n",
      " [0.46142519]\n",
      " [1.01256618]] , b =  [0.8752952]\n",
      "step =  6000 loss value =  6.747915124476114 W =  [[0.51539839]\n",
      " [0.46597661]\n",
      " [1.02452551]] , b =  [0.87461538]\n",
      "step =  6400 loss value =  6.657026633564467 W =  [[0.50014707]\n",
      " [0.47038414]\n",
      " [1.03504893]] , b =  [0.8739219]\n",
      "step =  6800 loss value =  6.583357894109013 W =  [[0.48636094]\n",
      " [0.47461442]\n",
      " [1.04432175]] , b =  [0.87321639]\n",
      "step =  7200 loss value =  6.523555116004546 W =  [[0.47389827]\n",
      " [0.47864491]\n",
      " [1.05250372]] , b =  [0.87250028]\n",
      "step =  7600 loss value =  6.47494215152827 W =  [[0.46263119]\n",
      " [0.48246163]\n",
      " [1.05973274]] , b =  [0.87177481]\n",
      "step =  8000 loss value =  6.435376769888003 W =  [[0.45244433]\n",
      " [0.48605734]\n",
      " [1.06612802]] , b =  [0.87104108]\n",
      "step =  8400 loss value =  6.403139242166735 W =  [[0.44323354]\n",
      " [0.48942993]\n",
      " [1.07179281]] , b =  [0.87030004]\n",
      "step =  8800 loss value =  6.376845505988728 W =  [[0.43490483]\n",
      " [0.49258131]\n",
      " [1.0768166 ]] , b =  [0.86955252]\n",
      "step =  9200 loss value =  6.355379141573507 W =  [[0.42737333]\n",
      " [0.49551631]\n",
      " [1.08127712]] , b =  [0.86879927]\n",
      "step =  9600 loss value =  6.337837829531898 W =  [[0.42056238]\n",
      " [0.49824198]\n",
      " [1.085242  ]] , b =  [0.86804094]\n",
      "step =  10000 loss value =  6.32349102097921 W =  [[0.41440278]\n",
      " [0.50076688]\n",
      " [1.0887701 ]] , b =  [0.8672781]\n",
      "step =  10400 loss value =  6.311746336130726 W =  [[0.40883199]\n",
      " [0.5031006 ]\n",
      " [1.09191283]] , b =  [0.86651125]\n",
      "step =  10800 loss value =  6.302122792965693 W =  [[0.40379353]\n",
      " [0.5052534 ]\n",
      " [1.09471505]] , b =  [0.86574084]\n",
      "step =  11200 loss value =  6.294229406371827 W =  [[0.39923638]\n",
      " [0.50723582]\n",
      " [1.09721604]] , b =  [0.86496727]\n",
      "step =  11600 loss value =  6.28774802911007 W =  [[0.3951144 ]\n",
      " [0.5090585 ]\n",
      " [1.09945023]] , b =  [0.86419089]\n",
      "step =  12000 loss value =  6.282419557009733 W =  [[0.39138594]\n",
      " [0.51073198]\n",
      " [1.10144782]] , b =  [0.863412]\n",
      "step =  12400 loss value =  6.278032812444369 W =  [[0.38801332]\n",
      " [0.51226654]\n",
      " [1.10323536]] , b =  [0.8626309]\n",
      "step =  12800 loss value =  6.274415567291023 W =  [[0.3849625 ]\n",
      " [0.51367213]\n",
      " [1.1048362 ]] , b =  [0.86184781]\n",
      "step =  13200 loss value =  6.271427280218711 W =  [[0.38220272]\n",
      " [0.51495828]\n",
      " [1.10627093]] , b =  [0.86106296]\n",
      "step =  13600 loss value =  6.2689532114114925 W =  [[0.37970614]\n",
      " [0.51613407]\n",
      " [1.10755773]] , b =  [0.86027655]\n",
      "step =  14000 loss value =  6.266899646737647 W =  [[0.37744762]\n",
      " [0.51720808]\n",
      " [1.10871265]] , b =  [0.85948874]\n",
      "step =  14400 loss value =  6.265190017445525 W =  [[0.37540443]\n",
      " [0.51818838]\n",
      " [1.10974989]] , b =  [0.8586997]\n",
      "step =  14800 loss value =  6.263761744086037 W =  [[0.37355598]\n",
      " [0.51908255]\n",
      " [1.11068204]] , b =  [0.85790956]\n",
      "step =  15200 loss value =  6.262563667107233 W =  [[0.37188371]\n",
      " [0.51989766]\n",
      " [1.11152026]] , b =  [0.85711845]\n",
      "step =  15600 loss value =  6.2615539533842455 W =  [[0.37037079]\n",
      " [0.52064028]\n",
      " [1.11227445]] , b =  [0.85632647]\n",
      "step =  16000 loss value =  6.260698389340774 W =  [[0.36900201]\n",
      " [0.52131652]\n",
      " [1.11295343]] , b =  [0.85553373]\n",
      "step =  16400 loss value =  6.259968988437471 W =  [[0.36776363]\n",
      " [0.52193204]\n",
      " [1.11356502]] , b =  [0.85474031]\n",
      "step =  16800 loss value =  6.259342854541233 W =  [[0.36664321]\n",
      " [0.52249205]\n",
      " [1.11411622]] , b =  [0.8539463]\n",
      "step =  17200 loss value =  6.258801253742206 W =  [[0.36562951]\n",
      " [0.52300139]\n",
      " [1.11461323]] , b =  [0.85315176]\n",
      "step =  17600 loss value =  6.258328856101425 W =  [[0.36471235]\n",
      " [0.52346448]\n",
      " [1.11506163]] , b =  [0.85235676]\n",
      "step =  18000 loss value =  6.257913116014246 W =  [[0.36388252]\n",
      " [0.5238854 ]\n",
      " [1.11546636]] , b =  [0.85156136]\n",
      "step =  18400 loss value =  6.257543765705893 W =  [[0.36313171]\n",
      " [0.52426788]\n",
      " [1.11583185]] , b =  [0.85076561]\n",
      "step =  18800 loss value =  6.257212401102901 W =  [[0.36245238]\n",
      " [0.52461537]\n",
      " [1.11616209]] , b =  [0.84996956]\n",
      "step =  19200 loss value =  6.256912143163098 W =  [[0.36183772]\n",
      " [0.52493099]\n",
      " [1.11646061]] , b =  [0.84917325]\n",
      "step =  19600 loss value =  6.25663736086452 W =  [[0.36128158]\n",
      " [0.52521761]\n",
      " [1.1167306 ]] , b =  [0.84837671]\n",
      "step =  20000 loss value =  6.256383444593334 W =  [[0.36077838]\n",
      " [0.52547787]\n",
      " [1.11697491]] , b =  [0.84757998]\n",
      "step =  20400 loss value =  6.256146620736062 W =  [[0.36032307]\n",
      " [0.52571416]\n",
      " [1.11719609]] , b =  [0.84678309]\n",
      "step =  20800 loss value =  6.255923799968221 W =  [[0.3599111 ]\n",
      " [0.52592867]\n",
      " [1.11739645]] , b =  [0.84598607]\n",
      "step =  21200 loss value =  6.255712453101998 W =  [[0.35953834]\n",
      " [0.52612339]\n",
      " [1.11757804]] , b =  [0.84518894]\n",
      "step =  21600 loss value =  6.25551050947915 W =  [[0.35920105]\n",
      " [0.52630013]\n",
      " [1.11774272]] , b =  [0.84439173]\n",
      "step =  22000 loss value =  6.255316273809041 W =  [[0.35889587]\n",
      " [0.52646056]\n",
      " [1.11789215]] , b =  [0.84359445]\n",
      "step =  22400 loss value =  6.255128358098814 W =  [[0.35861972]\n",
      " [0.52660617]\n",
      " [1.11802783]] , b =  [0.84279713]\n",
      "step =  22800 loss value =  6.254945625934699 W =  [[0.35836985]\n",
      " [0.52673835]\n",
      " [1.1181511 ]] , b =  [0.84199977]\n",
      "step =  23200 loss value =  6.254767146870605 W =  [[0.35814376]\n",
      " [0.52685833]\n",
      " [1.11826319]] , b =  [0.8412024]\n",
      "step =  23600 loss value =  6.254592159089477 W =  [[0.35793918]\n",
      " [0.52696725]\n",
      " [1.11836518]] , b =  [0.84040503]\n",
      "step =  24000 loss value =  6.254420038835907 W =  [[0.35775407]\n",
      " [0.52706613]\n",
      " [1.11845805]] , b =  [0.83960767]\n",
      "step =  24400 loss value =  6.254250275391366 W =  [[0.35758656]\n",
      " [0.52715592]\n",
      " [1.1185427 ]] , b =  [0.83881032]\n",
      "step =  24800 loss value =  6.2540824505864565 W =  [[0.357435  ]\n",
      " [0.52723747]\n",
      " [1.11861991]] , b =  [0.83801301]\n",
      "step =  25200 loss value =  6.253916222027418 W =  [[0.35729785]\n",
      " [0.52731153]\n",
      " [1.11869042]] , b =  [0.83721574]\n",
      "step =  25600 loss value =  6.253751309363058 W =  [[0.35717375]\n",
      " [0.52737883]\n",
      " [1.11875487]] , b =  [0.83641852]\n",
      "step =  26000 loss value =  6.253587483041189 W =  [[0.35706146]\n",
      " [0.52743998]\n",
      " [1.11881384]] , b =  [0.83562135]\n",
      "step =  26400 loss value =  6.2534245551027166 W =  [[0.35695985]\n",
      " [0.52749556]\n",
      " [1.11886787]] , b =  [0.83482424]\n",
      "step =  26800 loss value =  6.253262371644386 W =  [[0.35686791]\n",
      " [0.52754611]\n",
      " [1.11891743]] , b =  [0.8340272]\n",
      "step =  27200 loss value =  6.253100806647509 W =  [[0.35678471]\n",
      " [0.52759209]\n",
      " [1.11896295]] , b =  [0.83323024]\n",
      "step =  27600 loss value =  6.252939756924932 W =  [[0.35670943]\n",
      " [0.52763393]\n",
      " [1.11900483]] , b =  [0.83243335]\n",
      "step =  28000 loss value =  6.25277913798401 W =  [[0.35664131]\n",
      " [0.52767202]\n",
      " [1.1190434 ]] , b =  [0.83163654]\n",
      "step =  28400 loss value =  6.252618880638978 W =  [[0.35657966]\n",
      " [0.52770672]\n",
      " [1.11907899]] , b =  [0.83083981]\n",
      "step =  28800 loss value =  6.252458928237448 W =  [[0.35652389]\n",
      " [0.52773834]\n",
      " [1.11911189]] , b =  [0.83004318]\n",
      "step =  29200 loss value =  6.252299234389252 W =  [[0.35647341]\n",
      " [0.52776718]\n",
      " [1.11914235]] , b =  [0.82924664]\n",
      "step =  29600 loss value =  6.252139761107068 W =  [[0.35642774]\n",
      " [0.5277935 ]\n",
      " [1.11917061]] , b =  [0.82845019]\n",
      "step =  30000 loss value =  6.251980477283845 W =  [[0.35638641]\n",
      " [0.52781754]\n",
      " [1.11919688]] , b =  [0.82765385]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "step =  30400 loss value =  6.251821357446132 W =  [[0.35634901]\n",
      " [0.5278395 ]\n",
      " [1.11922135]] , b =  [0.8268576]\n",
      "step =  30800 loss value =  6.2516623807332135 W =  [[0.35631517]\n",
      " [0.5278596 ]\n",
      " [1.11924418]] , b =  [0.82606145]\n",
      "step =  31200 loss value =  6.251503530061469 W =  [[0.35628455]\n",
      " [0.527878  ]\n",
      " [1.11926555]] , b =  [0.82526541]\n",
      "step =  31600 loss value =  6.251344791439903 W =  [[0.35625684]\n",
      " [0.52789487]\n",
      " [1.11928558]] , b =  [0.82446948]\n",
      "step =  32000 loss value =  6.251186153409825 W =  [[0.35623176]\n",
      " [0.52791035]\n",
      " [1.11930441]] , b =  [0.82367365]\n",
      "step =  32400 loss value =  6.251027606585959 W =  [[0.35620907]\n",
      " [0.52792457]\n",
      " [1.11932214]] , b =  [0.82287793]\n",
      "step =  32800 loss value =  6.250869143280819 W =  [[0.35618854]\n",
      " [0.52793766]\n",
      " [1.11933889]] , b =  [0.82208232]\n",
      "step =  33200 loss value =  6.250710757197108 W =  [[0.35616996]\n",
      " [0.52794971]\n",
      " [1.11935475]] , b =  [0.82128683]\n",
      "step =  33600 loss value =  6.250552443175836 W =  [[0.35615314]\n",
      " [0.52796083]\n",
      " [1.1193698 ]] , b =  [0.82049144]\n",
      "step =  34000 loss value =  6.250394196989888 W =  [[0.35613793]\n",
      " [0.5279711 ]\n",
      " [1.11938413]] , b =  [0.81969617]\n",
      "step =  34400 loss value =  6.2502360151754495 W =  [[0.35612416]\n",
      " [0.52798062]\n",
      " [1.11939779]] , b =  [0.81890102]\n",
      "step =  34800 loss value =  6.250077894893602 W =  [[0.3561117 ]\n",
      " [0.52798944]\n",
      " [1.11941085]] , b =  [0.81810598]\n",
      "step =  35200 loss value =  6.249919833816968 W =  [[0.35610042]\n",
      " [0.52799763]\n",
      " [1.11942337]] , b =  [0.81731105]\n",
      "step =  35600 loss value =  6.249761830037191 W =  [[0.35609022]\n",
      " [0.52800526]\n",
      " [1.11943541]] , b =  [0.81651624]\n",
      "step =  36000 loss value =  6.249603881989044 W =  [[0.35608098]\n",
      " [0.52801237]\n",
      " [1.119447  ]] , b =  [0.81572155]\n",
      "step =  36400 loss value =  6.249445988388038 W =  [[0.35607263]\n",
      " [0.52801902]\n",
      " [1.11945819]] , b =  [0.81492698]\n",
      "step =  36800 loss value =  6.249288148179814 W =  [[0.35606506]\n",
      " [0.52802525]\n",
      " [1.11946902]] , b =  [0.81413252]\n",
      "step =  37200 loss value =  6.249130360498262 W =  [[0.35605822]\n",
      " [0.5280311 ]\n",
      " [1.11947952]] , b =  [0.81333818]\n",
      "step =  37600 loss value =  6.2489726246315005 W =  [[0.35605202]\n",
      " [0.5280366 ]\n",
      " [1.11948973]] , b =  [0.81254396]\n",
      "step =  38000 loss value =  6.248814939993938 W =  [[0.35604642]\n",
      " [0.52804179]\n",
      " [1.11949966]] , b =  [0.81174987]\n",
      "step =  38400 loss value =  6.248657306103347 W =  [[0.35604135]\n",
      " [0.5280467 ]\n",
      " [1.11950935]] , b =  [0.81095589]\n",
      "step =  38800 loss value =  6.248499722562131 W =  [[0.35603675]\n",
      " [0.52805136]\n",
      " [1.11951883]] , b =  [0.81016203]\n",
      "step =  39200 loss value =  6.248342189042059 W =  [[0.3560326 ]\n",
      " [0.52805578]\n",
      " [1.1195281 ]] , b =  [0.80936828]\n",
      "step =  39600 loss value =  6.248184705271655 W =  [[0.35602884]\n",
      " [0.52806   ]\n",
      " [1.11953719]] , b =  [0.80857466]\n",
      "step =  40000 loss value =  6.248027271025918 W =  [[0.35602543]\n",
      " [0.52806402]\n",
      " [1.11954612]] , b =  [0.80778116]\n"
     ]
    }
   ],
   "source": [
    "learning_rate = 1e-5  \n",
    "\n",
    "f = lambda x : loss_func(x_data,t_data)\n",
    "\n",
    "print(\"Initial loss value = \", loss_val(x_data, t_data), \"Initial W = \", W, \"\\n\", \", b = \", b )\n",
    "\n",
    "for step in  range(40001):  \n",
    "    \n",
    "    W -= learning_rate * numerical_derivative(f, W)\n",
    "    \n",
    "    b -= learning_rate * numerical_derivative(f, b)\n",
    "    \n",
    "    if (step % 400 == 0):\n",
    "        print(\"step = \", step, \"loss value = \", loss_val(x_data, t_data), \"W = \", W, \", b = \",b )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([178.84383468])"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_data = np.array([100, 98, 81])\n",
    "\n",
    "predict(test_data) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
