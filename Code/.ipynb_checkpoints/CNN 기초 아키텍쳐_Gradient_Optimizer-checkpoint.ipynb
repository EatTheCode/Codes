{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "컨볼루션 네트워크의 안전연결층에서는 3차원의 출력값을 1차원 벡터로 평탄화 시켜주는 작업을 수행하여 일반 신경망 연결처럼 출력층의 모든 노드와 연결시켜주는 역할을 수행한다\n",
    "\n",
    "컨볼루션 함수\n",
    "conv = tf.nn.conv2d(input, filter, strides, padding,...)\n",
    "\n",
    "input : 컨볼루션 연산을 위한 입력 데이터이며 [batch, in_height, in_width, in_channels] 예를들어, 100 개의 배치로 묶은 28X28 크기의 흑백 이미지를 입력 으로 넣을경우 input 은 [100, 28,28, 1] 로 나타냄\n",
    "\n",
    "filter : 컨볼루션 연산에 적용할 필터이며 [filter_height, filter_width, in_channels, out_channels] 예를들어, 필터 크기 3X3이며 입력채널 개수는 1이고 적용되는 필터 개수가 총 32개이면 filter 는 [3, 3, 1, 32] 로 나타냄\n",
    "\n",
    "strides: 컨볼루션 연산을 위해 필터를 이동시키는 간격을 나타냄. 예를들어 [1, 1, 1, 1] 로 strides를 나타낸다면 컨볼루션 적용을 위해 1 칸씩 이동 필터를 이동하는것을 의미함\n",
    "\n",
    "padding: ‘SAME’ 또는 ‘VALID’ 값을 가짐. padding=‘VALID’ 라면 컨볼루션 연산 공식에 의해서 가로/세로(차원) 크기가 축소된 결과가 리턴됨. 그러나 padding=‘SAME’ 으로 지정하면 입력 값의 가로/세로(차원) 크기와 같은 출력이 리턴되도록 작아진 차원 부분에 0 값을 채운 제로패딩을 수행함\n",
    "\n",
    "* 입력채널 : 데이터의 이동 통로"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "pooling 함수 tf.nn.max_pool(value, ksize, strides, padding,...)\n",
    "\n",
    "value: [batch, height, width, channels] 형식의 입력데이터. 일반적으로 relu 를 통과한 출력결과를 말한다\n",
    "\n",
    "ksize: 컨볼루션 신경망에서 일반적인 ksize는 다음과 같이 [1, height, width, 1] 형태로 표시함. 예를 들어 ksize = [1, 2, 2, 1]이라면 2칸씩 이동하면서 출력결과 1 개를 만들어 낸다는 것을 의미함. 즉 4개 (2X2) 데이터 중에서 가장 큰 값 1 개를 찾아서 반환하는 역할을 수행함. \n",
    "만약 ksize = [1, 3, 3, 1] 이라고 하면 3칸씩 이동, 즉 9개 (3X3) 데이터 중에서 가장 큰 값을 찾는다는 의미임\n",
    "\n",
    "strides: max pooling을 위해 윈도우를 이동시키는 간격을 나타냄. 예를들어 [1, 2, 2, 1] 로 strides를 나타낸다면 max pooling 적용을 위해 2 칸씩 이동하는 것을 의미함\n",
    "\n",
    "padding: max pooling 에서의 padding 값은 max pooling 을 수행하기에는 데이터가 부족한 경우에 주변을 0 등으로 채워주는 역할을 함. 예를들어 max pooling 에서 풀링층으로 들어오는 입력데이터가 7X7 이고, 데이터를 2개씩 묶어 최대값을 찾아내는 연산을 하기에는 입력으로 주어진 데이터가 부족한 상황임 (즉, 최소 8X8 이어야 가능). 이때padding=‘SAME’ 이면, 부족한 데이터 부분을 0 등으로 채운 후에 데이터를 2개씩 묶어 최대값을 뽑아낼 수 있음\n",
    "\n",
    "채널마다 바이어스가 존재해야함\n",
    "필터가 32개면 바이어스도 32개\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From <ipython-input-2-6e59b9544fb8>:6: read_data_sets (from tensorflow.contrib.learn.python.learn.datasets.mnist) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use alternatives such as official/mnist/dataset.py from tensorflow/models.\n",
      "WARNING:tensorflow:From c:\\users\\hogeu\\anaconda3\\envs\\tensorflow\\lib\\site-packages\\tensorflow_core\\contrib\\learn\\python\\learn\\datasets\\mnist.py:260: maybe_download (from tensorflow.contrib.learn.python.learn.datasets.base) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please write your own downloading logic.\n",
      "WARNING:tensorflow:From c:\\users\\hogeu\\anaconda3\\envs\\tensorflow\\lib\\site-packages\\tensorflow_core\\contrib\\learn\\python\\learn\\datasets\\base.py:252: _internal_retry.<locals>.wrap.<locals>.wrapped_fn (from tensorflow.contrib.learn.python.learn.datasets.base) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use urllib or similar directly.\n",
      "Successfully downloaded train-images-idx3-ubyte.gz 9912422 bytes.\n",
      "WARNING:tensorflow:From c:\\users\\hogeu\\anaconda3\\envs\\tensorflow\\lib\\site-packages\\tensorflow_core\\contrib\\learn\\python\\learn\\datasets\\mnist.py:262: extract_images (from tensorflow.contrib.learn.python.learn.datasets.mnist) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use tf.data to implement this functionality.\n",
      "Extracting MNIST_data/train-images-idx3-ubyte.gz\n",
      "Successfully downloaded train-labels-idx1-ubyte.gz 28881 bytes.\n",
      "WARNING:tensorflow:From c:\\users\\hogeu\\anaconda3\\envs\\tensorflow\\lib\\site-packages\\tensorflow_core\\contrib\\learn\\python\\learn\\datasets\\mnist.py:267: extract_labels (from tensorflow.contrib.learn.python.learn.datasets.mnist) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use tf.data to implement this functionality.\n",
      "Extracting MNIST_data/train-labels-idx1-ubyte.gz\n",
      "WARNING:tensorflow:From c:\\users\\hogeu\\anaconda3\\envs\\tensorflow\\lib\\site-packages\\tensorflow_core\\contrib\\learn\\python\\learn\\datasets\\mnist.py:110: dense_to_one_hot (from tensorflow.contrib.learn.python.learn.datasets.mnist) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use tf.one_hot on tensors.\n",
      "Successfully downloaded t10k-images-idx3-ubyte.gz 1648877 bytes.\n",
      "Extracting MNIST_data/t10k-images-idx3-ubyte.gz\n",
      "Successfully downloaded t10k-labels-idx1-ubyte.gz 4542 bytes.\n",
      "Extracting MNIST_data/t10k-labels-idx1-ubyte.gz\n",
      "WARNING:tensorflow:From c:\\users\\hogeu\\anaconda3\\envs\\tensorflow\\lib\\site-packages\\tensorflow_core\\contrib\\learn\\python\\learn\\datasets\\mnist.py:290: DataSet.__init__ (from tensorflow.contrib.learn.python.learn.datasets.mnist) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use alternatives such as official/mnist/dataset.py from tensorflow/models.\n",
      "\n",
      "train.num =  55000 , test.num =  10000  , validation.num =  5000\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.examples.tutorials.mnist import input_data\n",
    "import numpy as np\n",
    "from datetime import datetime\n",
    "\n",
    "mnist = input_data.read_data_sets(\"MNIST_data/\", one_hot = True)\n",
    "\n",
    "print(\"\")\n",
    "print(\"train.num = \", mnist.train.num_examples,\n",
    "     \", test.num = \", mnist.test.num_examples,\n",
    "     \" , validation.num = \", mnist.validation.num_examples)\n",
    "tf.__version__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Hyper Parameter\n",
    "\n",
    "learning_rate = 0.0001\n",
    "epochs = 30\n",
    "batch_size = 100\n",
    "\n",
    "#PlaceHolder\n",
    "X = tf.placeholder(tf.float32, [None, 784])\n",
    "\n",
    "T = tf.placeholder(tf.float32, [None, 10])\n",
    "\n",
    "A1 = X_img = tf.reshape(X, [-1, 28, 28, 1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "#1번째 컨볼루션 층 3X3X32 필터\n",
    "W2 = tf.Variable(tf.random_normal([3, 3, 1, 32], stddev = 0.01))\n",
    "b2 = tf.Variable(tf.random_normal([32]))\n",
    "\n",
    "#1번째 컨볼루션 연산\n",
    "C2 = tf.nn.conv2d(A1, W2, strides=[1, 1, 1, 1], padding = 'SAME')\n",
    "\n",
    "#relu\n",
    "Z2 = tf.nn.relu(C2 + b2)\n",
    "\n",
    "# 1번째 max pooling을 통해 28 X 28 X 32 => 14 X 14 X 32\n",
    "A2 = P2 = tf.nn.max_pool(Z2, ksize=[1, 2, 2, 1], strides=[1, 2, 2, 1], padding = 'SAME')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "A2_flat = P2_flat = tf.reshape(A2, [-1, 14*14*32])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "W3 = tf.Variable(tf.random_normal([14*14*32, 10], stddev = 0.01))\n",
    "b3 = tf.Variable(tf.random_normal([10]))\n",
    "\n",
    "Z3 = logits = tf.matmul(A2_flat, W3) + b3\n",
    "\n",
    "y = A3 = tf.nn.softmax(Z3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "loss = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits_v2(logits = Z3, labels = T))\n",
    "\n",
    "optimizer = tf.train.AdamOptimizer(learning_rate)\n",
    "\n",
    "train = optimizer.minimize(loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "predicted_val = tf.equal(tf.argmax(A3, 1), tf.argmax(T, 1))\n",
    "\n",
    "#batch_size X 10의 True, Flase를 1또는 0으로 변환\n",
    "accuracy = tf.reduce_mean(tf.cast(predicted_val, dtype=tf.float32))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epochs =  0 , step =  0 , loss_val =  2.7986486\n",
      "epochs =  0 , step =  100 , loss_val =  2.2545938\n",
      "epochs =  0 , step =  200 , loss_val =  2.0430956\n",
      "epochs =  0 , step =  300 , loss_val =  1.6278405\n",
      "epochs =  0 , step =  400 , loss_val =  1.3744081\n",
      "epochs =  0 , step =  500 , loss_val =  0.9543842\n",
      "epochs =  1 , step =  0 , loss_val =  0.8705112\n",
      "epochs =  1 , step =  100 , loss_val =  0.84716827\n",
      "epochs =  1 , step =  200 , loss_val =  0.5977492\n",
      "epochs =  1 , step =  300 , loss_val =  0.5445023\n",
      "epochs =  1 , step =  400 , loss_val =  0.5320075\n",
      "epochs =  1 , step =  500 , loss_val =  0.565763\n",
      "epochs =  2 , step =  0 , loss_val =  0.50711\n",
      "epochs =  2 , step =  100 , loss_val =  0.5042524\n",
      "epochs =  2 , step =  200 , loss_val =  0.43315727\n",
      "epochs =  2 , step =  300 , loss_val =  0.33906883\n",
      "epochs =  2 , step =  400 , loss_val =  0.5386726\n",
      "epochs =  2 , step =  500 , loss_val =  0.37395576\n",
      "epochs =  3 , step =  0 , loss_val =  0.3666271\n",
      "epochs =  3 , step =  100 , loss_val =  0.39896363\n",
      "epochs =  3 , step =  200 , loss_val =  0.26636788\n",
      "epochs =  3 , step =  300 , loss_val =  0.39328453\n",
      "epochs =  3 , step =  400 , loss_val =  0.40952733\n",
      "epochs =  3 , step =  500 , loss_val =  0.5347338\n",
      "epochs =  4 , step =  0 , loss_val =  0.30155167\n",
      "epochs =  4 , step =  100 , loss_val =  0.30898097\n",
      "epochs =  4 , step =  200 , loss_val =  0.42053398\n",
      "epochs =  4 , step =  300 , loss_val =  0.45888495\n",
      "epochs =  4 , step =  400 , loss_val =  0.18458156\n",
      "epochs =  4 , step =  500 , loss_val =  0.21588688\n",
      "epochs =  5 , step =  0 , loss_val =  0.361007\n",
      "epochs =  5 , step =  100 , loss_val =  0.34980756\n",
      "epochs =  5 , step =  200 , loss_val =  0.22610249\n",
      "epochs =  5 , step =  300 , loss_val =  0.3582411\n",
      "epochs =  5 , step =  400 , loss_val =  0.29303825\n",
      "epochs =  5 , step =  500 , loss_val =  0.35129392\n",
      "epochs =  6 , step =  0 , loss_val =  0.42980042\n",
      "epochs =  6 , step =  100 , loss_val =  0.3614711\n",
      "epochs =  6 , step =  200 , loss_val =  0.2999898\n",
      "epochs =  6 , step =  300 , loss_val =  0.24415341\n",
      "epochs =  6 , step =  400 , loss_val =  0.3246794\n",
      "epochs =  6 , step =  500 , loss_val =  0.23561627\n",
      "epochs =  7 , step =  0 , loss_val =  0.26521182\n",
      "epochs =  7 , step =  100 , loss_val =  0.5127658\n",
      "epochs =  7 , step =  200 , loss_val =  0.2363226\n",
      "epochs =  7 , step =  300 , loss_val =  0.21162523\n",
      "epochs =  7 , step =  400 , loss_val =  0.36264545\n",
      "epochs =  7 , step =  500 , loss_val =  0.31084237\n",
      "epochs =  8 , step =  0 , loss_val =  0.6112928\n",
      "epochs =  8 , step =  100 , loss_val =  0.29403478\n",
      "epochs =  8 , step =  200 , loss_val =  0.30402654\n",
      "epochs =  8 , step =  300 , loss_val =  0.32194793\n",
      "epochs =  8 , step =  400 , loss_val =  0.29009068\n",
      "epochs =  8 , step =  500 , loss_val =  0.5377436\n",
      "epochs =  9 , step =  0 , loss_val =  0.32262\n",
      "epochs =  9 , step =  100 , loss_val =  0.46004203\n",
      "epochs =  9 , step =  200 , loss_val =  0.36532825\n",
      "epochs =  9 , step =  300 , loss_val =  0.12596129\n",
      "epochs =  9 , step =  400 , loss_val =  0.2688208\n",
      "epochs =  9 , step =  500 , loss_val =  0.38303283\n",
      "epochs =  10 , step =  0 , loss_val =  0.2242976\n",
      "epochs =  10 , step =  100 , loss_val =  0.46864194\n",
      "epochs =  10 , step =  200 , loss_val =  0.12786502\n",
      "epochs =  10 , step =  300 , loss_val =  0.37905625\n",
      "epochs =  10 , step =  400 , loss_val =  0.3188999\n",
      "epochs =  10 , step =  500 , loss_val =  0.1457849\n",
      "epochs =  11 , step =  0 , loss_val =  0.45089027\n",
      "epochs =  11 , step =  100 , loss_val =  0.23774509\n",
      "epochs =  11 , step =  200 , loss_val =  0.30668223\n",
      "epochs =  11 , step =  300 , loss_val =  0.17684534\n",
      "epochs =  11 , step =  400 , loss_val =  0.35650036\n",
      "epochs =  11 , step =  500 , loss_val =  0.15585999\n",
      "epochs =  12 , step =  0 , loss_val =  0.24668655\n",
      "epochs =  12 , step =  100 , loss_val =  0.33221138\n",
      "epochs =  12 , step =  200 , loss_val =  0.20608903\n",
      "epochs =  12 , step =  300 , loss_val =  0.35237342\n",
      "epochs =  12 , step =  400 , loss_val =  0.3365152\n",
      "epochs =  12 , step =  500 , loss_val =  0.3371435\n",
      "epochs =  13 , step =  0 , loss_val =  0.26922828\n",
      "epochs =  13 , step =  100 , loss_val =  0.2654496\n",
      "epochs =  13 , step =  200 , loss_val =  0.4154889\n",
      "epochs =  13 , step =  300 , loss_val =  0.31566742\n",
      "epochs =  13 , step =  400 , loss_val =  0.30462295\n",
      "epochs =  13 , step =  500 , loss_val =  0.26304114\n",
      "epochs =  14 , step =  0 , loss_val =  0.19600685\n",
      "epochs =  14 , step =  100 , loss_val =  0.48039994\n",
      "epochs =  14 , step =  200 , loss_val =  0.16471687\n",
      "epochs =  14 , step =  300 , loss_val =  0.35588416\n",
      "epochs =  14 , step =  400 , loss_val =  0.30837595\n",
      "epochs =  14 , step =  500 , loss_val =  0.3451579\n",
      "epochs =  15 , step =  0 , loss_val =  0.23749168\n",
      "epochs =  15 , step =  100 , loss_val =  0.26101917\n",
      "epochs =  15 , step =  200 , loss_val =  0.22398826\n",
      "epochs =  15 , step =  300 , loss_val =  0.42371187\n",
      "epochs =  15 , step =  400 , loss_val =  0.22402114\n",
      "epochs =  15 , step =  500 , loss_val =  0.47619775\n",
      "epochs =  16 , step =  0 , loss_val =  0.17729022\n",
      "epochs =  16 , step =  100 , loss_val =  0.1498151\n",
      "epochs =  16 , step =  200 , loss_val =  0.27499312\n",
      "epochs =  16 , step =  300 , loss_val =  0.21881412\n",
      "epochs =  16 , step =  400 , loss_val =  0.14361894\n",
      "epochs =  16 , step =  500 , loss_val =  0.1740575\n",
      "epochs =  17 , step =  0 , loss_val =  0.15639566\n",
      "epochs =  17 , step =  100 , loss_val =  0.18337497\n",
      "epochs =  17 , step =  200 , loss_val =  0.39504388\n",
      "epochs =  17 , step =  300 , loss_val =  0.36130637\n",
      "epochs =  17 , step =  400 , loss_val =  0.29143354\n",
      "epochs =  17 , step =  500 , loss_val =  0.3069181\n",
      "epochs =  18 , step =  0 , loss_val =  0.28888237\n",
      "epochs =  18 , step =  100 , loss_val =  0.1939518\n",
      "epochs =  18 , step =  200 , loss_val =  0.3950342\n",
      "epochs =  18 , step =  300 , loss_val =  0.27891105\n",
      "epochs =  18 , step =  400 , loss_val =  0.24055064\n",
      "epochs =  18 , step =  500 , loss_val =  0.35625508\n",
      "epochs =  19 , step =  0 , loss_val =  0.2746373\n",
      "epochs =  19 , step =  100 , loss_val =  0.3531072\n",
      "epochs =  19 , step =  200 , loss_val =  0.221004\n",
      "epochs =  19 , step =  300 , loss_val =  0.37852395\n",
      "epochs =  19 , step =  400 , loss_val =  0.1976691\n",
      "epochs =  19 , step =  500 , loss_val =  0.115485385\n",
      "epochs =  20 , step =  0 , loss_val =  0.3880648\n",
      "epochs =  20 , step =  100 , loss_val =  0.23480621\n",
      "epochs =  20 , step =  200 , loss_val =  0.28681132\n",
      "epochs =  20 , step =  300 , loss_val =  0.22073257\n",
      "epochs =  20 , step =  400 , loss_val =  0.2835379\n",
      "epochs =  20 , step =  500 , loss_val =  0.38783357\n",
      "epochs =  21 , step =  0 , loss_val =  0.24945581\n",
      "epochs =  21 , step =  100 , loss_val =  0.2163058\n",
      "epochs =  21 , step =  200 , loss_val =  0.16009434\n",
      "epochs =  21 , step =  300 , loss_val =  0.3129157\n",
      "epochs =  21 , step =  400 , loss_val =  0.1590593\n",
      "epochs =  21 , step =  500 , loss_val =  0.32542273\n",
      "epochs =  22 , step =  0 , loss_val =  0.35534096\n",
      "epochs =  22 , step =  100 , loss_val =  0.3179156\n",
      "epochs =  22 , step =  200 , loss_val =  0.17342106\n",
      "epochs =  22 , step =  300 , loss_val =  0.23852383\n",
      "epochs =  22 , step =  400 , loss_val =  0.22372478\n",
      "epochs =  22 , step =  500 , loss_val =  0.30081522\n",
      "epochs =  23 , step =  0 , loss_val =  0.13450569\n",
      "epochs =  23 , step =  100 , loss_val =  0.16199093\n",
      "epochs =  23 , step =  200 , loss_val =  0.2007373\n",
      "epochs =  23 , step =  300 , loss_val =  0.2602748\n",
      "epochs =  23 , step =  400 , loss_val =  0.2557689\n",
      "epochs =  23 , step =  500 , loss_val =  0.2885459\n",
      "epochs =  24 , step =  0 , loss_val =  0.17893286\n",
      "epochs =  24 , step =  100 , loss_val =  0.17559937\n",
      "epochs =  24 , step =  200 , loss_val =  0.25089476\n"
     ]
    }
   ],
   "source": [
    "with tf.Session() as sess:\n",
    "    \n",
    "    sess.run(tf.global_variables_initializer())\n",
    "    \n",
    "    start_time = datetime.now()\n",
    "    \n",
    "    for i in range(epochs):\n",
    "        total_batch = int(mnist.train.num_examples / batch_size)\n",
    "        \n",
    "        for step in range(total_batch):\n",
    "            \n",
    "            batch_x_data, batch_t_data = mnist.train.next_batch(batch_size)\n",
    "            \n",
    "            loss_val, _ = sess.run([loss, train], feed_dict = {X : batch_x_data, T: batch_t_data})\n",
    "            \n",
    "            if step % 100 == 0:\n",
    "                print(\"epochs = \", i, \", step = \", step, \", loss_val = \", loss_val)\n",
    "                \n",
    "    end_time = datetime.now()\n",
    "    \n",
    "    print(\"\\nAccuracy = \", accuracy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
