{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 629
    },
    "colab_type": "code",
    "id": "hii0aBNc3dBu",
    "outputId": "de6958e6-58dd-44cc-aab0-8f631490baef"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From <ipython-input-1-026760e93c65>:6: read_data_sets (from tensorflow.contrib.learn.python.learn.datasets.mnist) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use alternatives such as official/mnist/dataset.py from tensorflow/models.\n",
      "WARNING:tensorflow:From c:\\users\\hogeu\\anaconda3\\envs\\tensorflow\\lib\\site-packages\\tensorflow_core\\contrib\\learn\\python\\learn\\datasets\\mnist.py:260: maybe_download (from tensorflow.contrib.learn.python.learn.datasets.base) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please write your own downloading logic.\n",
      "WARNING:tensorflow:From c:\\users\\hogeu\\anaconda3\\envs\\tensorflow\\lib\\site-packages\\tensorflow_core\\contrib\\learn\\python\\learn\\datasets\\mnist.py:262: extract_images (from tensorflow.contrib.learn.python.learn.datasets.mnist) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use tf.data to implement this functionality.\n",
      "Extracting MNIST_data/train-images-idx3-ubyte.gz\n",
      "WARNING:tensorflow:From c:\\users\\hogeu\\anaconda3\\envs\\tensorflow\\lib\\site-packages\\tensorflow_core\\contrib\\learn\\python\\learn\\datasets\\mnist.py:267: extract_labels (from tensorflow.contrib.learn.python.learn.datasets.mnist) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use tf.data to implement this functionality.\n",
      "Extracting MNIST_data/train-labels-idx1-ubyte.gz\n",
      "WARNING:tensorflow:From c:\\users\\hogeu\\anaconda3\\envs\\tensorflow\\lib\\site-packages\\tensorflow_core\\contrib\\learn\\python\\learn\\datasets\\mnist.py:110: dense_to_one_hot (from tensorflow.contrib.learn.python.learn.datasets.mnist) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use tf.one_hot on tensors.\n",
      "Extracting MNIST_data/t10k-images-idx3-ubyte.gz\n",
      "Extracting MNIST_data/t10k-labels-idx1-ubyte.gz\n",
      "WARNING:tensorflow:From c:\\users\\hogeu\\anaconda3\\envs\\tensorflow\\lib\\site-packages\\tensorflow_core\\contrib\\learn\\python\\learn\\datasets\\mnist.py:290: DataSet.__init__ (from tensorflow.contrib.learn.python.learn.datasets.mnist) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use alternatives such as official/mnist/dataset.py from tensorflow/models.\n",
      "\n",
      "train.num =  55000 , test.num =  10000 , validation.num =  5000\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.examples.tutorials.mnist import input_data\n",
    "import numpy as np\n",
    "from datetime import datetime\n",
    "\n",
    "mnist = input_data.read_data_sets(\"MNIST_data/\", one_hot=True)\n",
    "\n",
    "print(\"\")\n",
    "print(\"train.num = \", mnist.train.num_examples, \n",
    "      \", test.num = \", mnist.test.num_examples, \n",
    "      \", validation.num = \", mnist.validation.num_examples) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "8bAVGMSD3dB9"
   },
   "source": [
    "#### shape 및 type(mnist) 확인"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 213
    },
    "colab_type": "code",
    "id": "GV7tnrcl3dB-",
    "outputId": "6c0d8236-5b34-4ebe-f06e-ac8802318de4"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "type(mnist) =  <class 'tensorflow.contrib.learn.python.learn.datasets.base.Datasets'> , type(mnist.train.images) =  <class 'numpy.ndarray'> , type(mnist.train.labels) =  <class 'numpy.ndarray'>\n",
      "\n",
      "train image shape =  (55000, 784)\n",
      "train label shape =  (55000, 10)\n",
      "test image shape =  (10000, 784)\n",
      "test label shape =  (10000, 10)\n",
      "\n",
      "train image shape =  (55000, 784)\n",
      "test image shape =  (10000, 784)\n",
      "validation image shape =  (5000, 784)\n"
     ]
    }
   ],
   "source": [
    "print(\"type(mnist) = \", type(mnist), \n",
    "      \", type(mnist.train.images) = \", type(mnist.train.images), \n",
    "      \", type(mnist.train.labels) = \", type(mnist.train.labels))\n",
    "\n",
    "print(\"\\ntrain image shape = \", np.shape(mnist.train.images))\n",
    "print(\"train label shape = \", np.shape(mnist.train.labels))\n",
    "print(\"test image shape = \", np.shape(mnist.test.images))\n",
    "print(\"test label shape = \", np.shape(mnist.test.labels))\n",
    "\n",
    "print(\"\\ntrain image shape = \", mnist.train.images.shape)\n",
    "print(\"test image shape = \", mnist.test.images.shape)\n",
    "print(\"validation image shape = \", mnist.validation.images.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "3ohhMTGX3dCC"
   },
   "source": [
    "#### train data 정규화 및 label 의 one-hot encoding 확인"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "colab_type": "code",
    "id": "Q9U5KMpD3dCE",
    "outputId": "73f2cb1e-f9b0-4636-b7e6-a795b8a13edd"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=====================================================================\n",
      "정규화 확인\n",
      "\n",
      "len(mnist.train.images[0]) =  784\n",
      "[0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.3803922  0.37647063 0.3019608\n",
      " 0.46274513 0.2392157  0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.3529412\n",
      " 0.5411765  0.9215687  0.9215687  0.9215687  0.9215687  0.9215687\n",
      " 0.9215687  0.9843138  0.9843138  0.9725491  0.9960785  0.9607844\n",
      " 0.9215687  0.74509805 0.08235294 0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.54901963 0.9843138  0.9960785  0.9960785\n",
      " 0.9960785  0.9960785  0.9960785  0.9960785  0.9960785  0.9960785\n",
      " 0.9960785  0.9960785  0.9960785  0.9960785  0.9960785  0.9960785\n",
      " 0.7411765  0.09019608 0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.8862746  0.9960785  0.81568635 0.7803922  0.7803922  0.7803922\n",
      " 0.7803922  0.54509807 0.2392157  0.2392157  0.2392157  0.2392157\n",
      " 0.2392157  0.5019608  0.8705883  0.9960785  0.9960785  0.7411765\n",
      " 0.08235294 0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.14901961 0.32156864\n",
      " 0.0509804  0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.13333334 0.8352942  0.9960785  0.9960785  0.45098042 0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.32941177\n",
      " 0.9960785  0.9960785  0.9176471  0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.32941177 0.9960785  0.9960785\n",
      " 0.9176471  0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.4156863  0.6156863  0.9960785  0.9960785  0.95294124 0.20000002\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.09803922\n",
      " 0.45882356 0.8941177  0.8941177  0.8941177  0.9921569  0.9960785\n",
      " 0.9960785  0.9960785  0.9960785  0.94117653 0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.26666668 0.4666667  0.86274517 0.9960785  0.9960785\n",
      " 0.9960785  0.9960785  0.9960785  0.9960785  0.9960785  0.9960785\n",
      " 0.9960785  0.5568628  0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.14509805 0.73333335 0.9921569\n",
      " 0.9960785  0.9960785  0.9960785  0.8745099  0.8078432  0.8078432\n",
      " 0.29411766 0.26666668 0.8431373  0.9960785  0.9960785  0.45882356\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.4431373  0.8588236  0.9960785  0.9490197  0.89019614 0.45098042\n",
      " 0.34901962 0.12156864 0.         0.         0.         0.\n",
      " 0.7843138  0.9960785  0.9450981  0.16078432 0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.6627451  0.9960785\n",
      " 0.6901961  0.24313727 0.         0.         0.         0.\n",
      " 0.         0.         0.         0.18823531 0.9058824  0.9960785\n",
      " 0.9176471  0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.07058824 0.48627454 0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.32941177 0.9960785  0.9960785  0.6509804  0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.54509807\n",
      " 0.9960785  0.9333334  0.22352943 0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.8235295  0.9803922  0.9960785  0.65882355\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.9490197  0.9960785  0.93725497 0.22352943 0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.34901962 0.9843138  0.9450981\n",
      " 0.3372549  0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.01960784 0.8078432  0.96470594 0.6156863  0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.01568628 0.45882356\n",
      " 0.27058825 0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.        ]\n",
      "=====================================================================\n",
      "One-Hot Encoding 확인\n",
      "\n",
      "len(mnist.train.labels[0]) =  10\n",
      "[0. 0. 0. 0. 0. 0. 0. 1. 0. 0.]\n"
     ]
    }
   ],
   "source": [
    "# 정규화 / One-Hot Encoding 확인\n",
    "print('=====================================================================')\n",
    "print('정규화 확인\\n')\n",
    "print('len(mnist.train.images[0]) = ', len(mnist.train.images[0]))\n",
    "print(mnist.train.images[0])\n",
    "print('=====================================================================')\n",
    "print('One-Hot Encoding 확인\\n')\n",
    "print('len(mnist.train.labels[0]) = ', len(mnist.train.labels[0]))\n",
    "print(mnist.train.labels[0])  # One-Hot Encoding 확인"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "modsPHJk3dCN"
   },
   "source": [
    "#### Hyper-Parameter 설정"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "-A1Zg62n3dCO"
   },
   "outputs": [],
   "source": [
    "# 입력노드, 은닉노드, 출력노드, 학습율, 반복횟수, 배치 개수 등 설정\n",
    "learning_rate = 0.1  # 학습율\n",
    "epochs = 100            # 반복횟수\n",
    "batch_size = 100      # 한번에 입력으로 주어지는 MNIST 개수\n",
    "\n",
    "input_nodes = 784     # 입력노드 개수\n",
    "hidden_nodes = 100    # 은닉노드 개수\n",
    "output_nodes = 10     # 출력노드 개수"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "AiUHKuBe3dCW"
   },
   "source": [
    "#### 입력과 출력을 위한 플레이스홀더 정의"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "lWEwL-823dCX"
   },
   "outputs": [],
   "source": [
    "# 입력과 출력을 위한 플레이스홀더 정의\n",
    "X = tf.placeholder(tf.float32, [None, input_nodes])  \n",
    "T = tf.placeholder(tf.float32, [None, output_nodes])  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "_4h7965P3dCc"
   },
   "source": [
    "#### 가중치, 바이어스 정의"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "3h1WGBDF3dCd"
   },
   "outputs": [],
   "source": [
    "W2 = tf.Variable(tf.random_normal([input_nodes, hidden_nodes]))  # 은닉층 가중치 노드\n",
    "b2 = tf.Variable(tf.random_normal([hidden_nodes]))               # 은닉층 바이어스 노드\n",
    "\n",
    "W3 = tf.Variable(tf.random_normal([hidden_nodes, output_nodes])) # 출력층 가중치 노드\n",
    "b3 = tf.Variable(tf.random_normal([output_nodes]))               # 출력층 바이어스 노드"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "C85SQmbt3dCh"
   },
   "outputs": [],
   "source": [
    "Z2 = tf.matmul(X, W2) + b2    # 선형회귀 선형회귀 값 Z2\n",
    "A2 = tf.nn.relu(Z2)           # 은닉층 출력 값 A2, sigmoid 대신 relu 사용\n",
    "\n",
    "# 출력층 선형회귀  값 Z3, 즉 softmax 에 들어가는 입력 값\n",
    "Z3 = logits = tf.matmul(A2, W3) + b3   \n",
    "\n",
    "y = A3 = tf.nn.softmax(Z3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "SA4oMwL73dCl"
   },
   "outputs": [],
   "source": [
    "loss = tf.reduce_mean( tf.nn.softmax_cross_entropy_with_logits_v2(logits=Z3, labels=T) )\n",
    "\n",
    "optimizer = tf.train.GradientDescentOptimizer(learning_rate)\n",
    "\n",
    "train = optimizer.minimize(loss)  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "AAtYhqFg3dCp"
   },
   "outputs": [],
   "source": [
    "# batch_size X 10 데이터에 대해 argmax를 통해 행단위로 비교함\n",
    "predicted_val = tf.equal( tf.argmax(A3, 1), tf.argmax(T, 1) )\n",
    "\n",
    "# batch_size X 10 의 True, False 를 1 또는 0 으로 변환\n",
    "accuracy = tf.reduce_mean(tf.cast(predicted_val, dtype=tf.float32))\n",
    "\n",
    "# index list 출력(인덱스 값이 101110이런식으로 들어감)\n",
    "accuracy_index = tf.cast(predicted_val, dtype=tf.float32)\n",
    "\n",
    "# 예측값 처리\n",
    "predicted_list = tf.argmax(A3, 1)\n",
    "# 예측값이 리스트로 들어감"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 52
    },
    "colab_type": "code",
    "id": "yTd24gV23dCt",
    "outputId": "c7b65d94-7ce8-44e0-9e39-714ca71abfa1"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "type(predicted_val) =  <class 'tensorflow.python.framework.ops.Tensor'> , type(accuracy) =  <class 'tensorflow.python.framework.ops.Tensor'>\n",
      "type(accuracy_index) = <class 'tensorflow.python.framework.ops.Tensor'> , type(predicted_list) =  <class 'tensorflow.python.framework.ops.Tensor'>\n"
     ]
    }
   ],
   "source": [
    "# Type Check\n",
    "print('type(predicted_val) = ', type(predicted_val),  ', type(accuracy) = ', type(accuracy))\n",
    "print('type(accuracy_index) =', type(accuracy_index), ', type(predicted_list) = ', type(predicted_list))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "colab_type": "code",
    "id": "0msF9q3x3dCx",
    "outputId": "b226b632-dad1-4bbe-e463-c95748efbfc0"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epochs =  0 , step =  0 , loss_val =  88.258484\n",
      "epochs =  0 , step =  100 , loss_val =  4.285051\n",
      "epochs =  0 , step =  200 , loss_val =  2.5850124\n",
      "epochs =  0 , step =  300 , loss_val =  2.767647\n",
      "epochs =  0 , step =  400 , loss_val =  2.4639204\n",
      "epochs =  0 , step =  500 , loss_val =  2.4137762\n",
      "epochs =  1 , step =  0 , loss_val =  0.92336524\n",
      "epochs =  1 , step =  100 , loss_val =  0.6123057\n",
      "epochs =  1 , step =  200 , loss_val =  1.9457217\n",
      "epochs =  1 , step =  300 , loss_val =  0.87248504\n",
      "epochs =  1 , step =  400 , loss_val =  0.48372114\n",
      "epochs =  1 , step =  500 , loss_val =  1.1968483\n",
      "epochs =  2 , step =  0 , loss_val =  0.77534056\n",
      "epochs =  2 , step =  100 , loss_val =  1.0531557\n",
      "epochs =  2 , step =  200 , loss_val =  0.31741643\n",
      "epochs =  2 , step =  300 , loss_val =  0.30191064\n",
      "epochs =  2 , step =  400 , loss_val =  1.233346\n",
      "epochs =  2 , step =  500 , loss_val =  0.62638646\n",
      "epochs =  3 , step =  0 , loss_val =  0.63183206\n",
      "epochs =  3 , step =  100 , loss_val =  0.46002632\n",
      "epochs =  3 , step =  200 , loss_val =  0.8141959\n",
      "epochs =  3 , step =  300 , loss_val =  0.31618318\n",
      "epochs =  3 , step =  400 , loss_val =  0.47613433\n",
      "epochs =  3 , step =  500 , loss_val =  0.2475674\n",
      "epochs =  4 , step =  0 , loss_val =  0.2432351\n",
      "epochs =  4 , step =  100 , loss_val =  0.6441359\n",
      "epochs =  4 , step =  200 , loss_val =  0.25267398\n",
      "epochs =  4 , step =  300 , loss_val =  0.7825965\n",
      "epochs =  4 , step =  400 , loss_val =  0.5128345\n",
      "epochs =  4 , step =  500 , loss_val =  0.33484948\n",
      "epochs =  5 , step =  0 , loss_val =  0.49273437\n",
      "epochs =  5 , step =  100 , loss_val =  0.30353886\n",
      "epochs =  5 , step =  200 , loss_val =  0.40381828\n",
      "epochs =  5 , step =  300 , loss_val =  0.4050886\n",
      "epochs =  5 , step =  400 , loss_val =  0.37924674\n",
      "epochs =  5 , step =  500 , loss_val =  0.44629762\n",
      "epochs =  6 , step =  0 , loss_val =  0.38654256\n",
      "epochs =  6 , step =  100 , loss_val =  0.3802047\n",
      "epochs =  6 , step =  200 , loss_val =  0.4605417\n",
      "epochs =  6 , step =  300 , loss_val =  0.31776887\n",
      "epochs =  6 , step =  400 , loss_val =  0.35981902\n",
      "epochs =  6 , step =  500 , loss_val =  0.4339584\n",
      "epochs =  7 , step =  0 , loss_val =  0.47323895\n",
      "epochs =  7 , step =  100 , loss_val =  0.31777874\n",
      "epochs =  7 , step =  200 , loss_val =  0.31896192\n",
      "epochs =  7 , step =  300 , loss_val =  0.5189491\n",
      "epochs =  7 , step =  400 , loss_val =  0.2449736\n",
      "epochs =  7 , step =  500 , loss_val =  0.52179587\n",
      "epochs =  8 , step =  0 , loss_val =  0.4634063\n",
      "epochs =  8 , step =  100 , loss_val =  0.4642063\n",
      "epochs =  8 , step =  200 , loss_val =  0.11791717\n",
      "epochs =  8 , step =  300 , loss_val =  0.3729203\n",
      "epochs =  8 , step =  400 , loss_val =  0.2991371\n",
      "epochs =  8 , step =  500 , loss_val =  0.32557142\n",
      "epochs =  9 , step =  0 , loss_val =  0.35719055\n",
      "epochs =  9 , step =  100 , loss_val =  0.56073475\n",
      "epochs =  9 , step =  200 , loss_val =  0.43049073\n",
      "epochs =  9 , step =  300 , loss_val =  0.24856181\n",
      "epochs =  9 , step =  400 , loss_val =  0.21018295\n",
      "epochs =  9 , step =  500 , loss_val =  0.20152977\n",
      "epochs =  10 , step =  0 , loss_val =  0.20751162\n",
      "epochs =  10 , step =  100 , loss_val =  0.19849648\n",
      "epochs =  10 , step =  200 , loss_val =  0.19349778\n",
      "epochs =  10 , step =  300 , loss_val =  0.35813698\n",
      "epochs =  10 , step =  400 , loss_val =  0.40592796\n",
      "epochs =  10 , step =  500 , loss_val =  0.15518813\n",
      "epochs =  11 , step =  0 , loss_val =  0.20175768\n",
      "epochs =  11 , step =  100 , loss_val =  0.20012605\n",
      "epochs =  11 , step =  200 , loss_val =  0.19051853\n",
      "epochs =  11 , step =  300 , loss_val =  0.35060912\n",
      "epochs =  11 , step =  400 , loss_val =  0.28328398\n",
      "epochs =  11 , step =  500 , loss_val =  0.57105374\n",
      "epochs =  12 , step =  0 , loss_val =  0.26144904\n",
      "epochs =  12 , step =  100 , loss_val =  0.19828887\n",
      "epochs =  12 , step =  200 , loss_val =  0.22325303\n",
      "epochs =  12 , step =  300 , loss_val =  0.1927985\n",
      "epochs =  12 , step =  400 , loss_val =  0.26778576\n",
      "epochs =  12 , step =  500 , loss_val =  0.5738043\n",
      "epochs =  13 , step =  0 , loss_val =  0.33345914\n",
      "epochs =  13 , step =  100 , loss_val =  0.23265907\n",
      "epochs =  13 , step =  200 , loss_val =  0.39438155\n",
      "epochs =  13 , step =  300 , loss_val =  0.3850479\n",
      "epochs =  13 , step =  400 , loss_val =  0.13116884\n",
      "epochs =  13 , step =  500 , loss_val =  0.26923862\n",
      "epochs =  14 , step =  0 , loss_val =  0.34996736\n",
      "epochs =  14 , step =  100 , loss_val =  0.31573257\n",
      "epochs =  14 , step =  200 , loss_val =  0.32304564\n",
      "epochs =  14 , step =  300 , loss_val =  0.16793251\n",
      "epochs =  14 , step =  400 , loss_val =  0.32646692\n",
      "epochs =  14 , step =  500 , loss_val =  0.20673656\n",
      "epochs =  15 , step =  0 , loss_val =  0.3111151\n",
      "epochs =  15 , step =  100 , loss_val =  0.17196581\n",
      "epochs =  15 , step =  200 , loss_val =  0.25752965\n",
      "epochs =  15 , step =  300 , loss_val =  0.2642787\n",
      "epochs =  15 , step =  400 , loss_val =  0.0598804\n",
      "epochs =  15 , step =  500 , loss_val =  0.43742904\n",
      "epochs =  16 , step =  0 , loss_val =  0.21272136\n",
      "epochs =  16 , step =  100 , loss_val =  0.34628892\n",
      "epochs =  16 , step =  200 , loss_val =  0.15080297\n",
      "epochs =  16 , step =  300 , loss_val =  0.1651191\n",
      "epochs =  16 , step =  400 , loss_val =  0.37685245\n",
      "epochs =  16 , step =  500 , loss_val =  0.25480044\n",
      "epochs =  17 , step =  0 , loss_val =  0.26006016\n",
      "epochs =  17 , step =  100 , loss_val =  0.14427647\n",
      "epochs =  17 , step =  200 , loss_val =  0.3658905\n",
      "epochs =  17 , step =  300 , loss_val =  0.1462662\n",
      "epochs =  17 , step =  400 , loss_val =  0.19245899\n",
      "epochs =  17 , step =  500 , loss_val =  0.2831872\n",
      "epochs =  18 , step =  0 , loss_val =  0.5780989\n",
      "epochs =  18 , step =  100 , loss_val =  0.14494623\n",
      "epochs =  18 , step =  200 , loss_val =  0.102536075\n",
      "epochs =  18 , step =  300 , loss_val =  0.3434462\n",
      "epochs =  18 , step =  400 , loss_val =  0.47704858\n",
      "epochs =  18 , step =  500 , loss_val =  0.21543767\n",
      "epochs =  19 , step =  0 , loss_val =  0.08339982\n",
      "epochs =  19 , step =  100 , loss_val =  0.14585274\n",
      "epochs =  19 , step =  200 , loss_val =  0.3375977\n",
      "epochs =  19 , step =  300 , loss_val =  0.10917282\n",
      "epochs =  19 , step =  400 , loss_val =  0.21890286\n",
      "epochs =  19 , step =  500 , loss_val =  0.18951845\n",
      "epochs =  20 , step =  0 , loss_val =  0.177386\n",
      "epochs =  20 , step =  100 , loss_val =  0.15519719\n",
      "epochs =  20 , step =  200 , loss_val =  0.35500816\n",
      "epochs =  20 , step =  300 , loss_val =  0.26027566\n",
      "epochs =  20 , step =  400 , loss_val =  0.29757228\n",
      "epochs =  20 , step =  500 , loss_val =  0.13291997\n",
      "epochs =  21 , step =  0 , loss_val =  0.37595657\n",
      "epochs =  21 , step =  100 , loss_val =  0.2916012\n",
      "epochs =  21 , step =  200 , loss_val =  0.53278834\n",
      "epochs =  21 , step =  300 , loss_val =  0.23936585\n",
      "epochs =  21 , step =  400 , loss_val =  0.39956722\n",
      "epochs =  21 , step =  500 , loss_val =  0.2645334\n",
      "epochs =  22 , step =  0 , loss_val =  0.37851387\n",
      "epochs =  22 , step =  100 , loss_val =  0.09405103\n",
      "epochs =  22 , step =  200 , loss_val =  0.20035942\n",
      "epochs =  22 , step =  300 , loss_val =  0.17987442\n",
      "epochs =  22 , step =  400 , loss_val =  0.17077622\n",
      "epochs =  22 , step =  500 , loss_val =  0.24777214\n",
      "epochs =  23 , step =  0 , loss_val =  0.34751853\n",
      "epochs =  23 , step =  100 , loss_val =  0.18359475\n",
      "epochs =  23 , step =  200 , loss_val =  0.26132488\n",
      "epochs =  23 , step =  300 , loss_val =  0.18621044\n",
      "epochs =  23 , step =  400 , loss_val =  0.20573841\n",
      "epochs =  23 , step =  500 , loss_val =  0.26078904\n",
      "epochs =  24 , step =  0 , loss_val =  0.18674973\n",
      "epochs =  24 , step =  100 , loss_val =  0.102564335\n",
      "epochs =  24 , step =  200 , loss_val =  0.26541477\n",
      "epochs =  24 , step =  300 , loss_val =  0.21631314\n",
      "epochs =  24 , step =  400 , loss_val =  0.1920777\n",
      "epochs =  24 , step =  500 , loss_val =  0.1439589\n",
      "epochs =  25 , step =  0 , loss_val =  0.11335454\n",
      "epochs =  25 , step =  100 , loss_val =  0.03286966\n",
      "epochs =  25 , step =  200 , loss_val =  0.25148457\n",
      "epochs =  25 , step =  300 , loss_val =  0.24398322\n",
      "epochs =  25 , step =  400 , loss_val =  0.28729373\n",
      "epochs =  25 , step =  500 , loss_val =  0.14696223\n",
      "epochs =  26 , step =  0 , loss_val =  0.1811679\n",
      "epochs =  26 , step =  100 , loss_val =  0.3053631\n",
      "epochs =  26 , step =  200 , loss_val =  0.33324546\n",
      "epochs =  26 , step =  300 , loss_val =  0.41319838\n",
      "epochs =  26 , step =  400 , loss_val =  0.12311445\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epochs =  26 , step =  500 , loss_val =  0.1757234\n",
      "epochs =  27 , step =  0 , loss_val =  0.06825897\n",
      "epochs =  27 , step =  100 , loss_val =  0.16140084\n",
      "epochs =  27 , step =  200 , loss_val =  0.17088963\n",
      "epochs =  27 , step =  300 , loss_val =  0.42793834\n",
      "epochs =  27 , step =  400 , loss_val =  0.20390776\n",
      "epochs =  27 , step =  500 , loss_val =  0.19542143\n",
      "epochs =  28 , step =  0 , loss_val =  0.2769401\n",
      "epochs =  28 , step =  100 , loss_val =  0.13391577\n",
      "epochs =  28 , step =  200 , loss_val =  0.12400705\n",
      "epochs =  28 , step =  300 , loss_val =  0.23350632\n",
      "epochs =  28 , step =  400 , loss_val =  0.2515384\n",
      "epochs =  28 , step =  500 , loss_val =  0.21470365\n",
      "epochs =  29 , step =  0 , loss_val =  0.14265144\n",
      "epochs =  29 , step =  100 , loss_val =  0.08708371\n",
      "epochs =  29 , step =  200 , loss_val =  0.1154459\n",
      "epochs =  29 , step =  300 , loss_val =  0.1910579\n",
      "epochs =  29 , step =  400 , loss_val =  0.26933688\n",
      "epochs =  29 , step =  500 , loss_val =  0.08846429\n",
      "epochs =  30 , step =  0 , loss_val =  0.24990472\n",
      "epochs =  30 , step =  100 , loss_val =  0.10358986\n",
      "epochs =  30 , step =  200 , loss_val =  0.098199084\n",
      "epochs =  30 , step =  300 , loss_val =  0.13920166\n",
      "epochs =  30 , step =  400 , loss_val =  0.06180418\n",
      "epochs =  30 , step =  500 , loss_val =  0.17432915\n",
      "epochs =  31 , step =  0 , loss_val =  0.23615088\n",
      "epochs =  31 , step =  100 , loss_val =  0.10624411\n",
      "epochs =  31 , step =  200 , loss_val =  0.13301711\n",
      "epochs =  31 , step =  300 , loss_val =  0.39980066\n",
      "epochs =  31 , step =  400 , loss_val =  0.12163418\n",
      "epochs =  31 , step =  500 , loss_val =  0.22762835\n",
      "epochs =  32 , step =  0 , loss_val =  0.18683146\n",
      "epochs =  32 , step =  100 , loss_val =  0.116954476\n",
      "epochs =  32 , step =  200 , loss_val =  0.20014073\n",
      "epochs =  32 , step =  300 , loss_val =  0.10647997\n",
      "epochs =  32 , step =  400 , loss_val =  0.25645885\n",
      "epochs =  32 , step =  500 , loss_val =  0.33348247\n",
      "epochs =  33 , step =  0 , loss_val =  0.18175879\n",
      "epochs =  33 , step =  100 , loss_val =  0.1396853\n",
      "epochs =  33 , step =  200 , loss_val =  0.097473204\n",
      "epochs =  33 , step =  300 , loss_val =  0.1897406\n",
      "epochs =  33 , step =  400 , loss_val =  0.07903717\n",
      "epochs =  33 , step =  500 , loss_val =  0.0834277\n",
      "epochs =  34 , step =  0 , loss_val =  0.15165569\n",
      "epochs =  34 , step =  100 , loss_val =  0.10149478\n",
      "epochs =  34 , step =  200 , loss_val =  0.2728735\n",
      "epochs =  34 , step =  300 , loss_val =  0.1560338\n",
      "epochs =  34 , step =  400 , loss_val =  0.055403806\n",
      "epochs =  34 , step =  500 , loss_val =  0.20549929\n",
      "epochs =  35 , step =  0 , loss_val =  0.075314805\n",
      "epochs =  35 , step =  100 , loss_val =  0.26204684\n",
      "epochs =  35 , step =  200 , loss_val =  0.17070098\n",
      "epochs =  35 , step =  300 , loss_val =  0.21850443\n",
      "epochs =  35 , step =  400 , loss_val =  0.11882725\n",
      "epochs =  35 , step =  500 , loss_val =  0.1861874\n",
      "epochs =  36 , step =  0 , loss_val =  0.14989994\n",
      "epochs =  36 , step =  100 , loss_val =  0.10961792\n",
      "epochs =  36 , step =  200 , loss_val =  0.3212014\n",
      "epochs =  36 , step =  300 , loss_val =  0.10333067\n",
      "epochs =  36 , step =  400 , loss_val =  0.07773778\n",
      "epochs =  36 , step =  500 , loss_val =  0.11207758\n",
      "epochs =  37 , step =  0 , loss_val =  0.22492725\n",
      "epochs =  37 , step =  100 , loss_val =  0.09745757\n",
      "epochs =  37 , step =  200 , loss_val =  0.07399528\n",
      "epochs =  37 , step =  300 , loss_val =  0.24031788\n",
      "epochs =  37 , step =  400 , loss_val =  0.24772719\n",
      "epochs =  37 , step =  500 , loss_val =  0.07825626\n",
      "epochs =  38 , step =  0 , loss_val =  0.1431016\n",
      "epochs =  38 , step =  100 , loss_val =  0.14049898\n",
      "epochs =  38 , step =  200 , loss_val =  0.18355125\n",
      "epochs =  38 , step =  300 , loss_val =  0.09730482\n",
      "epochs =  38 , step =  400 , loss_val =  0.08308381\n",
      "epochs =  38 , step =  500 , loss_val =  0.17684445\n",
      "epochs =  39 , step =  0 , loss_val =  0.14949082\n",
      "epochs =  39 , step =  100 , loss_val =  0.038604166\n",
      "epochs =  39 , step =  200 , loss_val =  0.09994392\n",
      "epochs =  39 , step =  300 , loss_val =  0.09848213\n",
      "epochs =  39 , step =  400 , loss_val =  0.21468304\n",
      "epochs =  39 , step =  500 , loss_val =  0.10696533\n",
      "epochs =  40 , step =  0 , loss_val =  0.09541279\n",
      "epochs =  40 , step =  100 , loss_val =  0.14579362\n",
      "epochs =  40 , step =  200 , loss_val =  0.15615591\n",
      "epochs =  40 , step =  300 , loss_val =  0.10789252\n",
      "epochs =  40 , step =  400 , loss_val =  0.07965248\n",
      "epochs =  40 , step =  500 , loss_val =  0.18153696\n",
      "epochs =  41 , step =  0 , loss_val =  0.23575844\n",
      "epochs =  41 , step =  100 , loss_val =  0.12594958\n",
      "epochs =  41 , step =  200 , loss_val =  0.108670026\n",
      "epochs =  41 , step =  300 , loss_val =  0.12149458\n",
      "epochs =  41 , step =  400 , loss_val =  0.16094139\n",
      "epochs =  41 , step =  500 , loss_val =  0.12302948\n",
      "epochs =  42 , step =  0 , loss_val =  0.14805073\n",
      "epochs =  42 , step =  100 , loss_val =  0.04623777\n",
      "epochs =  42 , step =  200 , loss_val =  0.064896084\n",
      "epochs =  42 , step =  300 , loss_val =  0.12949923\n",
      "epochs =  42 , step =  400 , loss_val =  0.31285858\n",
      "epochs =  42 , step =  500 , loss_val =  0.27020508\n",
      "epochs =  43 , step =  0 , loss_val =  0.17276861\n",
      "epochs =  43 , step =  100 , loss_val =  0.13798296\n",
      "epochs =  43 , step =  200 , loss_val =  0.015478637\n",
      "epochs =  43 , step =  300 , loss_val =  0.13144794\n",
      "epochs =  43 , step =  400 , loss_val =  0.21414544\n",
      "epochs =  43 , step =  500 , loss_val =  0.15883064\n",
      "epochs =  44 , step =  0 , loss_val =  0.14146882\n",
      "epochs =  44 , step =  100 , loss_val =  0.113284074\n",
      "epochs =  44 , step =  200 , loss_val =  0.3450875\n",
      "epochs =  44 , step =  300 , loss_val =  0.15161902\n",
      "epochs =  44 , step =  400 , loss_val =  0.15694836\n",
      "epochs =  44 , step =  500 , loss_val =  0.19401902\n",
      "epochs =  45 , step =  0 , loss_val =  0.15056992\n",
      "epochs =  45 , step =  100 , loss_val =  0.06724712\n",
      "epochs =  45 , step =  200 , loss_val =  0.14437729\n",
      "epochs =  45 , step =  300 , loss_val =  0.101634674\n",
      "epochs =  45 , step =  400 , loss_val =  0.08071299\n",
      "epochs =  45 , step =  500 , loss_val =  0.24532609\n",
      "epochs =  46 , step =  0 , loss_val =  0.07833497\n",
      "epochs =  46 , step =  100 , loss_val =  0.17090438\n",
      "epochs =  46 , step =  200 , loss_val =  0.13912444\n",
      "epochs =  46 , step =  300 , loss_val =  0.17387907\n",
      "epochs =  46 , step =  400 , loss_val =  0.1978894\n",
      "epochs =  46 , step =  500 , loss_val =  0.22826847\n",
      "epochs =  47 , step =  0 , loss_val =  0.11025888\n",
      "epochs =  47 , step =  100 , loss_val =  0.11851633\n",
      "epochs =  47 , step =  200 , loss_val =  0.19228375\n",
      "epochs =  47 , step =  300 , loss_val =  0.07136347\n",
      "epochs =  47 , step =  400 , loss_val =  0.101077445\n",
      "epochs =  47 , step =  500 , loss_val =  0.092961855\n",
      "epochs =  48 , step =  0 , loss_val =  0.12277754\n",
      "epochs =  48 , step =  100 , loss_val =  0.15953946\n",
      "epochs =  48 , step =  200 , loss_val =  0.13670762\n",
      "epochs =  48 , step =  300 , loss_val =  0.26421475\n",
      "epochs =  48 , step =  400 , loss_val =  0.19583769\n",
      "epochs =  48 , step =  500 , loss_val =  0.1006822\n",
      "epochs =  49 , step =  0 , loss_val =  0.059565526\n",
      "epochs =  49 , step =  100 , loss_val =  0.082022816\n",
      "epochs =  49 , step =  200 , loss_val =  0.07275589\n",
      "epochs =  49 , step =  300 , loss_val =  0.22239742\n",
      "epochs =  49 , step =  400 , loss_val =  0.053101864\n",
      "epochs =  49 , step =  500 , loss_val =  0.1339323\n",
      "epochs =  50 , step =  0 , loss_val =  0.17776775\n",
      "epochs =  50 , step =  100 , loss_val =  0.14692108\n",
      "epochs =  50 , step =  200 , loss_val =  0.20596956\n",
      "epochs =  50 , step =  300 , loss_val =  0.06231895\n",
      "epochs =  50 , step =  400 , loss_val =  0.049193382\n",
      "epochs =  50 , step =  500 , loss_val =  0.0578422\n",
      "epochs =  51 , step =  0 , loss_val =  0.17863098\n",
      "epochs =  51 , step =  100 , loss_val =  0.138907\n",
      "epochs =  51 , step =  200 , loss_val =  0.2049148\n",
      "epochs =  51 , step =  300 , loss_val =  0.046257515\n",
      "epochs =  51 , step =  400 , loss_val =  0.24685793\n",
      "epochs =  51 , step =  500 , loss_val =  0.21826008\n",
      "epochs =  52 , step =  0 , loss_val =  0.1470838\n",
      "epochs =  52 , step =  100 , loss_val =  0.110603906\n",
      "epochs =  52 , step =  200 , loss_val =  0.2328071\n",
      "epochs =  52 , step =  300 , loss_val =  0.10111643\n",
      "epochs =  52 , step =  400 , loss_val =  0.24296266\n",
      "epochs =  52 , step =  500 , loss_val =  0.079024695\n",
      "epochs =  53 , step =  0 , loss_val =  0.15486111\n",
      "epochs =  53 , step =  100 , loss_val =  0.037842445\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epochs =  53 , step =  200 , loss_val =  0.1171014\n",
      "epochs =  53 , step =  300 , loss_val =  0.15824348\n",
      "epochs =  53 , step =  400 , loss_val =  0.1082837\n",
      "epochs =  53 , step =  500 , loss_val =  0.1280882\n",
      "epochs =  54 , step =  0 , loss_val =  0.062473822\n",
      "epochs =  54 , step =  100 , loss_val =  0.2675686\n",
      "epochs =  54 , step =  200 , loss_val =  0.06623571\n",
      "epochs =  54 , step =  300 , loss_val =  0.105337426\n",
      "epochs =  54 , step =  400 , loss_val =  0.080433704\n",
      "epochs =  54 , step =  500 , loss_val =  0.13815087\n",
      "epochs =  55 , step =  0 , loss_val =  0.1058255\n",
      "epochs =  55 , step =  100 , loss_val =  0.069165744\n",
      "epochs =  55 , step =  200 , loss_val =  0.077313446\n",
      "epochs =  55 , step =  300 , loss_val =  0.33955058\n",
      "epochs =  55 , step =  400 , loss_val =  0.14454475\n",
      "epochs =  55 , step =  500 , loss_val =  0.11616917\n",
      "epochs =  56 , step =  0 , loss_val =  0.22023101\n",
      "epochs =  56 , step =  100 , loss_val =  0.055809002\n",
      "epochs =  56 , step =  200 , loss_val =  0.17223214\n",
      "epochs =  56 , step =  300 , loss_val =  0.13236427\n",
      "epochs =  56 , step =  400 , loss_val =  0.10773161\n",
      "epochs =  56 , step =  500 , loss_val =  0.17456102\n",
      "epochs =  57 , step =  0 , loss_val =  0.23903275\n",
      "epochs =  57 , step =  100 , loss_val =  0.13310444\n",
      "epochs =  57 , step =  200 , loss_val =  0.33905846\n",
      "epochs =  57 , step =  300 , loss_val =  0.09676979\n",
      "epochs =  57 , step =  400 , loss_val =  0.056271914\n",
      "epochs =  57 , step =  500 , loss_val =  0.0918086\n",
      "epochs =  58 , step =  0 , loss_val =  0.2144609\n",
      "epochs =  58 , step =  100 , loss_val =  0.17208809\n",
      "epochs =  58 , step =  200 , loss_val =  0.0510367\n",
      "epochs =  58 , step =  300 , loss_val =  0.2580686\n",
      "epochs =  58 , step =  400 , loss_val =  0.08577095\n",
      "epochs =  58 , step =  500 , loss_val =  0.16758212\n",
      "epochs =  59 , step =  0 , loss_val =  0.119210094\n",
      "epochs =  59 , step =  100 , loss_val =  0.15825492\n",
      "epochs =  59 , step =  200 , loss_val =  0.05023115\n",
      "epochs =  59 , step =  300 , loss_val =  0.12781633\n",
      "epochs =  59 , step =  400 , loss_val =  0.15618324\n",
      "epochs =  59 , step =  500 , loss_val =  0.123011395\n",
      "epochs =  60 , step =  0 , loss_val =  0.16169935\n",
      "epochs =  60 , step =  100 , loss_val =  0.21424343\n",
      "epochs =  60 , step =  200 , loss_val =  0.117032945\n",
      "epochs =  60 , step =  300 , loss_val =  0.10757797\n",
      "epochs =  60 , step =  400 , loss_val =  0.10643775\n",
      "epochs =  60 , step =  500 , loss_val =  0.031786848\n",
      "epochs =  61 , step =  0 , loss_val =  0.11543818\n",
      "epochs =  61 , step =  100 , loss_val =  0.081109345\n",
      "epochs =  61 , step =  200 , loss_val =  0.09678085\n",
      "epochs =  61 , step =  300 , loss_val =  0.114906885\n",
      "epochs =  61 , step =  400 , loss_val =  0.082290076\n",
      "epochs =  61 , step =  500 , loss_val =  0.096366994\n",
      "epochs =  62 , step =  0 , loss_val =  0.062886804\n",
      "epochs =  62 , step =  100 , loss_val =  0.110939115\n",
      "epochs =  62 , step =  200 , loss_val =  0.056561008\n",
      "epochs =  62 , step =  300 , loss_val =  0.1637138\n",
      "epochs =  62 , step =  400 , loss_val =  0.050427448\n",
      "epochs =  62 , step =  500 , loss_val =  0.050779704\n",
      "epochs =  63 , step =  0 , loss_val =  0.08997106\n",
      "epochs =  63 , step =  100 , loss_val =  0.20757702\n",
      "epochs =  63 , step =  200 , loss_val =  0.058737297\n",
      "epochs =  63 , step =  300 , loss_val =  0.051086172\n",
      "epochs =  63 , step =  400 , loss_val =  0.092125416\n",
      "epochs =  63 , step =  500 , loss_val =  0.16998495\n",
      "epochs =  64 , step =  0 , loss_val =  0.06942044\n",
      "epochs =  64 , step =  100 , loss_val =  0.100908846\n",
      "epochs =  64 , step =  200 , loss_val =  0.05863731\n",
      "epochs =  64 , step =  300 , loss_val =  0.07854876\n",
      "epochs =  64 , step =  400 , loss_val =  0.1976201\n",
      "epochs =  64 , step =  500 , loss_val =  0.23331185\n",
      "epochs =  65 , step =  0 , loss_val =  0.06703968\n",
      "epochs =  65 , step =  100 , loss_val =  0.13487004\n",
      "epochs =  65 , step =  200 , loss_val =  0.118491404\n",
      "epochs =  65 , step =  300 , loss_val =  0.07445683\n",
      "epochs =  65 , step =  400 , loss_val =  0.08136133\n",
      "epochs =  65 , step =  500 , loss_val =  0.10899656\n",
      "epochs =  66 , step =  0 , loss_val =  0.09359718\n",
      "epochs =  66 , step =  100 , loss_val =  0.15604639\n",
      "epochs =  66 , step =  200 , loss_val =  0.2244646\n",
      "epochs =  66 , step =  300 , loss_val =  0.065590955\n",
      "epochs =  66 , step =  400 , loss_val =  0.08562718\n",
      "epochs =  66 , step =  500 , loss_val =  0.06734613\n",
      "epochs =  67 , step =  0 , loss_val =  0.08155941\n",
      "epochs =  67 , step =  100 , loss_val =  0.14866419\n",
      "epochs =  67 , step =  200 , loss_val =  0.19538778\n",
      "epochs =  67 , step =  300 , loss_val =  0.05739794\n",
      "epochs =  67 , step =  400 , loss_val =  0.099927425\n",
      "epochs =  67 , step =  500 , loss_val =  0.11708637\n",
      "epochs =  68 , step =  0 , loss_val =  0.034078937\n",
      "epochs =  68 , step =  100 , loss_val =  0.102450006\n",
      "epochs =  68 , step =  200 , loss_val =  0.09834628\n",
      "epochs =  68 , step =  300 , loss_val =  0.06729374\n",
      "epochs =  68 , step =  400 , loss_val =  0.10341963\n",
      "epochs =  68 , step =  500 , loss_val =  0.077357896\n",
      "epochs =  69 , step =  0 , loss_val =  0.051374827\n",
      "epochs =  69 , step =  100 , loss_val =  0.07536508\n",
      "epochs =  69 , step =  200 , loss_val =  0.15350725\n",
      "epochs =  69 , step =  300 , loss_val =  0.028470535\n",
      "epochs =  69 , step =  400 , loss_val =  0.055780236\n",
      "epochs =  69 , step =  500 , loss_val =  0.11939441\n",
      "epochs =  70 , step =  0 , loss_val =  0.054130163\n",
      "epochs =  70 , step =  100 , loss_val =  0.033955507\n",
      "epochs =  70 , step =  200 , loss_val =  0.1881071\n",
      "epochs =  70 , step =  300 , loss_val =  0.08576244\n",
      "epochs =  70 , step =  400 , loss_val =  0.13077116\n",
      "epochs =  70 , step =  500 , loss_val =  0.08590454\n",
      "epochs =  71 , step =  0 , loss_val =  0.08526251\n",
      "epochs =  71 , step =  100 , loss_val =  0.17408806\n",
      "epochs =  71 , step =  200 , loss_val =  0.13492969\n",
      "epochs =  71 , step =  300 , loss_val =  0.1795073\n",
      "epochs =  71 , step =  400 , loss_val =  0.10069585\n",
      "epochs =  71 , step =  500 , loss_val =  0.09098326\n",
      "epochs =  72 , step =  0 , loss_val =  0.17668615\n",
      "epochs =  72 , step =  100 , loss_val =  0.13800962\n",
      "epochs =  72 , step =  200 , loss_val =  0.09271111\n",
      "epochs =  72 , step =  300 , loss_val =  0.10268736\n",
      "epochs =  72 , step =  400 , loss_val =  0.07478924\n",
      "epochs =  72 , step =  500 , loss_val =  0.10962956\n",
      "epochs =  73 , step =  0 , loss_val =  0.14260241\n",
      "epochs =  73 , step =  100 , loss_val =  0.13139315\n",
      "epochs =  73 , step =  200 , loss_val =  0.039386924\n",
      "epochs =  73 , step =  300 , loss_val =  0.034545716\n",
      "epochs =  73 , step =  400 , loss_val =  0.14922363\n",
      "epochs =  73 , step =  500 , loss_val =  0.051536426\n",
      "epochs =  74 , step =  0 , loss_val =  0.09223476\n",
      "epochs =  74 , step =  100 , loss_val =  0.13170448\n",
      "epochs =  74 , step =  200 , loss_val =  0.15475132\n",
      "epochs =  74 , step =  300 , loss_val =  0.07626998\n",
      "epochs =  74 , step =  400 , loss_val =  0.11331708\n",
      "epochs =  74 , step =  500 , loss_val =  0.048598643\n",
      "epochs =  75 , step =  0 , loss_val =  0.11623247\n",
      "epochs =  75 , step =  100 , loss_val =  0.13827762\n",
      "epochs =  75 , step =  200 , loss_val =  0.10325782\n",
      "epochs =  75 , step =  300 , loss_val =  0.109329015\n",
      "epochs =  75 , step =  400 , loss_val =  0.06583724\n",
      "epochs =  75 , step =  500 , loss_val =  0.226133\n",
      "epochs =  76 , step =  0 , loss_val =  0.029945776\n",
      "epochs =  76 , step =  100 , loss_val =  0.06467885\n",
      "epochs =  76 , step =  200 , loss_val =  0.10890225\n",
      "epochs =  76 , step =  300 , loss_val =  0.06027238\n",
      "epochs =  76 , step =  400 , loss_val =  0.12126721\n",
      "epochs =  76 , step =  500 , loss_val =  0.0446433\n",
      "epochs =  77 , step =  0 , loss_val =  0.12602511\n",
      "epochs =  77 , step =  100 , loss_val =  0.16485855\n",
      "epochs =  77 , step =  200 , loss_val =  0.15751268\n",
      "epochs =  77 , step =  300 , loss_val =  0.10926539\n",
      "epochs =  77 , step =  400 , loss_val =  0.062436312\n",
      "epochs =  77 , step =  500 , loss_val =  0.05800657\n",
      "epochs =  78 , step =  0 , loss_val =  0.047928352\n",
      "epochs =  78 , step =  100 , loss_val =  0.1170261\n",
      "epochs =  78 , step =  200 , loss_val =  0.035653625\n",
      "epochs =  78 , step =  300 , loss_val =  0.03757347\n",
      "epochs =  78 , step =  400 , loss_val =  0.14477429\n",
      "epochs =  78 , step =  500 , loss_val =  0.13843128\n",
      "epochs =  79 , step =  0 , loss_val =  0.12046799\n",
      "epochs =  79 , step =  100 , loss_val =  0.11763682\n",
      "epochs =  79 , step =  200 , loss_val =  0.04856708\n",
      "epochs =  79 , step =  300 , loss_val =  0.16860417\n",
      "epochs =  79 , step =  400 , loss_val =  0.084459126\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epochs =  79 , step =  500 , loss_val =  0.1478642\n",
      "epochs =  80 , step =  0 , loss_val =  0.055432625\n",
      "epochs =  80 , step =  100 , loss_val =  0.12743479\n",
      "epochs =  80 , step =  200 , loss_val =  0.030776316\n",
      "epochs =  80 , step =  300 , loss_val =  0.15349373\n",
      "epochs =  80 , step =  400 , loss_val =  0.09447918\n",
      "epochs =  80 , step =  500 , loss_val =  0.028044846\n",
      "epochs =  81 , step =  0 , loss_val =  0.03515416\n",
      "epochs =  81 , step =  100 , loss_val =  0.069205664\n",
      "epochs =  81 , step =  200 , loss_val =  0.07306589\n",
      "epochs =  81 , step =  300 , loss_val =  0.088154316\n",
      "epochs =  81 , step =  400 , loss_val =  0.21642338\n",
      "epochs =  81 , step =  500 , loss_val =  0.10338978\n",
      "epochs =  82 , step =  0 , loss_val =  0.074160315\n",
      "epochs =  82 , step =  100 , loss_val =  0.022367883\n",
      "epochs =  82 , step =  200 , loss_val =  0.10211132\n",
      "epochs =  82 , step =  300 , loss_val =  0.03528421\n",
      "epochs =  82 , step =  400 , loss_val =  0.12186744\n",
      "epochs =  82 , step =  500 , loss_val =  0.13628082\n",
      "epochs =  83 , step =  0 , loss_val =  0.07838286\n",
      "epochs =  83 , step =  100 , loss_val =  0.07450149\n",
      "epochs =  83 , step =  200 , loss_val =  0.08813408\n",
      "epochs =  83 , step =  300 , loss_val =  0.14975788\n",
      "epochs =  83 , step =  400 , loss_val =  0.062280703\n",
      "epochs =  83 , step =  500 , loss_val =  0.04468558\n",
      "epochs =  84 , step =  0 , loss_val =  0.07287962\n",
      "epochs =  84 , step =  100 , loss_val =  0.056210615\n",
      "epochs =  84 , step =  200 , loss_val =  0.051458824\n",
      "epochs =  84 , step =  300 , loss_val =  0.17065544\n",
      "epochs =  84 , step =  400 , loss_val =  0.11401605\n",
      "epochs =  84 , step =  500 , loss_val =  0.14820167\n",
      "epochs =  85 , step =  0 , loss_val =  0.09818731\n",
      "epochs =  85 , step =  100 , loss_val =  0.16474503\n",
      "epochs =  85 , step =  200 , loss_val =  0.10377619\n",
      "epochs =  85 , step =  300 , loss_val =  0.05068507\n",
      "epochs =  85 , step =  400 , loss_val =  0.031480443\n",
      "epochs =  85 , step =  500 , loss_val =  0.07905407\n",
      "epochs =  86 , step =  0 , loss_val =  0.14210403\n",
      "epochs =  86 , step =  100 , loss_val =  0.07267575\n",
      "epochs =  86 , step =  200 , loss_val =  0.09200665\n",
      "epochs =  86 , step =  300 , loss_val =  0.20333844\n",
      "epochs =  86 , step =  400 , loss_val =  0.123651326\n",
      "epochs =  86 , step =  500 , loss_val =  0.06403955\n",
      "epochs =  87 , step =  0 , loss_val =  0.21738611\n",
      "epochs =  87 , step =  100 , loss_val =  0.07189813\n",
      "epochs =  87 , step =  200 , loss_val =  0.12619318\n",
      "epochs =  87 , step =  300 , loss_val =  0.11540162\n",
      "epochs =  87 , step =  400 , loss_val =  0.20729391\n",
      "epochs =  87 , step =  500 , loss_val =  0.027709456\n",
      "epochs =  88 , step =  0 , loss_val =  0.09502941\n",
      "epochs =  88 , step =  100 , loss_val =  0.1007584\n",
      "epochs =  88 , step =  200 , loss_val =  0.09017129\n",
      "epochs =  88 , step =  300 , loss_val =  0.23505783\n",
      "epochs =  88 , step =  400 , loss_val =  0.13055177\n",
      "epochs =  88 , step =  500 , loss_val =  0.085697025\n",
      "epochs =  89 , step =  0 , loss_val =  0.0416191\n",
      "epochs =  89 , step =  100 , loss_val =  0.12806955\n",
      "epochs =  89 , step =  200 , loss_val =  0.05443409\n",
      "epochs =  89 , step =  300 , loss_val =  0.12578379\n",
      "epochs =  89 , step =  400 , loss_val =  0.055473052\n",
      "epochs =  89 , step =  500 , loss_val =  0.22192138\n",
      "epochs =  90 , step =  0 , loss_val =  0.1630655\n",
      "epochs =  90 , step =  100 , loss_val =  0.0736757\n",
      "epochs =  90 , step =  200 , loss_val =  0.039185043\n",
      "epochs =  90 , step =  300 , loss_val =  0.11341836\n",
      "epochs =  90 , step =  400 , loss_val =  0.024232602\n",
      "epochs =  90 , step =  500 , loss_val =  0.048894882\n",
      "epochs =  91 , step =  0 , loss_val =  0.074891984\n",
      "epochs =  91 , step =  100 , loss_val =  0.05411312\n",
      "epochs =  91 , step =  200 , loss_val =  0.19139646\n",
      "epochs =  91 , step =  300 , loss_val =  0.062093653\n",
      "epochs =  91 , step =  400 , loss_val =  0.054422878\n",
      "epochs =  91 , step =  500 , loss_val =  0.050216116\n",
      "epochs =  92 , step =  0 , loss_val =  0.0410325\n",
      "epochs =  92 , step =  100 , loss_val =  0.07453432\n",
      "epochs =  92 , step =  200 , loss_val =  0.108221985\n",
      "epochs =  92 , step =  300 , loss_val =  0.27683553\n",
      "epochs =  92 , step =  400 , loss_val =  0.09007982\n",
      "epochs =  92 , step =  500 , loss_val =  0.11959965\n",
      "epochs =  93 , step =  0 , loss_val =  0.13769414\n",
      "epochs =  93 , step =  100 , loss_val =  0.112725906\n",
      "epochs =  93 , step =  200 , loss_val =  0.054963168\n",
      "epochs =  93 , step =  300 , loss_val =  0.121234186\n",
      "epochs =  93 , step =  400 , loss_val =  0.12413361\n",
      "epochs =  93 , step =  500 , loss_val =  0.19105303\n",
      "epochs =  94 , step =  0 , loss_val =  0.12479608\n",
      "epochs =  94 , step =  100 , loss_val =  0.101492636\n",
      "epochs =  94 , step =  200 , loss_val =  0.0476196\n",
      "epochs =  94 , step =  300 , loss_val =  0.08465955\n",
      "epochs =  94 , step =  400 , loss_val =  0.12938865\n",
      "epochs =  94 , step =  500 , loss_val =  0.2218583\n",
      "epochs =  95 , step =  0 , loss_val =  0.07124061\n",
      "epochs =  95 , step =  100 , loss_val =  0.025768815\n",
      "epochs =  95 , step =  200 , loss_val =  0.07336869\n",
      "epochs =  95 , step =  300 , loss_val =  0.13162625\n",
      "epochs =  95 , step =  400 , loss_val =  0.033196904\n",
      "epochs =  95 , step =  500 , loss_val =  0.1203384\n",
      "epochs =  96 , step =  0 , loss_val =  0.06540157\n",
      "epochs =  96 , step =  100 , loss_val =  0.05925724\n",
      "epochs =  96 , step =  200 , loss_val =  0.10765971\n",
      "epochs =  96 , step =  300 , loss_val =  0.091401726\n",
      "epochs =  96 , step =  400 , loss_val =  0.21325922\n",
      "epochs =  96 , step =  500 , loss_val =  0.028504701\n",
      "epochs =  97 , step =  0 , loss_val =  0.22297317\n",
      "epochs =  97 , step =  100 , loss_val =  0.09609787\n",
      "epochs =  97 , step =  200 , loss_val =  0.16661783\n",
      "epochs =  97 , step =  300 , loss_val =  0.04961573\n",
      "epochs =  97 , step =  400 , loss_val =  0.29535758\n",
      "epochs =  97 , step =  500 , loss_val =  0.1180835\n",
      "epochs =  98 , step =  0 , loss_val =  0.12134534\n",
      "epochs =  98 , step =  100 , loss_val =  0.164705\n",
      "epochs =  98 , step =  200 , loss_val =  0.071977705\n",
      "epochs =  98 , step =  300 , loss_val =  0.04891797\n",
      "epochs =  98 , step =  400 , loss_val =  0.15228604\n",
      "epochs =  98 , step =  500 , loss_val =  0.11189277\n",
      "epochs =  99 , step =  0 , loss_val =  0.037980873\n",
      "epochs =  99 , step =  100 , loss_val =  0.09114379\n",
      "epochs =  99 , step =  200 , loss_val =  0.068632014\n",
      "epochs =  99 , step =  300 , loss_val =  0.09714427\n",
      "epochs =  99 , step =  400 , loss_val =  0.085566185\n",
      "epochs =  99 , step =  500 , loss_val =  0.11968712\n",
      "\n",
      "Elapsed Time =>  0:02:28.380770\n",
      "\n",
      "\n",
      "Accuracy =  0.9493\n",
      "type(accuracy_val) =  <class 'numpy.float32'> , type(predicted_list_val) =  <class 'numpy.ndarray'> , type(index_label) =  <class 'numpy.ndarray'>\n",
      "index_label.shape =  (10000,)\n",
      "length of index_label_list =  10000\n",
      "false label count =  507\n",
      "\n",
      "length of index_label_prediction_list 507\n"
     ]
    }
   ],
   "source": [
    "index_label_prediction_list = []\n",
    "# with문 안에다가 넣으면 세션이 끝나면 사라지므로 밖에다가 선언\n",
    "\n",
    "with  tf.Session()  as sess:\n",
    "    \n",
    "    sess.run(tf.global_variables_initializer())  # 변수 노드(tf.Variable) 초기화\n",
    "        \n",
    "    start_time = datetime.now()\n",
    "    \n",
    "    for i in range(epochs):    # 50 번 반복수행\n",
    "        \n",
    "        total_batch = int(mnist.train.num_examples / batch_size)  # 55,000 / 100\n",
    "\n",
    "        for step in range(total_batch):\n",
    "            \n",
    "            batch_x_data, batch_t_data = mnist.train.next_batch(batch_size)\n",
    "      \n",
    "            loss_val, _ = sess.run([loss, train], feed_dict={X: batch_x_data, T: batch_t_data})    \n",
    "        \n",
    "            if step % 100 == 0:\n",
    "                print(\"epochs = \", i, \", step = \", step, \", loss_val = \", loss_val)             \n",
    "    \n",
    "    end_time = datetime.now()\n",
    "    \n",
    "    print(\"\")\n",
    "    print(\"Elapsed Time => \", end_time-start_time)\n",
    "    print(\"\")\n",
    "    \n",
    "    # Accuracy 확인\n",
    "    test_x_data = mnist.test.images    # 10000 X 784\n",
    "    test_t_data = mnist.test.labels    # 10000 X 10\n",
    "    \n",
    "    accuracy_val, predicted_list_val, index_label = sess.run([accuracy, predicted_list, accuracy_index], feed_dict={X: test_x_data, T: test_t_data})\n",
    "    # 이부분이 굉장히 중요, 앞에선 추가된 변수들을 여기에 추가해서 세션을 돌린다\n",
    "    \n",
    "    print(\"\\nAccuracy = \", accuracy_val)\n",
    "    print(\"type(accuracy_val) = \", type(accuracy_val), ', type(predicted_list_val) = ', type(predicted_list_val), ', type(index_label) = ', type(index_label))\n",
    "    print(\"index_label.shape = \", index_label.shape)\n",
    "    \n",
    "    index_label_list = list(index_label)\n",
    "    print(\"length of index_label_list = \", len(index_label_list))\n",
    "    print(\"false label count = \", index_label_list.count([0]))\n",
    "    \n",
    "    \n",
    "    # list type 으로 디버그\n",
    "    temp_list = [] \n",
    "    \n",
    "    for index in range(len(index_label_list)):\n",
    "        \n",
    "        if index_label_list[index] == 0:\n",
    "            \n",
    "            temp_list.append(index)\n",
    "            temp_list.append(np.argmax(test_t_data[index]))  # one-hot encoding 이므로 argmax 로 정답 추출\n",
    "            temp_list.append(predicted_list_val[index])\n",
    "            \n",
    "            index_label_prediction_list.append(temp_list)\n",
    "            \n",
    "            temp_list = []\n",
    "            \n",
    "    print(\"\\nlength of index_label_prediction_list\", len(index_label_prediction_list))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "K3Yz46CcplnA"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[8, 5, 2], [115, 4, 9], [124, 7, 4], [149, 2, 3], [167, 5, 0], [193, 9, 4], [235, 9, 7], [241, 9, 8], [247, 4, 2], [250, 4, 9], [259, 6, 0], [266, 8, 0], [290, 8, 9], [300, 4, 1], [303, 2, 8], [321, 2, 7], [324, 0, 9], [340, 5, 3], [352, 5, 0], [380, 0, 5], [381, 3, 7], [412, 5, 3], [444, 2, 8], [445, 6, 0], [449, 3, 5], [464, 3, 7], [507, 3, 5], [508, 6, 5], [557, 7, 3], [565, 4, 9], [578, 3, 7], [582, 8, 2], [610, 4, 6], [613, 2, 8], [619, 1, 8], [658, 7, 4], [659, 2, 9], [667, 7, 3], [684, 7, 3], [689, 7, 4], [691, 8, 4], [707, 4, 9], [717, 0, 6], [720, 5, 6], [726, 7, 3], [738, 2, 3], [760, 4, 9], [763, 0, 2], [786, 6, 5], [834, 6, 1], [839, 8, 3], [844, 8, 5], [846, 7, 9], [874, 9, 4], [890, 3, 5], [900, 1, 3], [919, 2, 1], [924, 2, 7], [947, 8, 9], [951, 5, 9], [962, 9, 7], [965, 6, 0], [1003, 5, 3], [1014, 6, 0], [1032, 5, 8], [1039, 7, 1], [1044, 6, 8], [1062, 3, 7], [1101, 8, 3], [1107, 9, 8], [1112, 4, 6], [1131, 5, 4], [1156, 7, 1], [1187, 2, 8], [1192, 9, 4], [1194, 7, 9], [1200, 8, 5], [1226, 7, 6], [1232, 9, 4], [1234, 8, 5], [1247, 9, 3], [1256, 2, 9], [1260, 7, 1], [1319, 8, 3], [1327, 9, 3], [1364, 8, 2], [1374, 2, 3], [1378, 5, 6], [1393, 5, 7], [1410, 2, 6], [1414, 9, 7], [1422, 4, 9], [1436, 6, 8], [1440, 4, 6], [1444, 6, 7], [1467, 5, 9], [1469, 3, 5], [1470, 8, 3], [1476, 5, 3], [1494, 7, 0], [1495, 3, 2], [1496, 7, 9], [1500, 7, 8], [1520, 7, 8], [1522, 7, 9], [1525, 5, 0], [1530, 8, 7], [1549, 4, 6], [1553, 9, 3], [1581, 7, 3], [1601, 3, 7], [1609, 2, 0], [1618, 5, 8], [1621, 0, 6], [1637, 5, 3], [1640, 9, 7], [1641, 5, 8], [1670, 5, 3], [1671, 7, 9], [1681, 3, 7], [1686, 8, 5], [1709, 9, 5], [1718, 7, 2], [1721, 7, 9], [1732, 9, 3], [1782, 8, 2], [1790, 2, 8], [1800, 6, 4], [1809, 7, 9], [1819, 6, 1], [1828, 3, 9], [1850, 8, 3], [1868, 1, 3], [1880, 6, 0], [1889, 3, 1], [1901, 9, 4], [1908, 6, 4], [1913, 3, 8], [1917, 5, 8], [1941, 7, 9], [1948, 5, 4], [1952, 9, 5], [2024, 7, 9], [2040, 5, 4], [2043, 4, 8], [2053, 4, 9], [2070, 7, 9], [2093, 8, 3], [2098, 2, 0], [2099, 8, 5], [2109, 3, 7], [2118, 6, 5], [2121, 8, 1], [2130, 4, 9], [2182, 1, 2], [2185, 0, 5], [2189, 9, 1], [2224, 5, 6], [2253, 2, 8], [2272, 8, 0], [2291, 5, 3], [2293, 9, 6], [2299, 2, 7], [2326, 0, 5], [2333, 0, 2], [2369, 5, 9], [2387, 9, 1], [2395, 8, 3], [2414, 9, 4], [2422, 6, 4], [2449, 0, 5], [2454, 6, 5], [2488, 2, 4], [2497, 9, 4], [2573, 5, 3], [2574, 5, 9], [2578, 7, 2], [2584, 8, 6], [2607, 7, 1], [2610, 2, 8], [2618, 3, 5], [2631, 0, 6], [2648, 9, 0], [2654, 6, 1], [2670, 5, 8], [2721, 6, 8], [2723, 3, 2], [2736, 7, 2], [2758, 8, 0], [2760, 9, 4], [2769, 9, 4], [2780, 2, 3], [2810, 5, 3], [2896, 8, 0], [2907, 4, 9], [2915, 7, 3], [2920, 6, 4], [2921, 3, 2], [2927, 3, 2], [2932, 0, 6], [2939, 9, 7], [2953, 3, 5], [2995, 6, 5], [3005, 9, 1], [3021, 2, 7], [3030, 6, 8], [3060, 9, 7], [3067, 6, 5], [3073, 1, 3], [3102, 5, 3], [3117, 5, 9], [3130, 6, 0], [3145, 5, 8], [3166, 7, 3], [3172, 4, 9], [3206, 8, 3], [3242, 0, 6], [3254, 6, 5], [3269, 6, 2], [3289, 8, 5], [3323, 8, 3], [3330, 2, 3], [3335, 5, 3], [3336, 5, 7], [3369, 9, 1], [3377, 4, 6], [3384, 2, 6], [3385, 9, 7], [3412, 0, 7], [3422, 6, 0], [3437, 4, 9], [3460, 9, 4], [3475, 3, 7], [3490, 4, 9], [3503, 9, 1], [3520, 6, 4], [3549, 3, 2], [3558, 5, 0], [3559, 8, 5], [3567, 8, 5], [3597, 9, 3], [3662, 8, 5], [3674, 8, 3], [3702, 5, 4], [3727, 8, 4], [3749, 6, 0], [3751, 7, 2], [3767, 7, 3], [3769, 3, 8], [3778, 5, 2], [3780, 4, 6], [3796, 2, 8], [3801, 6, 0], [3808, 7, 1], [3811, 2, 3], [3818, 0, 4], [3838, 7, 1], [3853, 6, 5], [3862, 2, 3], [3869, 9, 4], [3875, 4, 2], [3893, 5, 6], [3902, 5, 3], [3906, 1, 3], [3941, 4, 6], [3946, 2, 8], [3967, 9, 4], [3968, 5, 3], [3976, 7, 1], [3985, 9, 4], [3998, 4, 6], [4000, 9, 5], [4007, 7, 4], [4056, 5, 9], [4059, 5, 8], [4063, 6, 5], [4065, 0, 9], [4075, 8, 0], [4078, 9, 3], [4152, 5, 1], [4156, 2, 3], [4163, 9, 5], [4199, 7, 5], [4224, 9, 7], [4238, 7, 3], [4248, 2, 1], [4284, 9, 5], [4294, 9, 7], [4297, 7, 3], [4306, 3, 7], [4313, 4, 9], [4315, 5, 8], [4344, 9, 3], [4355, 5, 9], [4356, 5, 6], [4374, 5, 9], [4382, 4, 9], [4400, 7, 1], [4405, 9, 4], [4443, 3, 8], [4451, 2, 8], [4497, 8, 7], [4498, 7, 8], [4500, 9, 3], [4511, 9, 4], [4523, 8, 3], [4534, 9, 8], [4536, 6, 5], [4540, 7, 3], [4547, 6, 4], [4575, 4, 2], [4615, 2, 4], [4639, 8, 9], [4671, 8, 3], [4731, 8, 7], [4735, 9, 4], [4740, 3, 1], [4761, 9, 8], [4807, 8, 3], [4816, 2, 6], [4823, 9, 6], [4837, 7, 3], [4838, 6, 5], [4860, 4, 9], [4874, 9, 0], [4879, 8, 6], [4880, 0, 8], [4886, 7, 1], [4890, 8, 6], [4924, 1, 8], [4941, 2, 6], [4943, 2, 1], [4950, 2, 3], [4952, 6, 5], [4956, 8, 4], [4966, 7, 4], [4990, 3, 2], [4995, 2, 3], [5046, 3, 5], [5138, 8, 5], [5140, 3, 6], [5176, 8, 4], [5183, 8, 4], [5210, 9, 7], [5236, 8, 4], [5246, 7, 8], [5269, 9, 4], [5331, 1, 6], [5457, 1, 8], [5495, 8, 3], [5569, 8, 0], [5600, 7, 9], [5634, 2, 3], [5642, 1, 5], [5677, 4, 6], [5734, 3, 7], [5735, 5, 6], [5749, 8, 6], [5752, 5, 3], [5754, 9, 7], [5835, 7, 9], [5842, 4, 7], [5887, 7, 9], [5913, 5, 3], [5936, 4, 9], [5937, 5, 3], [5955, 3, 8], [5957, 5, 3], [5973, 3, 8], [5981, 5, 3], [5982, 5, 3], [5997, 5, 3], [6009, 3, 8], [6026, 7, 9], [6042, 5, 3], [6043, 5, 3], [6059, 3, 8], [6065, 3, 8], [6071, 9, 3], [6093, 2, 8], [6166, 9, 1], [6390, 5, 8], [6421, 3, 2], [6505, 9, 0], [6555, 8, 7], [6557, 0, 5], [6558, 6, 3], [6560, 9, 3], [6564, 3, 9], [6571, 9, 7], [6575, 3, 4], [6577, 7, 1], [6596, 6, 2], [6597, 0, 7], [6598, 5, 0], [6605, 6, 5], [6625, 8, 7], [6632, 9, 5], [6651, 0, 1], [6710, 9, 8], [6739, 3, 2], [6744, 2, 8], [6746, 5, 6], [6749, 6, 5], [6755, 8, 7], [6769, 4, 8], [6783, 1, 6], [6817, 9, 4], [6847, 6, 4], [6912, 2, 6], [6926, 6, 4], [7121, 8, 9], [7195, 5, 0], [7208, 8, 2], [7216, 0, 5], [7220, 8, 3], [7432, 7, 2], [7451, 5, 4], [7454, 5, 4], [7492, 2, 7], [7514, 8, 6], [7574, 4, 1], [7595, 3, 8], [7858, 3, 2], [7859, 5, 0], [7886, 2, 4], [7899, 1, 8], [7900, 1, 8], [7909, 2, 9], [7918, 5, 0], [7928, 1, 8], [7990, 1, 8], [8020, 1, 8], [8047, 2, 8], [8062, 5, 8], [8091, 2, 8], [8095, 4, 1], [8102, 2, 3], [8160, 5, 3], [8246, 3, 9], [8277, 3, 8], [8278, 0, 2], [8279, 8, 4], [8325, 0, 6], [8362, 3, 5], [8408, 8, 4], [8410, 8, 6], [8493, 1, 8], [8519, 7, 3], [8520, 4, 9], [8522, 8, 6], [8527, 4, 9], [8847, 5, 6], [9008, 4, 0], [9009, 7, 2], [9015, 7, 2], [9016, 0, 5], [9019, 7, 2], [9024, 7, 2], [9051, 1, 8], [9103, 4, 9], [9163, 3, 2], [9168, 2, 8], [9210, 6, 8], [9252, 9, 7], [9253, 4, 5], [9316, 8, 9], [9342, 3, 2], [9425, 0, 6], [9482, 5, 3], [9587, 9, 4], [9613, 2, 3], [9624, 3, 8], [9642, 9, 7], [9662, 3, 2], [9698, 6, 5], [9700, 2, 0], [9709, 5, 6], [9728, 4, 8], [9729, 5, 6], [9738, 4, 6], [9744, 8, 2], [9745, 4, 2], [9749, 5, 6], [9752, 2, 0], [9755, 8, 9], [9764, 4, 2], [9768, 2, 0], [9770, 5, 0], [9777, 5, 0], [9779, 2, 0], [9792, 4, 1], [9808, 9, 4], [9839, 2, 3], [9840, 3, 2], [9850, 0, 6], [9867, 2, 3], [9879, 0, 8], [9883, 5, 1], [9888, 6, 0], [9890, 9, 4], [9893, 2, 3], [9926, 8, 3], [9943, 3, 8], [9945, 9, 4], [9970, 5, 3], [9982, 5, 6]]\n"
     ]
    }
   ],
   "source": [
    "print(index_label_prediction_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAQUAAAEICAYAAABWCOFPAAAABHNCSVQICAgIfAhkiAAAAAlwSFlzAAALEgAACxIB0t1+/AAAADh0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uMy4yLjEsIGh0dHA6Ly9tYXRwbG90bGliLm9yZy+j8jraAAAWiElEQVR4nO3de7RcZX3G8e8DCIGAIYSLJECiXEXEYLNQK9Z0eQlBLVpBRFSkaKBKKy5YFZGW2AaqLhCISjQ2XCICRm6CVRRYsPBSWQQaIRBULscQEhIRkHArBH79431PnPdwZs/J3BOez1qzzsx+Z+/9m/fseebde/bMKCIwMxu0Ua8LMLP+4lAws4JDwcwKDgUzKzgUzKzgUDCzQttCQdJdkqY2Oe8Fkma1q5YNkaRJkkLSJiO471RJy5pcT9Pzrq8kDUh6Z75+sqT/anI5TT8H+knbQiEiXhcRN7Vreb0iaW9JCyU9li/XS9q7pn0zSd+StFLSo5KukTRhyDI+K+kBSU9JWiJpjzxdkr4oaamkJyRdKumV3X6MvSDpfZIWS3pS0q9q+7SfRMTpEfHJRvcb7oWsX58DknaX9Kyki0Zyf+8+vNRy4BBgG2Bb4Grg0pr2zwJvAfYFxgOPA18fbJT0SeBo4D3AlsB7gUdy88eBjwFvzfNuXjvvhkrS7sD3gGOBrYFrgKtHMuppYl1tX+YG4JvArSO9czt3H2qHYDMlLZA0X9LqPKyaUnPf/STdntu+D4wasqz3Slok6fH8qrJvnn6YpPsHX10lTZf0sKTt2vU4IuLxiBiIdKqngBeA3Wru8mrgpxGxMiKeJQXG63I9GwGnAp+LiLsjuS8iHs3zvg+YFxEPRsSTwFeAwyRtsa51Sjoqj0JW5z45Zpj7nCzpkfy/OaJm+maSzsgjlpV55LP5utawDqYBP4+IX0TEGtLjngC8fSQz5/q/IOnuPHo7X9Ko3DZV0jJJn5f0MHC+pI0knSTpPkl/ytviNjXL+5ikP+S2Lw5Z18zaV1RJB+Rt8HFJD0r6hKQZwBHAv+SRzzU1dQ4+BzaTdLak5flytqTNhtR8gqRVklZIOqqF/q3quw+TXrhuGOk8nRwp/B3pCbM16dX2GwCSNgWuAr5LejX+AfDBwZkkvRE4DzgGGAd8m/SqsllEfB/4H2C2pHHAPOCTEfHH4QrI/8h6l5Oqipf0OPAs6ZX89JqmecBbJY3PT+YjgJ/ktp3yZZ+8AT0g6Us5LCCFjGpXA2wG7F5VSx2rSKOQVwJHAWflvhv0KtJIZwJwJDBX0p657SvAHsBkUuBNAP5tJCuVdEdFn55bbzZe+rgF7DOSdWZHkMJl11z7KTVtryJtSxOBGcA/A+8nhc544DHSqyV5t2UOacQ2nrSN7VTnse5C+t9+HdiO1F+LImIuaeTz1YjYMiLeN8zsXwTenOd5A7D/MDWPIfX90cA3JY2tU8e5FX1+R53+Ir94/jtwQr37DCsi2nIBBoB35uszgetr2vYGnsnX/4Y0RFdN+6+AWfn6HOA/hiz7t8Db8/WtgaXAncC321V/ncc0Gvg08J6aaa8ELgECWAP8L7BNbvvrPP2/c52TgN8Bn8rtn8y3J5E2iKvz/d8yglom5ftuUqf9KuCz+frUXNvomvYFwL+SnoxPAbvWtL0FeKBm3mVt7se98jqnApvmOl4EvrAO29axNbcPAu6rqfc5YFRN+xLgHTW3dwSeBzYhhd+lQ/7Hzw3Zdi/K178AXFmnpgsGt9k6z4H7gINq2qYBAzU1P1P7vySF/Jvb3O/nAJ8f+rgaXTo5Uni45vrTwCil/b3xwEORK83+UHN9InBCbRoCO+f5iIjHSaOLfYAzO1g/EfEU8C1gvqTt8+Q5pN2dcaQN6gr+MlJ4Jv/9auTdENJI56A8/TxSoNwE3AXcmKev89H+vOv0a6WDnY/ndWxbc5fHcv2D/kDqw+2ALYDbavr32jy9IyLiHtJo5RvAilzn3azb436w5vrgYxn0x0i7coMmAlfWPL4lpN3AHfJ8a5eV++hPdda5M+nJ3YzxlNv10Jr/FGlXatDTpGNQbSFpMvBO4Kx1nbcXBxpXABMk1Q4nd6m5/iBwWkRsXXPZIiIugbUP9h9IT67ZVSvK+3v1LiePsN6NSE+iwXcY3gBcEBGPRsT/kYaW+0valjSieY70iv4SEfFiRJwaEZMiYidSMDyULyOW900vB84AdoiIrYEfUw7Rx0oaXXN7F9II7RFSeL2upn/HRMSINkil40P1+vRb9eaLiMsiYp+IGEc67jKRdTj4RXqCDn0saxc/5L4PAtOHbEOjIuIh0va3dll5F3BcnXU+SNpdGU6jjxcvJz3GejWPWD7mU6/P76oz21TS6HJpPtZyIvBBSbc3XGEbhyoDDDMEiyFDX9LwcSnpKP4mwN+ThnaDuw9TSP+MN5E28tGkI/lbkV6hFwP/SNoXvxP4dJuHXO8C9gM2Ju0qzCb9M0fl9vNJT8gxwCuAk0kjn8H55wM/yvXuBNwDHJ3btiFtZCLtUi0GZtTMOxO4qU5dtX24FemV7+15WdNJrzSDfTiVtPtwRu7vt5GG73vVDCsXANvn2xOAaTXztnX3IS/3r3Kfbgd8H7i4pm1q2hQrt607c39uA/wcOL1evcDnSKOxifn2dsDB+frrgCeBA3LfnJH7arjdh12A1cCHcr+PAybnti/XPoZhngOzSLvF25FGRr8Y8v8ZWvPaedvU31uQjlsMXs4ALgO2azRv10cKEfEcKQg+QToAdBhpCD7YvhD4FGmo+Rhwb74vwH+SOnNOpFfpjwKzlN7yapetSaOQP5OGjrsBB8Zfhqcnkg5A/h74I2nY/oGa+Y8jbXTLSQdFLybtNkDaOH5MeoL+BDgv0kGrQTsDv2xUYESsJh1MW0Dqo4+Qjk/Ueji3LSe/HRhpGA/weVK//lrSE8D1wJ501jmko+C/zX8/VdO2M6mvqlwM/Ay4P1+qTnY7h9QfP5O0Gvg16UWGiLgL+Exe3gpSHw27GxMRS0n/3xOAR4FFpJEipAPOe+ddlKuGmX0WsBC4gxRotzeoua0i4umIeHjwQtomn406B+VrKYpde+slSYtIB8jq7eNukJTOIPxBRPy0TvsA6V2m67ta2MuUT/ToIxExudc19EKM4AxC6x6f0WhmBe8+mFnBIwUzK3T1mIIkD0vMOiwi1Phe9bU0UpB0oKTfSrq30WcJzGz90PQxBUkbk87jfxfpfd5bgcMj4u6KeTxSMOuwXo4U9gfujYj78wlJlwIHt1KMmfVeK6EwgfJDKsv4y+cD1pI0Q+mbjBa2sC4z65JWDjQON0R5ye5BPo13Lnj3wWx90MpIYRnlJ9d2oslPgZlZ/2glFG4Fdpf06vxtSh/mpR/KMbP1TNO7DxGxRtJxwE9JH4k9L38CzczWY109zdnHFMw6r6cnL5nZhsehYGYFh4KZFRwKZlZwKJhZwaFgZgWHgpkVHApmVnAomFnBoWBmBYeCmRUcCmZWcCiYWcGhYGYFh4KZFRwKZlZwKJhZwaFgZgWHgpkVHApmVnAomFnBoWBmBYeCmRUcCmZWcCiYWcGhYGYFh4KZFRwKZlZwKJhZwaFgZoVNWplZ0gCwGngBWBMRU9pRlJn1TkuhkP1tRDzShuWYWR/w7oOZFVoNhQB+Juk2STOGu4OkGZIWSlrY4rrMrAsUEc3PLI2PiOWStgeuA/4pIm6uuH/zKzOzEYkItTJ/SyOFiFie/64CrgT2b2V5ZtZ7TYeCpNGSthq8DrwbWNyuwsysN1p592EH4EpJg8u5OCKubUtVZiMwadKkyvYbbrihbtvTTz9dOe/ll19e2T5z5szK9vVZ06EQEfcDb2hjLWbWB/yWpJkVHApmVnAomFnBoWBmBYeCmRVaOqNxnVfmMxr7zj777FPZfuaZZ7a0/JtvrnuCK6eddlpLy16yZEll+5577tn0sgcGBirbX/Oa1zS97E7r6RmNZrbhcSiYWcGhYGYFh4KZFRwKZlZwKJhZwaFgZoV2fHGrdVj+eHpd48ePr9t2yimnVM572GGHVbaPGTOmsn3NmjWV7a9//evrtjU6T+Hwww+vbN9jjz0q2605HimYWcGhYGYFh4KZFRwKZlZwKJhZwaFgZgWHgpkVfJ7CeuDjH/94Zfv555/f9LKfeeaZyvZDDjmksv3KK6+sbK/6vobddtutct7Zs2dXtjc6f6MVl112WceW3e88UjCzgkPBzAoOBTMrOBTMrOBQMLOCQ8HMCg4FMyv4PIU+cNRRR1W2z5kzp2PrPv300yvbG52H0MioUaPqtl144YWV844bN66ldVc5++yzK9tPOumkjq273zUcKUg6T9IqSYtrpm0j6TpJv89/x3a2TDPrlpHsPlwAHDhk2knADRGxO3BDvm1mG4CGoRARNwOPDpl8MDA49rsQeH+b6zKzHmn2mMIOEbECICJWSNq+3h0lzQBmNLkeM+uyjh9ojIi5wFzwD8yarQ+afUtypaQdAfLfVe0rycx6qdlQuBo4Ml8/Evhhe8oxs15TRPWIXtIlwFRgW2AlcCpwFbAA2AVYChwaEUMPRg63rJfl7sNee+1V2b5o0aLK9k033bTpdV9xxRWV7Y1+9+GFF16obJ88eXJl+3XXXVe3rZPnIQDcd999ddsOOOCAynlXrlzZ7nK6JiJa+qKJhscUIqLeL3K8o5UVm1l/8mnOZlZwKJhZwaFgZgWHgpkVHApmVvBHp9tgo42qs/UjH/lIZXsrbzlC9de0z5o1q3LeRl+TPn369Mr2Xn78+fnnn69snzGj/tn16/Nbjp3mkYKZFRwKZlZwKJhZwaFgZgWHgpkVHApmVnAomFnB5ym0QaOPH59yyikdXX/V17Q3+lj2EUccUdn+3e9+t6mauuGXv/xlZfuNN97YpUo2LB4pmFnBoWBmBYeCmRUcCmZWcCiYWcGhYGYFh4KZFXyeQhtMmjSpo8uv+r4EgGuuuaZu2/z58yvnbXSeQj879NBDe13CBskjBTMrOBTMrOBQMLOCQ8HMCg4FMys4FMys4FAws0LDn6Jv68o20J+if+CBByrbJ06c2KVKNiwLFiyobP/oRz9a2b5mzZp2lrPeaPWn6BuOFCSdJ2mVpMU102ZKekjSonw5qJUizKx/jGT34QLgwGGmnxURk/Plx+0ty8x6pWEoRMTNwKNdqMXM+kArBxqPk3RH3r0YW+9OkmZIWihpYQvrMrMuaTYU5gC7ApOBFcCZ9e4YEXMjYkpETGlyXWbWRU2FQkSsjIgXIuJF4DvA/u0ty8x6palQkLRjzc0PAIvr3dfM1i8Nv09B0iXAVGBbScuAU4GpkiYDAQwAx3Swxr53zz33VLb7PIXh/eY3v6lsP/rooyvbX67nIXRaw1CIiMOHmTyvA7WYWR/wac5mVnAomFnBoWBmBYeCmRUcCmZW8Fe8t8GXvvSlyvYtttiisv1tb3tbS+tfvXp13bZnn322ct5GtY0ePbqpmkbi+uuvr2x/6qmnOrZuq88jBTMrOBTMrOBQMLOCQ8HMCg4FMys4FMys4FAws4K/4r0LxowZU9l+4IHDfS/uyN1yyy112wYGBirnPffccyvbjz322GZKWuuJJ56o27bvvvtWzrt06dKW1v1y1fGveDezlxeHgpkVHApmVnAomFnBoWBmBYeCmRUcCmZW8HkKG7hp06ZVtv/oRz+qbN94441bWv/s2bPrth1//PEtLduG5/MUzKytHApmVnAomFnBoWBmBYeCmRUcCmZWcCiYWWEkP0W/MzAfeBXwIjA3Is6RtA3wfWAS6efoPxQRj3WuVGvG2LFjK9tbPQ+hkVmzZnV0+dZ+IxkprAFOiIjXAm8GPiNpb+Ak4IaI2B24Id82s/Vcw1CIiBURcXu+vhpYAkwADgYuzHe7EHh/p4o0s+5Zp2MKkiYB+wG3ADtExApIwQFs3+7izKz7RvxbkpK2BC4Hjo+IJ6SRnV4taQYwo7nyzKzbRjRSkPQKUiB8LyKuyJNXStoxt+8IrBpu3oiYGxFTImJKOwo2s85qGApKQ4J5wJKI+FpN09XAkfn6kcAP21+emXXbSHYf3gp8DLhT0qI87WTgy8ACSUcDS4FDO1OitWL69Oktzd9oN/Gmm26qbP/zn//c0vqt+xqGQkT8Aqi3ZbyjveWYWa/5jEYzKzgUzKzgUDCzgkPBzAoOBTMrOBTMrDDi05ytf+2111512w455JCWlt3oJwCuvfbayvbnn3++pfVb93mkYGYFh4KZFRwKZlZwKJhZwaFgZgWHgpkVHApmVvB5ChuAE088sW7b5ptv3tF1v/a1r+3o8q37PFIws4JDwcwKDgUzKzgUzKzgUDCzgkPBzAoOBTMr+DyFDcCyZct6tu6NNvLryobG/1EzKzgUzKzgUDCzgkPBzAoOBTMrOBTMrOBQMLNCw/MUJO0MzAdeBbwIzI2IcyTNBD4F/DHf9eSI+HGnCrX65s2bV7ftTW96U+W806ZNa2ndF110UUvzW/8ZyclLa4ATIuJ2SVsBt0m6LredFRFndK48M+u2hqEQESuAFfn6aklLgAmdLszMemOdjilImgTsB9ySJx0n6Q5J50kaW2eeGZIWSlrYUqVm1hUjDgVJWwKXA8dHxBPAHGBXYDJpJHHmcPNFxNyImBIRU9pQr5l12IhCQdIrSIHwvYi4AiAiVkbECxHxIvAdYP/OlWlm3dIwFCQJmAcsiYiv1UzfseZuHwAWt788M+s2NfqpcUkHAD8H7iS9JQlwMnA4adchgAHgmHxQsmpZ1Sszs5ZFhFqZv2EotJNDwazzWg0Fn9FoZgWHgpkVHApmVnAomFnBoWBmBYeCmRUcCmZWcCiYWcGhYGYFh4KZFRwKZlZwKJhZwaFgZgWHgpkVuv1T9I8Af6i5vW2e1o/6tbZ+rQtcW7PaWdvEVhfQ1e9TeMnKpYX9+t2N/Vpbv9YFrq1Z/Vabdx/MrOBQMLNCr0Nhbo/XX6Vfa+vXusC1NauvauvpMQUz6z+9HimYWZ9xKJhZoSehIOlASb+VdK+kk3pRQz2SBiTdKWlRr3//Mv9G5ypJi2umbSPpOkm/z3+H/Q3PHtU2U9JDue8WSTqoR7XtLOlGSUsk3SXps3l6T/uuoq6+6Le1dXb7mIKkjYHfAe8ClgG3AodHxN1dLaQOSQPAlIjo+Ykukv4GeBKYHxH75GlfBR6NiC/nQB0bEZ/vk9pmAk9GxBndrmdIbTsCO0bE7ZK2Am4D3g98gh72XUVdH6IP+m1QL0YK+wP3RsT9EfEccClwcA/q6HsRcTPw6JDJBwMX5usXkjaqrqtTW1+IiBURcXu+vhpYAkygx31XUVdf6UUoTAAerLm9jP7qmAB+Juk2STN6Xcwwdhj8eb78d/se1zPUcZLuyLsXPdm1qSVpErAfcAt91HdD6oI+6rdehMJwP2nVT++LvjUi3ghMBz6Th8k2MnOAXUm/MboCOLOXxUjakvRr6cdHxBO9rKXWMHX1Vb/1IhSWATvX3N4JWN6DOoYVEcvz31XAlaTdnX6ycvAXv/PfVT2uZ62IWBkRL0TEi8B36GHfSXoF6Yn3vYi4Ik/ued8NV1c/9Rv0JhRuBXaX9GpJmwIfBq7uQR0vIWl0PgCEpNHAu4HF1XN13dXAkfn6kcAPe1hLYfAJl32AHvWdJAHzgCUR8bWapp72Xb26+qXfBvXkjMb8lsvZwMbAeRFxWteLGIak15BGB5A+Vn5xL2uTdAkwlfTR2pXAqcBVwAJgF2ApcGhEdP2AX53appKGwAEMAMcM7sN3ubYDgJ8DdwIv5sknk/bfe9Z3FXUdTh/02yCf5mxmBZ/RaGYFh4KZFRwKZlZwKJhZwaFgZgWHgpkVHApmVvh/PFEH38Cl4tUAAAAASUVORK5CYII=\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# check false data\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "random_index = int(np.random.choice(len(index_label_prediction_list), 1))\n",
    "\n",
    "index_str = 'index = ' + str(index_label_prediction_list[random_index][0]) + ', '\n",
    "label_str = 'label = ' + str(index_label_prediction_list[random_index][1]) + ', '\n",
    "prediction_str = 'prediction = ' + str(index_label_prediction_list[random_index][2])\n",
    "\n",
    "title_str = index_str + label_str + prediction_str\n",
    "\n",
    "img = test_x_data[index_label_prediction_list[random_index][0]].reshape(28,28)  \n",
    "\n",
    "plt.title(title_str)\n",
    "plt.imshow(img, cmap='gray')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "anaconda-cloud": {},
  "colab": {
   "collapsed_sections": [],
   "name": "(191123)tensorflow_MNIST_debug_for_class.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
