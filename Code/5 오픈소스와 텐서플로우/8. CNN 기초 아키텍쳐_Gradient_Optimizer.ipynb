{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "컨볼루션 네트워크의 안전연결층에서는 3차원의 출력값을 1차원 벡터로 평탄화 시켜주는 작업을 수행하여 일반 신경망 연결처럼 출력층의 모든 노드와 연결시켜주는 역할을 수행한다\n",
    "\n",
    "컨볼루션 함수\n",
    "conv = tf.nn.conv2d(input, filter, strides, padding,...)\n",
    "\n",
    "input : 컨볼루션 연산을 위한 입력 데이터이며 [batch, in_height, in_width, in_channels] 예를들어, 100 개의 배치로 묶은 28X28 크기의 흑백 이미지를 입력 으로 넣을경우 input 은 [100, 28,28, 1] 로 나타냄\n",
    "\n",
    "filter : 컨볼루션 연산에 적용할 필터이며 [filter_height, filter_width, in_channels, out_channels] 예를들어, 필터 크기 3X3이며 입력채널 개수는 1이고 적용되는 필터 개수가 총 32개이면 filter 는 [3, 3, 1, 32] 로 나타냄\n",
    "\n",
    "strides: 컨볼루션 연산을 위해 필터를 이동시키는 간격을 나타냄. 예를들어 [1, 1, 1, 1] 로 strides를 나타낸다면 컨볼루션 적용을 위해 1 칸씩 이동 필터를 이동하는것을 의미함\n",
    "\n",
    "padding: ‘SAME’ 또는 ‘VALID’ 값을 가짐. padding=‘VALID’ 라면 컨볼루션 연산 공식에 의해서 가로/세로(차원) 크기가 축소된 결과가 리턴됨. 그러나 padding=‘SAME’ 으로 지정하면 입력 값의 가로/세로(차원) 크기와 같은 출력이 리턴되도록 작아진 차원 부분에 0 값을 채운 제로패딩을 수행함\n",
    "\n",
    "* 입력채널 : 데이터의 이동 통로"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "pooling 함수 tf.nn.max_pool(value, ksize, strides, padding,...)\n",
    "\n",
    "value: [batch, height, width, channels] 형식의 입력데이터. 일반적으로 relu 를 통과한 출력결과를 말한다\n",
    "\n",
    "ksize: 컨볼루션 신경망에서 일반적인 ksize는 다음과 같이 [1, height, width, 1] 형태로 표시함. 예를 들어 ksize = [1, 2, 2, 1]이라면 2칸씩 이동하면서 출력결과 1 개를 만들어 낸다는 것을 의미함. 즉 4개 (2X2) 데이터 중에서 가장 큰 값 1 개를 찾아서 반환하는 역할을 수행함. \n",
    "만약 ksize = [1, 3, 3, 1] 이라고 하면 3칸씩 이동, 즉 9개 (3X3) 데이터 중에서 가장 큰 값을 찾는다는 의미임\n",
    "\n",
    "strides: max pooling을 위해 윈도우를 이동시키는 간격을 나타냄. 예를들어 [1, 2, 2, 1] 로 strides를 나타낸다면 max pooling 적용을 위해 2 칸씩 이동하는 것을 의미함\n",
    "\n",
    "padding: max pooling 에서의 padding 값은 max pooling 을 수행하기에는 데이터가 부족한 경우에 주변을 0 등으로 채워주는 역할을 함. 예를들어 max pooling 에서 풀링층으로 들어오는 입력데이터가 7X7 이고, 데이터를 2개씩 묶어 최대값을 찾아내는 연산을 하기에는 입력으로 주어진 데이터가 부족한 상황임 (즉, 최소 8X8 이어야 가능). 이때padding=‘SAME’ 이면, 부족한 데이터 부분을 0 등으로 채운 후에 데이터를 2개씩 묶어 최대값을 뽑아낼 수 있음\n",
    "\n",
    "채널마다 바이어스가 존재해야함\n",
    "필터가 32개면 바이어스도 32개\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting MNIST_data/train-images-idx3-ubyte.gz\n",
      "Extracting MNIST_data/train-labels-idx1-ubyte.gz\n",
      "Extracting MNIST_data/t10k-images-idx3-ubyte.gz\n",
      "Extracting MNIST_data/t10k-labels-idx1-ubyte.gz\n",
      "\n",
      "train.num =  55000 , test.num =  10000  , validation.num =  5000\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'1.15.0'"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.examples.tutorials.mnist import input_data\n",
    "import numpy as np\n",
    "from datetime import datetime\n",
    "\n",
    "mnist = input_data.read_data_sets(\"MNIST_data/\", one_hot = True)\n",
    "\n",
    "print(\"\")\n",
    "print(\"train.num = \", mnist.train.num_examples,\n",
    "     \", test.num = \", mnist.test.num_examples,\n",
    "     \" , validation.num = \", mnist.validation.num_examples)\n",
    "tf.__version__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Hyper Parameter\n",
    "\n",
    "learning_rate = 0.0001\n",
    "epochs = 30\n",
    "batch_size = 100\n",
    "\n",
    "#PlaceHolder\n",
    "X = tf.placeholder(tf.float32, [None, 784])\n",
    "\n",
    "T = tf.placeholder(tf.float32, [None, 10])\n",
    "\n",
    "A1 = X_img = tf.reshape(X, [-1, 28, 28, 1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "#1번째 컨볼루션 층 3X3X32 필터\n",
    "W2 = tf.Variable(tf.random_normal([3, 3, 1, 32], stddev = 0.01))\n",
    "b2 = tf.Variable(tf.random_normal([32]))\n",
    "\n",
    "#1번째 컨볼루션 연산\n",
    "C2 = tf.nn.conv2d(A1, W2, strides=[1, 1, 1, 1], padding = 'SAME')\n",
    "\n",
    "#relu\n",
    "Z2 = tf.nn.relu(C2 + b2)\n",
    "\n",
    "# 1번째 max pooling을 통해 28 X 28 X 32 => 14 X 14 X 32\n",
    "A2 = P2 = tf.nn.max_pool(Z2, ksize=[1, 2, 2, 1], strides=[1, 2, 2, 1], padding = 'SAME')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "A2_flat = P2_flat = tf.reshape(A2, [-1, 14*14*32])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "W3 = tf.Variable(tf.random_normal([14*14*32, 10], stddev = 0.01))\n",
    "b3 = tf.Variable(tf.random_normal([10]))\n",
    "\n",
    "Z3 = logits = tf.matmul(A2_flat, W3) + b3\n",
    "\n",
    "y = A3 = tf.nn.softmax(Z3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "loss = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits_v2(logits = Z3, labels = T))\n",
    "\n",
    "optimizer = tf.train.GradientDescentOptimizer(learning_rate)\n",
    "\n",
    "train = optimizer.minimize(loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "predicted_val = tf.equal(tf.argmax(A3, 1), tf.argmax(T, 1))\n",
    "\n",
    "#batch_size X 10의 True, Flase를 1또는 0으로 변환\n",
    "accuracy = tf.reduce_mean(tf.cast(predicted_val, dtype=tf.float32))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epochs =  0 , step =  0 , loss_val =  3.0205503\n",
      "epochs =  0 , step =  100 , loss_val =  2.3118703\n",
      "epochs =  0 , step =  200 , loss_val =  2.3037217\n",
      "epochs =  0 , step =  300 , loss_val =  2.3105526\n",
      "epochs =  0 , step =  400 , loss_val =  2.2989044\n",
      "epochs =  0 , step =  500 , loss_val =  2.3071725\n",
      "epochs =  1 , step =  0 , loss_val =  2.3076527\n",
      "epochs =  1 , step =  100 , loss_val =  2.308975\n",
      "epochs =  1 , step =  200 , loss_val =  2.3028376\n",
      "epochs =  1 , step =  300 , loss_val =  2.2948828\n",
      "epochs =  1 , step =  400 , loss_val =  2.3090494\n",
      "epochs =  1 , step =  500 , loss_val =  2.3038225\n",
      "epochs =  2 , step =  0 , loss_val =  2.2996845\n",
      "epochs =  2 , step =  100 , loss_val =  2.3048458\n",
      "epochs =  2 , step =  200 , loss_val =  2.3072047\n",
      "epochs =  2 , step =  300 , loss_val =  2.291049\n",
      "epochs =  2 , step =  400 , loss_val =  2.3030562\n",
      "epochs =  2 , step =  500 , loss_val =  2.2987397\n",
      "epochs =  3 , step =  0 , loss_val =  2.314963\n",
      "epochs =  3 , step =  100 , loss_val =  2.2916152\n",
      "epochs =  3 , step =  200 , loss_val =  2.2926564\n",
      "epochs =  3 , step =  300 , loss_val =  2.2978182\n",
      "epochs =  3 , step =  400 , loss_val =  2.29748\n",
      "epochs =  3 , step =  500 , loss_val =  2.3086197\n",
      "epochs =  4 , step =  0 , loss_val =  2.294431\n",
      "epochs =  4 , step =  100 , loss_val =  2.2979484\n",
      "epochs =  4 , step =  200 , loss_val =  2.2976332\n",
      "epochs =  4 , step =  300 , loss_val =  2.2950797\n",
      "epochs =  4 , step =  400 , loss_val =  2.3031347\n",
      "epochs =  4 , step =  500 , loss_val =  2.2973127\n",
      "epochs =  5 , step =  0 , loss_val =  2.3063095\n",
      "epochs =  5 , step =  100 , loss_val =  2.2907941\n",
      "epochs =  5 , step =  200 , loss_val =  2.302987\n",
      "epochs =  5 , step =  300 , loss_val =  2.2992759\n",
      "epochs =  5 , step =  400 , loss_val =  2.2983787\n",
      "epochs =  5 , step =  500 , loss_val =  2.291916\n",
      "epochs =  6 , step =  0 , loss_val =  2.3138463\n",
      "epochs =  6 , step =  100 , loss_val =  2.301981\n",
      "epochs =  6 , step =  200 , loss_val =  2.2970748\n",
      "epochs =  6 , step =  300 , loss_val =  2.2996876\n",
      "epochs =  6 , step =  400 , loss_val =  2.2979782\n",
      "epochs =  6 , step =  500 , loss_val =  2.3015668\n",
      "epochs =  7 , step =  0 , loss_val =  2.2909427\n",
      "epochs =  7 , step =  100 , loss_val =  2.2992895\n",
      "epochs =  7 , step =  200 , loss_val =  2.3042536\n",
      "epochs =  7 , step =  300 , loss_val =  2.295902\n",
      "epochs =  7 , step =  400 , loss_val =  2.297808\n",
      "epochs =  7 , step =  500 , loss_val =  2.291106\n",
      "epochs =  8 , step =  0 , loss_val =  2.293332\n",
      "epochs =  8 , step =  100 , loss_val =  2.2972023\n",
      "epochs =  8 , step =  200 , loss_val =  2.302787\n",
      "epochs =  8 , step =  300 , loss_val =  2.2916439\n",
      "epochs =  8 , step =  400 , loss_val =  2.3069198\n",
      "epochs =  8 , step =  500 , loss_val =  2.296765\n",
      "epochs =  9 , step =  0 , loss_val =  2.2998269\n",
      "epochs =  9 , step =  100 , loss_val =  2.2891495\n",
      "epochs =  9 , step =  200 , loss_val =  2.2950983\n",
      "epochs =  9 , step =  300 , loss_val =  2.2958984\n",
      "epochs =  9 , step =  400 , loss_val =  2.3001664\n",
      "epochs =  9 , step =  500 , loss_val =  2.299449\n",
      "epochs =  10 , step =  0 , loss_val =  2.2968447\n",
      "epochs =  10 , step =  100 , loss_val =  2.2930803\n",
      "epochs =  10 , step =  200 , loss_val =  2.2996545\n",
      "epochs =  10 , step =  300 , loss_val =  2.3005822\n",
      "epochs =  10 , step =  400 , loss_val =  2.2906632\n",
      "epochs =  10 , step =  500 , loss_val =  2.296775\n",
      "epochs =  11 , step =  0 , loss_val =  2.2991483\n",
      "epochs =  11 , step =  100 , loss_val =  2.289968\n",
      "epochs =  11 , step =  200 , loss_val =  2.2891147\n",
      "epochs =  11 , step =  300 , loss_val =  2.2879863\n",
      "epochs =  11 , step =  400 , loss_val =  2.2904606\n",
      "epochs =  11 , step =  500 , loss_val =  2.2911882\n",
      "epochs =  12 , step =  0 , loss_val =  2.2771838\n",
      "epochs =  12 , step =  100 , loss_val =  2.2953522\n",
      "epochs =  12 , step =  200 , loss_val =  2.2892187\n",
      "epochs =  12 , step =  300 , loss_val =  2.2929194\n",
      "epochs =  12 , step =  400 , loss_val =  2.3011503\n",
      "epochs =  12 , step =  500 , loss_val =  2.3024192\n",
      "epochs =  13 , step =  0 , loss_val =  2.28689\n",
      "epochs =  13 , step =  100 , loss_val =  2.2978854\n",
      "epochs =  13 , step =  200 , loss_val =  2.2948573\n",
      "epochs =  13 , step =  300 , loss_val =  2.2896886\n",
      "epochs =  13 , step =  400 , loss_val =  2.302417\n",
      "epochs =  13 , step =  500 , loss_val =  2.2988508\n",
      "epochs =  14 , step =  0 , loss_val =  2.2864785\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-17-a862e11c28d5>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     12\u001b[0m             \u001b[0mbatch_x_data\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mbatch_t_data\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mmnist\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtrain\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mnext_batch\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mbatch_size\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     13\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 14\u001b[1;33m             \u001b[0mloss_val\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0m_\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0msess\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mrun\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mloss\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtrain\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfeed_dict\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m{\u001b[0m\u001b[0mX\u001b[0m \u001b[1;33m:\u001b[0m \u001b[0mbatch_x_data\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mT\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mbatch_t_data\u001b[0m\u001b[1;33m}\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     15\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     16\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[0mstep\u001b[0m \u001b[1;33m%\u001b[0m \u001b[1;36m100\u001b[0m \u001b[1;33m==\u001b[0m \u001b[1;36m0\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\users\\hogeu\\anaconda3\\envs\\tensorflow\\lib\\site-packages\\tensorflow_core\\python\\client\\session.py\u001b[0m in \u001b[0;36mrun\u001b[1;34m(self, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[0;32m    954\u001b[0m     \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    955\u001b[0m       result = self._run(None, fetches, feed_dict, options_ptr,\n\u001b[1;32m--> 956\u001b[1;33m                          run_metadata_ptr)\n\u001b[0m\u001b[0;32m    957\u001b[0m       \u001b[1;32mif\u001b[0m \u001b[0mrun_metadata\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    958\u001b[0m         \u001b[0mproto_data\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtf_session\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mTF_GetBuffer\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mrun_metadata_ptr\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\users\\hogeu\\anaconda3\\envs\\tensorflow\\lib\\site-packages\\tensorflow_core\\python\\client\\session.py\u001b[0m in \u001b[0;36m_run\u001b[1;34m(self, handle, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[0;32m   1178\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mfinal_fetches\u001b[0m \u001b[1;32mor\u001b[0m \u001b[0mfinal_targets\u001b[0m \u001b[1;32mor\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0mhandle\u001b[0m \u001b[1;32mand\u001b[0m \u001b[0mfeed_dict_tensor\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1179\u001b[0m       results = self._do_run(handle, final_targets, final_fetches,\n\u001b[1;32m-> 1180\u001b[1;33m                              feed_dict_tensor, options, run_metadata)\n\u001b[0m\u001b[0;32m   1181\u001b[0m     \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1182\u001b[0m       \u001b[0mresults\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\users\\hogeu\\anaconda3\\envs\\tensorflow\\lib\\site-packages\\tensorflow_core\\python\\client\\session.py\u001b[0m in \u001b[0;36m_do_run\u001b[1;34m(self, handle, target_list, fetch_list, feed_dict, options, run_metadata)\u001b[0m\n\u001b[0;32m   1357\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mhandle\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1358\u001b[0m       return self._do_call(_run_fn, feeds, fetches, targets, options,\n\u001b[1;32m-> 1359\u001b[1;33m                            run_metadata)\n\u001b[0m\u001b[0;32m   1360\u001b[0m     \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1361\u001b[0m       \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_do_call\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0m_prun_fn\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mhandle\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfeeds\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfetches\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\users\\hogeu\\anaconda3\\envs\\tensorflow\\lib\\site-packages\\tensorflow_core\\python\\client\\session.py\u001b[0m in \u001b[0;36m_do_call\u001b[1;34m(self, fn, *args)\u001b[0m\n\u001b[0;32m   1363\u001b[0m   \u001b[1;32mdef\u001b[0m \u001b[0m_do_call\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfn\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1364\u001b[0m     \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1365\u001b[1;33m       \u001b[1;32mreturn\u001b[0m \u001b[0mfn\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1366\u001b[0m     \u001b[1;32mexcept\u001b[0m \u001b[0merrors\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mOpError\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1367\u001b[0m       \u001b[0mmessage\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mcompat\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mas_text\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0me\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmessage\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\users\\hogeu\\anaconda3\\envs\\tensorflow\\lib\\site-packages\\tensorflow_core\\python\\client\\session.py\u001b[0m in \u001b[0;36m_run_fn\u001b[1;34m(feed_dict, fetch_list, target_list, options, run_metadata)\u001b[0m\n\u001b[0;32m   1348\u001b[0m       \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_extend_graph\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1349\u001b[0m       return self._call_tf_sessionrun(options, feed_dict, fetch_list,\n\u001b[1;32m-> 1350\u001b[1;33m                                       target_list, run_metadata)\n\u001b[0m\u001b[0;32m   1351\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1352\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0m_prun_fn\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mhandle\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfeed_dict\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfetch_list\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\users\\hogeu\\anaconda3\\envs\\tensorflow\\lib\\site-packages\\tensorflow_core\\python\\client\\session.py\u001b[0m in \u001b[0;36m_call_tf_sessionrun\u001b[1;34m(self, options, feed_dict, fetch_list, target_list, run_metadata)\u001b[0m\n\u001b[0;32m   1441\u001b[0m     return tf_session.TF_SessionRun_wrapper(self._session, options, feed_dict,\n\u001b[0;32m   1442\u001b[0m                                             \u001b[0mfetch_list\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtarget_list\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1443\u001b[1;33m                                             run_metadata)\n\u001b[0m\u001b[0;32m   1444\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1445\u001b[0m   \u001b[1;32mdef\u001b[0m \u001b[0m_call_tf_sessionprun\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mhandle\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfeed_dict\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfetch_list\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "with tf.Session() as sess:\n",
    "    \n",
    "    sess.run(tf.global_variables_initializer())\n",
    "    \n",
    "    start_time = datetime.now()\n",
    "    \n",
    "    for i in range(epochs):\n",
    "        total_batch = int(mnist.train.num_examples / batch_size)\n",
    "        \n",
    "        for step in range(total_batch):\n",
    "            \n",
    "            batch_x_data, batch_t_data = mnist.train.next_batch(batch_size)\n",
    "            \n",
    "            loss_val, _ = sess.run([loss, train], feed_dict = {X : batch_x_data, T: batch_t_data})\n",
    "            \n",
    "            if step % 100 == 0:\n",
    "                print(\"epochs = \", i, \", step = \", step, \", loss_val = \", loss_val)\n",
    "                \n",
    "    end_time = datetime.now()\n",
    "    \n",
    "    print(\"\\nelapsed time = \", end_time = start_time)\n",
    "    \n",
    "    # Accuracy\n",
    "    test_x_data = mnist.test.images\n",
    "    test_t_data = mnist.test.labels\n",
    "    \n",
    "    accuract_val = sess.tun(accuracy, feed_dict = {X: test_x_data, T: test_t_data})\n",
    "    \n",
    "    print(\"\\nAccuracy = \", accuracy_val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
