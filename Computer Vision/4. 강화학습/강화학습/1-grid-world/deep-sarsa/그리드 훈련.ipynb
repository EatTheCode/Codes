{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_2\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "dense_4 (Dense)              (None, 30)                480       \n",
      "_________________________________________________________________\n",
      "dense_5 (Dense)              (None, 30)                930       \n",
      "_________________________________________________________________\n",
      "dense_6 (Dense)              (None, 5)                 155       \n",
      "=================================================================\n",
      "Total params: 1,565\n",
      "Trainable params: 1,565\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "episode: 0   score: -7 global_step 95   epsilon: 0.9905445119027773\n"
     ]
    },
    {
     "ename": "TclError",
     "evalue": "invalid command name \".!canvas\"",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTclError\u001b[0m                                  Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-2-6f97df49cf8f>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     93\u001b[0m             \u001b[0maction\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0magent\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mget_action\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mstate\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     94\u001b[0m             \u001b[1;31m# 선택한 행동으로 환경에서 한 타임스텝 진행 후 샘플 수집\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 95\u001b[1;33m             \u001b[0mnext_state\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mreward\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdone\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0menv\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0maction\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     96\u001b[0m             \u001b[0mnext_state\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mreshape\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mnext_state\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m15\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     97\u001b[0m             \u001b[0mnext_action\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0magent\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mget_action\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mnext_state\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\IT비지니스\\OTHERS\\강화학습\\1-grid-world\\deep-sarsa\\environment.py\u001b[0m in \u001b[0;36mstep\u001b[1;34m(self, action)\u001b[0m\n\u001b[0;32m    139\u001b[0m             \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mrewards\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmove_rewards\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    140\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 141\u001b[1;33m         \u001b[0mnext_coords\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmove\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mrectangle\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0maction\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    142\u001b[0m         \u001b[0mcheck\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcheck_if_reward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcoords_to_state\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mnext_coords\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    143\u001b[0m         \u001b[0mdone\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mcheck\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'if_goal'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\IT비지니스\\OTHERS\\강화학습\\1-grid-world\\deep-sarsa\\environment.py\u001b[0m in \u001b[0;36mmove\u001b[1;34m(self, target, action)\u001b[0m\n\u001b[0;32m    208\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    209\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mmove\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtarget\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0maction\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 210\u001b[1;33m         \u001b[0ms\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcanvas\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcoords\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtarget\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    211\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    212\u001b[0m         \u001b[0mbase_action\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0marray\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\tkinter\\__init__.py\u001b[0m in \u001b[0;36mcoords\u001b[1;34m(self, *args)\u001b[0m\n\u001b[0;32m   2467\u001b[0m         return [self.tk.getdouble(x) for x in\n\u001b[0;32m   2468\u001b[0m                            self.tk.splitlist(\n\u001b[1;32m-> 2469\u001b[1;33m                    self.tk.call((self._w, 'coords') + args))]\n\u001b[0m\u001b[0;32m   2470\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0m_create\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mitemType\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mkw\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m \u001b[1;31m# Args: (val, val, ..., cnf={})\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2471\u001b[0m         \u001b[1;34m\"\"\"Internal function.\"\"\"\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mTclError\u001b[0m: invalid command name \".!canvas\""
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "import copy\n",
    "import pylab\n",
    "import random\n",
    "import numpy as np\n",
    "from environment import Env\n",
    "from keras.layers import Dense\n",
    "from keras.optimizers import Adam\n",
    "from keras.models import Sequential\n",
    "\n",
    "EPISODES = 1000\n",
    "\n",
    "# 그리드월드 예제에서의 딥살사 에이전트\n",
    "class DeepSARSAgent:\n",
    "    def __init__(self):\n",
    "        self.load_model = False\n",
    "        # 에이전트가 가능한 모든 행동 정의\n",
    "        self.action_space = [0, 1, 2, 3, 4]\n",
    "        # 상태의 크기와 행동의 크기 정의\n",
    "        self.action_size = len(self.action_space)\n",
    "        self.state_size = 15\n",
    "        self.discount_factor = 0.99\n",
    "        self.learning_rate = 0.001\n",
    "\n",
    "        self.epsilon = 1.  # exploration\n",
    "        self.epsilon_decay = .9999\n",
    "        self.epsilon_min = 0.01\n",
    "        self.model = self.build_model()\n",
    "\n",
    "        if self.load_model:\n",
    "            self.epsilon = 0.05\n",
    "            self.model.load_weights('./save_model/deep_sarsa_trained.h5')\n",
    "\n",
    "    # 상태가 입력 큐함수가 출력인 인공신경망 생성\n",
    "    def build_model(self):\n",
    "        model = Sequential()\n",
    "        model.add(Dense(30, input_dim=self.state_size, activation='relu'))\n",
    "        model.add(Dense(30, activation='relu'))\n",
    "        model.add(Dense(self.action_size, activation='linear'))\n",
    "        model.summary()\n",
    "        model.compile(loss='mse', optimizer=Adam(lr=self.learning_rate))\n",
    "        return model\n",
    "\n",
    "    # 입실론 탐욕 방법으로 행동 선택\n",
    "    def get_action(self, state):\n",
    "        if np.random.rand() <= self.epsilon:\n",
    "            # 무작위 행동 반환\n",
    "            return random.randrange(self.action_size)\n",
    "        else:\n",
    "            # 모델로부터 행동 산출\n",
    "            state = np.float32(state)\n",
    "            q_values = self.model.predict(state)\n",
    "            return np.argmax(q_values[0])\n",
    "\n",
    "    def train_model(self, state, action, reward, next_state, next_action, done):\n",
    "        if self.epsilon > self.epsilon_min:\n",
    "            self.epsilon *= self.epsilon_decay\n",
    "\n",
    "        state = np.float32(state)\n",
    "        next_state = np.float32(next_state)\n",
    "        target = self.model.predict(state)[0]\n",
    "        # 살사의 큐함수 업데이트 식\n",
    "        if done:\n",
    "            target[action] = reward\n",
    "        else:\n",
    "            target[action] = (reward + self.discount_factor *\n",
    "                              self.model.predict(next_state)[0][next_action])\n",
    "\n",
    "        # 출력 값 reshape\n",
    "        target = np.reshape(target, [1, 5])\n",
    "        # 인공신경망 업데이트\n",
    "        self.model.fit(state, target, epochs=1, verbose=0)\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    # 환경과 에이전트 생성\n",
    "    env = Env()\n",
    "    agent = DeepSARSAgent()\n",
    "\n",
    "    global_step = 0\n",
    "    scores, episodes = [], []\n",
    "\n",
    "    for e in range(EPISODES):\n",
    "        done = False\n",
    "        score = 0\n",
    "        state = env.reset()\n",
    "        state = np.reshape(state, [1, 15])\n",
    "\n",
    "        while not done:\n",
    "            # env 초기화\n",
    "            global_step += 1\n",
    "\n",
    "            # 현재 상태에 대한 행동 선택\n",
    "            action = agent.get_action(state)\n",
    "            # 선택한 행동으로 환경에서 한 타임스텝 진행 후 샘플 수집\n",
    "            next_state, reward, done = env.step(action)\n",
    "            next_state = np.reshape(next_state, [1, 15])\n",
    "            next_action = agent.get_action(next_state)\n",
    "            # 샘플로 모델 학습\n",
    "            agent.train_model(state, action, reward, next_state, next_action,\n",
    "                              done)\n",
    "            state = next_state\n",
    "            score += reward\n",
    "\n",
    "            state = copy.deepcopy(next_state)\n",
    "\n",
    "            if done:\n",
    "                # 에피소드마다 학습 결과 출력\n",
    "                scores.append(score)\n",
    "                episodes.append(e)\n",
    "                pylab.plot(episodes, scores, 'b')\n",
    "                pylab.savefig(\"./save_graph/deep_sarsa_.png\")\n",
    "                print(\"episode:\", e, \"  score:\", score, \"global_step\",\n",
    "                      global_step, \"  epsilon:\", agent.epsilon)\n",
    "\n",
    "        # 100 에피소드마다 모델 저장\n",
    "        if e % 100 == 0:\n",
    "            agent.model.save_weights(\"./save_model/deep_sarsa.h5\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
